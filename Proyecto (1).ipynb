{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proyecto\n",
    "\n",
    "\n",
    "En este proyecto vamos a hacer machine learning aplicado al ámbito medico ,específicamente diagnostico medico.\n",
    "Vamos a crear una aplicación que ayuda a diagnosticar si un tumor de seno es cancerígeno (maligno) o no (benigno)   .\n",
    "Para lograr esto vamos a utilizar un set de datos recopilado por diversos médicos , el cual contiene características de tumores de seno y el diagnostico final . Este es un problema de clasificación binaria ya que solo tenemos dos clases(maligno o benigno), se va a implementar regresión logística para obtener la probabilidad de que el tumor sea maligno.\n",
    "\n",
    "<img src=\"images/ai_cancer.png\" width=\"300\">\n",
    "\n",
    "Según lo visto en la clase en este proyecto el estudiante hará  todo el proceso de entrenamiento, evaluación y selección del modelo de machine learning(pasos 1 al 5 del diagrama de flujo de ejemplo) ,luego de seleccionar el mejor va a exportar el modelo (como fue explicado en la clase deployment/despliegue de modelos y va a entregarlo al profesor) ya que  este será utilizado dentro de una aplicación móvil (desarrollada por el profesor, paso 6 del diagrama )\n",
    "\n",
    "\n",
    "<img src=\"images/diagrama.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "from sklearn.utils import resample\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score\n",
    "import datetime\n",
    "import helper\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Registro de Experimentos\n",
    "En machine learning y ciencia de datos es importante tener un registro de los resultados de cada experimento realizado, así como la configuración del sistema(tal como learning-rate, numero de iteraciones, cantidad de observaciones y features, etc) que llevo a determinados resultados, esto porque el proceso puede llegar a ser muy iterativo y nos es útil saber en todo momento que caso tuvo los mejores resultados. Por esta razón en este proyecto utilizamos una bitácora o log-book científico en el cual tenemos la configuración de nuestros modelos y los resultados(métricas de evaluación) de cada uno."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_book = helper.load_log_book()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez la bitacora contenga informacion ,puedes consultarla en cualquier celda usando:\n",
    "\n",
    "helper.print_log_book(log_book)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para guardar la bitacora (una vez has capturado el resultado de los experimentos que realizaremos) usaremos el siguiente codigo(hay mas detalles mas adelante)\n",
    "\n",
    "helper.guardar_log_book(log_book)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carnet estudiante"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " La aplicación móvil resultante tendrá un modelo por estudiante, por lo cual es importante identificar a que estudiante pertenece cada modelo, por esta razón  en el proceso de exportar el modelo el archivo resultante tendrá en su nombre el carnet del estudiante. \n",
    " \n",
    " En una variable deben ingresar su numero de carnet, para que al exportar el modelo el archivo se llame `carnet+\"model.csv\"`. Por ejemplo 200818835model.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "carnet = \"20150066\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Breast Cancer Data Set\n",
    "\n",
    "El dataset que usaremos contiene varias características(features) , más adelante encontraras instrucciones de cómo trabajar con estas. En el siguiente enlace puedes acceder a mayor información y detalles del dataset.\n",
    "\n",
    "[Link al set de datos](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html#sklearn.datasets.load_breast_cancer)\n",
    "\n",
    "#### Data Set Information:\n",
    "\n",
    "Features are computed from a digitized image of a fine needle aspirate (FNA) of a breast mass. They describe characteristics of the cell nuclei present in the image. A few of the images can be found at [Web Link] \n",
    "\n",
    "Separating plane described above was obtained using Multisurface Method-Tree (MSM-T) [K. P. Bennett, \"Decision Tree Construction Via Linear Programming.\" Proceedings of the 4th Midwest Artificial Intelligence and Cognitive Science Society, pp. 97-101, 1992], a classification method which uses linear programming to construct a decision tree. Relevant features were selected using an exhaustive search in the space of 1-4 features and 1-3 separating planes. \n",
    "\n",
    "The actual linear program used to obtain the separating plane in the 3-dimensional space is that described in: [K. P. Bennett and O. L. Mangasarian: \"Robust Linear Programming Discrimination of Two Linearly Inseparable Sets\", Optimization Methods and Software 1, 1992, 23-34]. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_breast_cancer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este es un problema de clasificacion binario por que que existen solo dos posibles valores(predecimos tumores cancerigenos de seno)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['malignant', 'benign'], dtype='<U9')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El data set tiene varios features, por lo  que debemos experimentar con diferentes features y analizar los resultados, en esta celda vamos a imprimir los nombres de las features que poseemos.\n",
    "\n",
    "En este [link](https://goo.gl/U2Uwz2) podemos encontrar mas información sobre las features.\n",
    "\n",
    "Nota: Aunque a lo largo del proyecto, usaremos distintas features para entrenar diversos modelos y entender cómo funciona el proceso de ML y el impacto del uso de diversas features, en el resultado final (el modelo que será exportado e integrado a la aplicación móvil) solo usaremos las primeras 5 features.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['mean radius', 'mean texture', 'mean perimeter', 'mean area',\n",
       "       'mean smoothness', 'mean compactness', 'mean concavity',\n",
       "       'mean concave points', 'mean symmetry', 'mean fractal dimension',\n",
       "       'radius error', 'texture error', 'perimeter error', 'area error',\n",
       "       'smoothness error', 'compactness error', 'concavity error',\n",
       "       'concave points error', 'symmetry error',\n",
       "       'fractal dimension error', 'worst radius', 'worst texture',\n",
       "       'worst perimeter', 'worst area', 'worst smoothness',\n",
       "       'worst compactness', 'worst concavity', 'worst concave points',\n",
       "       'worst symmetry', 'worst fractal dimension'], dtype='<U23')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "muestra de las primeras 5 labels/etiquetas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.target[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "las features para estos 5 casos de ejemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.799e+01, 1.038e+01, 1.228e+02, 1.001e+03, 1.184e-01, 2.776e-01,\n",
       "        3.001e-01, 1.471e-01, 2.419e-01, 7.871e-02, 1.095e+00, 9.053e-01,\n",
       "        8.589e+00, 1.534e+02, 6.399e-03, 4.904e-02, 5.373e-02, 1.587e-02,\n",
       "        3.003e-02, 6.193e-03, 2.538e+01, 1.733e+01, 1.846e+02, 2.019e+03,\n",
       "        1.622e-01, 6.656e-01, 7.119e-01, 2.654e-01, 4.601e-01, 1.189e-01],\n",
       "       [2.057e+01, 1.777e+01, 1.329e+02, 1.326e+03, 8.474e-02, 7.864e-02,\n",
       "        8.690e-02, 7.017e-02, 1.812e-01, 5.667e-02, 5.435e-01, 7.339e-01,\n",
       "        3.398e+00, 7.408e+01, 5.225e-03, 1.308e-02, 1.860e-02, 1.340e-02,\n",
       "        1.389e-02, 3.532e-03, 2.499e+01, 2.341e+01, 1.588e+02, 1.956e+03,\n",
       "        1.238e-01, 1.866e-01, 2.416e-01, 1.860e-01, 2.750e-01, 8.902e-02],\n",
       "       [1.969e+01, 2.125e+01, 1.300e+02, 1.203e+03, 1.096e-01, 1.599e-01,\n",
       "        1.974e-01, 1.279e-01, 2.069e-01, 5.999e-02, 7.456e-01, 7.869e-01,\n",
       "        4.585e+00, 9.403e+01, 6.150e-03, 4.006e-02, 3.832e-02, 2.058e-02,\n",
       "        2.250e-02, 4.571e-03, 2.357e+01, 2.553e+01, 1.525e+02, 1.709e+03,\n",
       "        1.444e-01, 4.245e-01, 4.504e-01, 2.430e-01, 3.613e-01, 8.758e-02],\n",
       "       [1.142e+01, 2.038e+01, 7.758e+01, 3.861e+02, 1.425e-01, 2.839e-01,\n",
       "        2.414e-01, 1.052e-01, 2.597e-01, 9.744e-02, 4.956e-01, 1.156e+00,\n",
       "        3.445e+00, 2.723e+01, 9.110e-03, 7.458e-02, 5.661e-02, 1.867e-02,\n",
       "        5.963e-02, 9.208e-03, 1.491e+01, 2.650e+01, 9.887e+01, 5.677e+02,\n",
       "        2.098e-01, 8.663e-01, 6.869e-01, 2.575e-01, 6.638e-01, 1.730e-01],\n",
       "       [2.029e+01, 1.434e+01, 1.351e+02, 1.297e+03, 1.003e-01, 1.328e-01,\n",
       "        1.980e-01, 1.043e-01, 1.809e-01, 5.883e-02, 7.572e-01, 7.813e-01,\n",
       "        5.438e+00, 9.444e+01, 1.149e-02, 2.461e-02, 5.688e-02, 1.885e-02,\n",
       "        1.756e-02, 5.115e-03, 2.254e+01, 1.667e+01, 1.522e+02, 1.575e+03,\n",
       "        1.374e-01, 2.050e-01, 4.000e-01, 1.625e-01, 2.364e-01, 7.678e-02]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.data[0:5,]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementación de Regresión Logística "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hipótesis\n",
    "Función Sigmoid:\n",
    "\n",
    "$sig(t) = {\\frac {1} {1 + e^{-t}}}$\n",
    "\n",
    "Combinamos la función sigmoid/lógistica con la  hipótesis conocida (aprendida en regresión lineal) y tenemos la nueva hipótesis para clasificación con regresión lógistica:\n",
    "\n",
    "$z = \\theta_{0} + \\theta_{1}x_{1} + \\theta_{2}x_{2} + \\ldots + \\theta_{n}x_{n}$\n",
    "\n",
    "$g(z) = {\\frac {1} {1 + e^{-z}}}$\n",
    "\n",
    "** Hipótesis para clasificación con regresión logística: **\n",
    "\n",
    "$h_{\\theta}(x) = {\\frac {1} {1 + e^{-(\\theta_{0} + \\theta_{1}x_{1} + \\theta_{2}x_{2} + \\ldots + \\theta_{n}x_{n})}}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sigmoid \n",
    "\n",
    "def sigmoid(z):\n",
    "    ### INICIO: TU CODIGO AQUI:  (~1 linea)###\n",
    "    s = 1/(1+np.exp(-z))\n",
    "    ### FIN ##\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigmoid(0) = 0.5\n",
      "sigmoid(9.2) = 0.9998989708060922\n"
     ]
    }
   ],
   "source": [
    "### Validando nuestra funcion sigmoid\n",
    "print (\"sigmoid(0) = \" + str(sigmoid(0)))\n",
    "print (\"sigmoid(9.2) = \" + str(sigmoid(9.2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resultados esperados\n",
    "\n",
    "**sigmoid(0)** = 0.5\n",
    "\n",
    "**sigmoid(9.2)** = 0.999898970806\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementado la hipótesis/modelo para clasificación con regresión logística\n",
    "\n",
    "** Hipótesis para clasificación con regresión logística: **\n",
    "\n",
    "$h_{\\theta}(x) = {\\frac {1} {1 + e^{-(\\theta_{0} + \\theta_{1}x_{1} + \\theta_{2}x_{2} + \\ldots + \\theta_{n}x_{n})}}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hipotesis(features, theta_values):\n",
    "    ## Calculo de la hipotesis de froma vectorizada,producto punto entre el vector de features y el vector de parametros theta ##\n",
    "    z = features.dot(theta_values)\n",
    "    \n",
    "    ### INICIO: TU CODIGO AQUI:  (~1 linea)###\n",
    "    h = 1/(1+np.exp(-z))\n",
    "    ### FIN ##\n",
    "    \n",
    "    return h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Métricas de evaluación\n",
    "\n",
    "Tal como lo vimos en la clase 22: evaluación , utilizamos diversas métricas para evaluar y reportar la exactitud y rendimiento de nuestros modelos de machine learning. En clasificación usamos el costo: cross-entropy para entrenar, pero evaluamos y reportamos diversas métricas mas ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Costo\n",
    "\n",
    "El proceso de gradient descent busca minimizar la siguiente función de costo en función de los parámetros theta, y así encontrar los parámetros que producen una hipótesis optima. \n",
    "\n",
    "El costo mide que tan buena o mala es una hipótesis por lo cual esperamos que este disminuya durante el entrenamiento.\n",
    "\n",
    "$J(\\Theta) = - {\\frac {1}{m}}\\sum _{i=1}^{m} y \\log(h(x)) + (1-y)\\log(1-h(x))$\n",
    "\n",
    "Utiliza la funcion log de numpy, Ejemplo:\n",
    "`np.log(y)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "def costo(X, theta_vector, y):\n",
    "    m = len(y)\n",
    "    \n",
    "    ### INICIO: TU CODIGO AQUI:  (~2 lineas)###\n",
    "    y_hat = get_hipotesis(X,theta_vector)\n",
    "    \n",
    "    costo = y*(np.log(y_hat+(1-y)*np.log(1-y_hat)))\n",
    "\n",
    "    ### FIN ##\n",
    "    \n",
    "    return costo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy Score\n",
    "\n",
    "Implementa la funcion de accuracy utlizando sklearn(puedes utilizar como referencia la clase 22: evaluación )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(X, theta_vector, y):\n",
    "\n",
    "    hipotesis = get_hipotesis(X, theta_vector)\n",
    "    \n",
    "    # Convertimos el resultado probabilistico a deterministico (0 1)\n",
    "    y_pred = [1 if (y >= 0.5) else 0 for y in hipotesis]\n",
    "    \n",
    "    ### INICIO: TU CODIGO AQUI:  (~1 linea)###\n",
    "    acc_score = accuracy_score(y, y_pred)\n",
    "    ### FIN ##\n",
    "    \n",
    "    return acc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F1 Score\n",
    "\n",
    "Implementa la funcion de f1_score utlizando sklearn(puedes utilizar como referencia la clase 22: evaluación )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1(X, theta_vector, y):\n",
    "\n",
    "    hipotesis = get_hipotesis(X, theta_vector)\n",
    "\n",
    "    # Convertimos el resultado probabilistico a deterministico (0 1)    \n",
    "    y_pred = [1 if (y >= 0.5) else 0 for y in hipotesis ]\n",
    "    \n",
    "    ## INICIO: TU CODIGO AQUI:  (~1 linea)###\n",
    "    f1 = f1_score(y, y_pred)\n",
    "       ### FIN ##\n",
    "    \n",
    "    return f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision Score\n",
    "\n",
    "Implementa la funcion de precision_score utlizando sklearn(puedes utilizar como referencia la clase 22: evaluación )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(X, theta_vector, y):\n",
    "\n",
    "    hipotesis = get_hipotesis(X, theta_vector)\n",
    "\n",
    "    # Convertimos el resultado probabilistico a deterministico (0 1)    \n",
    "    y_pred = [1 if (y >= 0.5) else 0 for y in hipotesis ]\n",
    "    \n",
    "    ## INICIO: TU CODIGO AQUI:  (~1 linea)###\n",
    "    precision = precision_score(y, y_pred)\n",
    "    ### FIN ##\n",
    "    \n",
    "    return precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recall Score\n",
    "\n",
    "Implementa la funcion de recall_score utlizando sklearn(puedes utilizar como referencia la clase 22: evaluación )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall(X, theta_vector, y):\n",
    "\n",
    "    hipotesis = get_hipotesis(X, theta_vector)\n",
    "\n",
    "    # Convertimos el resultado probabilistico a deterministico (0 1)    \n",
    "    y_pred = [1 if (y >= 0.5) else 0 for y in hipotesis ]\n",
    "    \n",
    "    ## INICIO: TU CODIGO AQUI:  (~1 linea)###\n",
    "    reacall = recall_score(y, y_pred)\n",
    "    ### FIN ##\n",
    "    \n",
    "    return recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-procesamiento\n",
    "\n",
    "En Machine learning, la fase de pre-procesamiento(incluyendo feature engineering) de datos puede llegar a abarcar el 70% o más del tiempo de un proyecto , pero este es un tema que se profundiza en otros cursos del área de ciencia de datos por lo cual en este proyecto solo vamos a realizar como pre-procesamiento una normalización de features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estandarización o Z-score Normalizacion \n",
    "\n",
    "El objetivo de la normalización es ayudar a gradient descent a encontrar el mínimo del costo mas rápido y fácilmente , esto se logra a través de convertir las features de manera que tengan media = 0 y desviación estándar de 1 , similar a una distribución de probabilidad normal(gaussiana).\n",
    "\n",
    "Lo calculamos utilizando:\n",
    "\n",
    "$z = {\\frac {x - \\mu} {\\sigma}}$\n",
    "\n",
    "Donde $\\mu$ es la media y $\\sigma$ es la desviacion estandard\n",
    "\n",
    "##### Comentario opcional:\n",
    "el Z-score en el nombre concuerda con el uso de \"z-scores\" en estadística cuando se hace estadística inferencial(prueba de hipótesis e intervalos de confianza)\n",
    "\n",
    "Implementa la funcion de normalizacion sin utlizar sklearn solo puedes utilizar numpy.\n",
    "\n",
    "Para utilizar la columna de features, utilizamos `features[:,i]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizar(features):\n",
    "    std = 1\n",
    "    media = 0\n",
    "    \n",
    "    # Utilizamos el ciclo, para normalizar cada feature(una iteracion por feature)\n",
    "    for i in list(range(0, features.shape[1])):\n",
    "        ### INICIO: TU CODIGO AQUI:  (~3 lineas)###\n",
    "        std = np.std(features)\n",
    "        media = np.mean(features)\n",
    "        features[:,i] = (features[:,i] - media) / std;\n",
    "        ### FIN ##\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediccion\n",
    "\n",
    "Una vez entrenado, el modelo de ML se utiliza para realizar predicciones ,en este proyecto realizaremos predicciones en un train y un test set(no usaremos cross-validation set)  .Una  vez exportado y entregado el modelo al profesor, el modelo realizara predicciones dentro de una aplicación móvil . \n",
    "\n",
    "La siguiente función la vamos a utilizar para realizar predicciones a lo largo del proyecto(y así poder medir contra los datos reales para realizar evaluación), no hay que cambiar nada del código, solo ejecuta la celda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediccion(x, theta_vector, normalizar_valores):\n",
    "    numero_features = x.shape[1] + 1\n",
    "    \n",
    "    if (normalizar_valores):\n",
    "        x = normalizar(x)\n",
    "    \n",
    "    x_features = np.ones((x.shape[0], numero_features))\n",
    "    x_features[:,:-1] = x\n",
    "\n",
    "    \n",
    "    y_hat = get_hipotesis(x_features, theta_vector)\n",
    "    \n",
    "    return np.array([1 if (y >= 0.5) else 0 for y in y_hat])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Gradient Descent\n",
    "\n",
    "Gradient descent es el algoritmo o método de optimización matemática que nos permite realizar el “entrenamiento” de nuestro modelos, ya que minimiza el costo y nos devuelve los parámetros del modelo óptimos.\n",
    "\n",
    "En la siguiente celda, debes implementar gradient descent tal como visto en clase y practicado en las tareas .\n",
    "\n",
    "Lo definimos e implementamos dentro de una función de Python para que podamos ejecutarlo(“llamarlo”) múltiples veces con distintos parámetros, sin tener que volver a programarlo cada vez , esto nos será útil para experimentar y entrenar varios modelos.\n",
    "\n",
    "Repetir{\n",
    "\n",
    "$\\theta_{j} := \\theta _{j} - \\alpha {\\frac {1}{m}} \\sum _{i=1}^{m}(h_{\\theta}(x^{(i)})-y^{(i)})x^{(i)}_{j}$\n",
    "\n",
    "}\n",
    "        \n",
    "Simultaneamente para cada $j = 0,\\ldots,n $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, theta_vector, alpha, iterations, X_test, y_test, nombre_modelo):\n",
    "    global log_book\n",
    "    m = len(y)  #numero de ejemplos de entrenamiento\n",
    "\n",
    "    ## Las siguientes 3 lineas de codigo crean listas vacias para almacenar el costo y accuracy obtenido en cada iteracion\n",
    "    ## Se espera que el costo disminuya y el accuracy aumente\n",
    "    cost_vect = []\n",
    "    accuracy_vect = []\n",
    "    accuracy_vect_test = []\n",
    "    \n",
    "    for iteration in range(iterations):\n",
    "        '''\n",
    "        IMPLELEMTA TU CODIGO AQUI, llama a la funcion para calcular la hipotesis,es decir calcular h(x)\n",
    "        que se implemento mas arriba. Parametros matriz de features X y el vector\n",
    "        con los valores Theta (~1 linea)\n",
    "        '''\n",
    "        hipotesis = get_hipotesis(X, theta_vector)\n",
    "\n",
    "\n",
    "        theta_vector_temp = [] # vector que guarda los parametros theta temporales\n",
    "        \n",
    "        # Utilizamos un ciclo para realizar la actualizacion de gradient descent una vez para cada parametro\n",
    "        # En cada iteracion \"i\" guarda el numero de parametro theta, y la varaible \"theta\" guarda el parametro como tal\n",
    "        for i, theta in enumerate(theta_vector):\n",
    "            '''\n",
    "            IMPLELEMTA TU CODIGO AQUI, adentro de este ciclo vamos a calcular\n",
    "            elnuevo valor de Theta[i], utilizando la formula de la celda de arriba\n",
    "            Recuerda que para multiplicar por utilizamos X[:,i]  (~1 linea)\n",
    "            '''\n",
    "            nuevo_theta = theta-alpha*(1/(int(m))*np.sum(np.subtract(hipotesis,y)*X[:,i]))\n",
    "\n",
    "            ### aqui agregamos el nuevo valor the theta[i] a un vector temporal\n",
    "            theta_vector_temp.append(nuevo_theta)\n",
    "\n",
    "\n",
    "        ## Actualizamos el vector theta con los nuevos valores\n",
    "        theta_vector = theta_vector_temp\n",
    "\n",
    "        # En las sigiuentes lineas calculamos las metricas de evaluacion en cada iteracion(aplicadas al train-set)\n",
    "        '''\n",
    "        IMPLEMENTA TU CODIGO AQUI, llama a la funcion de costo (~1 linea)\n",
    "        '''\n",
    "        cost = costo(X, theta_vector, y)\n",
    "        \n",
    "        '''\n",
    "        IMPLEMENTA TU CODIGO AQUI, utiliza Accuracy que se definio arriba (~1 linea)\n",
    "        '''\n",
    "        y_pred = [1 if (y >= 0.5) else 0 for y in hipotesis]\n",
    "        acc_score = accuracy_score(y, y_pred, normalize=True)\n",
    "        \n",
    "        '''\n",
    "        IMPLEMENTA TU CODIGO AQUI, utiliza f1 score que se definio arriba (~1 linea)\n",
    "        '''\n",
    "        f1_training = f1(X, theta_vector, y)\n",
    "\n",
    "        '''\n",
    "        IMPLEMENTA TU CODIGO AQUI, utiliza precision score que se definio arriba (~1 linea)\n",
    "        '''\n",
    "        precision_training = precision(X, theta_vector, y) \n",
    "        \n",
    "        '''\n",
    "        IMPLEMENTA TU CODIGO AQUI, utiliza recall score que se definio arriba (~1 linea)\n",
    "        '''\n",
    "        recall_training = recall(X, theta_vector, y)\n",
    "        \n",
    "        \n",
    "        ## En las siguientes lineas calculamos las metricas de evaluacion aplicadas al test-set, este codigo no hay que cambiarlo ####\n",
    "        y_predict = prediccion(X_test, theta_vector, True)\n",
    "        acc_test = accuracy_score(y_predict, y_test)\n",
    "        accuracy_vect_test.append(acc_test)\n",
    "        f1_score_test = f1_score(y_predict, y_test)\n",
    "        precision_test = precision_score(y_test, y_predict)\n",
    "        recall_test = recall_score(y_test, y_predict)\n",
    "        \n",
    "        ## Guardando el valor del costo para graficarlo, este codigo no hay que cambiarlo ##\n",
    "        cost_vect.append(cost)\n",
    "        accuracy_vect.append(acc_score)\n",
    "        \n",
    "        n = 300 ### Ingresa aqui cada cuantas iteraciones quieres que se imprima el valor de las metricas ###\n",
    "        \n",
    "        ## Este codigo no hay que modificarlo ##\n",
    "        if(iteration % n == 0):\n",
    "            print('#####################')\n",
    "            print('TRAINING: [Iteracion: ', iteration,' Costo: ', cost, \\\n",
    "                  ' Accuracy:', acc_score,' F1 Score:',f1_training, \\\n",
    "                  'Precision training:', precision_training, 'Recall training:', recall_training,']')\n",
    "            print('TEST: [Iteracion: ', iteration,'Accuracy:', acc_test, \\\n",
    "                  ' F1 Score:', f1_score_test, 'Precision Test:', precision_test,'Recall Test:', recall_test,']')\n",
    "        \n",
    "    ## Log book, este codigo no hay que cambiarlo, sirve para guardar en la bitacora los resultados del experimento ##\n",
    "    nombre_modelo = nombre_modelo + '_'+datetime.datetime.now().strftime(\"%d%H%M%f\")\n",
    "    numero_features = X.shape[1] - 1\n",
    "    experiment = np.array([nombre_modelo, numero_features, m, alpha, iterations, accuracy_vect[-1], \\\n",
    "                           accuracy_vect_test[-1], f1_score_test, precision_test, \\\n",
    "                           recall_test,theta_vector], dtype=object)\n",
    "    log_book = np.vstack([log_book, experiment])\n",
    "        \n",
    "    return theta_vector, cost_vect, accuracy_vect,accuracy_vect_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dividimos la data en training y test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tal como visto en la clase 23, nos sirve para evaluar objetivamente el rendimiento de nuestros modelos en datos que nunca ha visto durante su entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, stratify=data.target, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(426, 30)"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrenamiento\n",
    "\n",
    "## Features a utilizar\n",
    "A continuación, vamos a entrenar varios modelos, vamos a utilizar las funciones que creamos previamente.\n",
    "Es importante experimentar con diferentes features,diverso tamaño del training set, y diversos hyper-parametros(learning rate e iteraciones)\n",
    "\n",
    "En este proyecto como máximo vamos a utilizar 5 features(pero en algunos experimentos usaremos menos) de manera obligatoria, pero de forma opcional se recomienda experimentar con más y diversas combinaciones ,esto para practicar y obtener experiencia con ML. \n",
    "\n",
    "También vamos a utilizar diferente número de ejemplos en el set de datos, diferente número de iteraciones y diferente learning rate . Todas estas distintas configuraciones serán almacenadas en el log-book(bitácora) automáticamente .\n",
    "\n",
    "En este proyecto solo vamos a utilizar las primeras 5 features  en este orden:\n",
    "\n",
    "`['mean radius', 'mean texture', 'mean perimeter', 'mean area','mean smoothness']`\n",
    "\n",
    "La razón por las que usaremos estas 5 features específicamente(y en este orden) en el modelo final(el que será exportado) es que estas features son las que serán utilizadas en la aplicación móvil en la cual integraremos el modelo(y tendremos en la interfaz de usuario de la aplicación componentes para ingresar valores para estas features),ero como se mencionó se recomienda experimentar y evaluar resultados con distintas features y distinto número  de ellas .\n",
    "\n",
    "## Descripcion del entrenamiento\n",
    "El entrenamiento de cada modelo se compone básicamente de tres pasos, sampleo bootstrap, escoger las features y ejecutar el entrenamiento llamando a la función de gradient descent que ya creamos.\n",
    "\n",
    "En el primer paso vamos a utilizar la función resample para hacer un sampleo del set de datos de entrenamiento, debemos definir el numero de samples que queremos utilizar.\n",
    "\n",
    "En el segundo paso, en un arreglo vamos a definir el nombre de las features que queremos utilizar para entrenar el modelo.\n",
    "En el tercer paso ejecutamos la función gradient descent.\n",
    "\n",
    "Por ultimo vamos a obtener el accuracy del modelo y vamos a imprimir la matriz de confusión, imprimiremos el precisión y el recall del modelo para entender mejor los resultados y tener una mejor idea de que tan bien clasifica nuestro modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento modelo 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sampleo bootstrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_m1, Y_train_m1 = resample(X_train, y_train, n_samples = 460)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtrar Features\n",
    "Solo seleccionar los features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Ingresa en la lista el nombre de las fatrues que quieres utlizar en este modelo (~1 linea)\n",
    "'''\n",
    "features_filtro = ['mean radius', 'mean texture', 'mean perimeter', 'mean area','mean smoothness']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "boolean index did not match indexed array along dimension 1; dimension is 5 but corresponding boolean dimension is 30",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-310-a95263100cf5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mX_train_m1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhelper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfitrar_nombre\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_names\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfeatures_filtro\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mX_test_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhelper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfitrar_nombre\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_names\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfeatures_filtro\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Machine Learning\\helper.py\u001b[0m in \u001b[0;36mfitrar_nombre\u001b[1;34m(x, feature_names, filtro)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mfitrar_nombre\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeature_names\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfiltro\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[0mselecciones\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeature_names\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfiltro\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mselecciones\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mload_log_book\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: boolean index did not match indexed array along dimension 1; dimension is 5 but corresponding boolean dimension is 30"
     ]
    }
   ],
   "source": [
    "X_train_m1 = helper.fitrar_nombre(X_train_m1,data.feature_names,features_filtro)\n",
    "X_test_features = helper.fitrar_nombre(X_test,data.feature_names,features_filtro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(460, 5)"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_m1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(143, 5)"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Numero de iteracions ##\n",
    "iterations = 2000  # Define el numero de iteraciones #\n",
    "\n",
    "## Alpha ##\n",
    "alpha = 0.01 # Define el valor de alpha #\n",
    "\n",
    "#### No hay que cambiar el codigo despues de esta linea #####\n",
    "## Numero de feautres ##\n",
    "numero_features = X_train_m1.shape[1] + 1\n",
    "\n",
    "## Creamos un vector donde se almacenan los valores de Theta,\n",
    "## lo inicializamos con numeros aleatorios\n",
    "theta_vector = np.random.rand(numero_features)\n",
    "\n",
    "## Normaliza las features de X_train_features (~1 linea) ##\n",
    "X_train_m1 = normalizar(X_train_m1)\n",
    "\n",
    "# Creando la Matriz X de features, \n",
    "# utilizamos np.ones para agregar el valor constante '1' que es el bias o feature 0\n",
    "X_train_features = np.ones((X_train_m1.shape[0], numero_features))\n",
    "X_train_features[:,:-1] = X_train_m1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin VDK\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: RuntimeWarning: invalid value encountered in log\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#####################\n",
      "TRAINING: [Iteracion:  0  Costo:  [-0.59660798         nan -0.59425946         nan -0.53388773         nan\n",
      "         nan         nan -0.70089653         nan -0.40888194 -0.64286072\n",
      "         nan         nan -0.63052361         nan -0.56796439 -0.49401282\n",
      " -0.62562623 -0.60962109         nan -0.54928233 -0.5096284          nan\n",
      "         nan -0.53310627 -0.50046052         nan -0.758161           nan\n",
      "         nan         nan         nan         nan -0.53876829         nan\n",
      " -0.56382193 -0.54270236 -0.62459869 -0.42866129         nan         nan\n",
      " -0.5096284          nan -0.60511711         nan -0.47614276 -0.63934991\n",
      " -0.58399841 -0.73624463 -0.65217662 -0.54270236 -0.54536645 -0.61734363\n",
      "         nan         nan         nan -0.46074014         nan -0.70585654\n",
      "         nan         nan         nan -0.70235769 -0.62459869 -0.59308651\n",
      " -0.49085573         nan -0.56423332 -0.60533104         nan -0.5621258\n",
      " -0.65025474         nan -0.5805448  -0.68754622 -0.65646841         nan\n",
      " -0.43899696 -0.45298374 -0.60252764 -0.44819005 -0.6530857  -0.51197963\n",
      "         nan -0.56161914         nan -0.7243418  -0.50156906 -0.5805448\n",
      "         nan         nan -0.71599426         nan         nan -0.5621258\n",
      "         nan -0.6162832  -0.58808649         nan -0.58083562 -0.50022237\n",
      "         nan -0.68114433         nan -0.57764003         nan         nan\n",
      "         nan -0.70089653 -0.51304393         nan         nan         nan\n",
      " -0.43707537 -0.4870445  -0.49127719         nan -0.57528385         nan\n",
      " -0.5996651  -0.6584575  -0.49480025 -0.43282664 -0.44962974 -0.68611377\n",
      "         nan -0.59308651         nan         nan         nan -0.69407783\n",
      " -0.45287236         nan -0.50505853         nan -0.37878862         nan\n",
      " -0.5763839  -0.52468546 -0.52361921 -0.5330147          nan -0.54306103\n",
      " -0.4870445          nan -0.53012834 -0.50022237         nan -0.76236178\n",
      " -0.54548698 -0.55543436 -0.59440539 -0.49085573         nan -0.57246906\n",
      " -0.48631878 -0.70235769         nan -0.62421618 -0.56382193 -0.56796439\n",
      " -0.63014368 -0.758161   -0.43562492 -0.61773904 -0.59560907 -0.46228955\n",
      " -0.56430331 -0.65733216         nan         nan -0.48612924         nan\n",
      " -0.51827219 -0.57246906 -0.45136783 -0.56049075 -0.45298374         nan\n",
      " -0.42264722         nan         nan -0.56382193 -0.758161   -0.52468546\n",
      " -0.62459869         nan -0.57642143 -0.52204693         nan -0.50156906\n",
      " -0.52603384 -0.74233635 -0.42866129         nan -0.43447618 -0.45287236\n",
      "         nan         nan         nan         nan -0.43940937 -0.56781728\n",
      " -0.50950281 -0.60787744         nan         nan -0.53146977         nan\n",
      " -0.44364839         nan         nan         nan -0.55482245         nan\n",
      " -0.63135793 -0.58569731 -0.55857321         nan         nan -0.45868717\n",
      "         nan         nan -0.48895309         nan -0.53609539         nan\n",
      "         nan         nan         nan         nan         nan -0.59440539\n",
      " -0.60373395 -0.53442321         nan         nan -0.54817464 -0.68409219\n",
      " -0.58808649         nan         nan -0.68090485 -0.50908901         nan\n",
      " -0.61295201         nan         nan         nan         nan -0.7243418\n",
      " -0.43282664         nan -0.54878727         nan -0.48612924         nan\n",
      " -0.62562623         nan -0.5239305  -0.53609539 -0.70686201 -0.60715676\n",
      " -0.68754622 -0.57012245 -0.49085573 -0.51304393         nan -0.66995242\n",
      "         nan -0.5330147  -0.70235769         nan         nan -0.4870445\n",
      " -0.50156906 -0.66305624 -0.5805448  -0.64286072         nan -0.478885\n",
      " -0.61521643 -0.45281617 -0.59810101         nan         nan -0.45136783\n",
      " -0.44962974 -0.68551192         nan -0.60254546 -0.50078506         nan\n",
      "         nan         nan         nan -0.53256391 -0.54928233 -0.47614276\n",
      " -0.65733216 -0.48928037 -0.52923734 -0.70235769 -0.54255661 -0.5330147\n",
      "         nan -0.66957643 -0.65025474         nan -0.60254546         nan\n",
      "         nan -0.45287236         nan -0.61734363         nan         nan\n",
      " -0.56656516 -0.67167526         nan         nan -0.6186039  -0.55736841\n",
      " -0.54493235 -0.66957643 -0.44364839 -0.44944156 -0.58780962         nan\n",
      " -0.5996651          nan         nan         nan -0.60715676 -0.5330147\n",
      " -0.60533104 -0.65217662         nan -0.72364812 -0.65646841         nan\n",
      "         nan -0.69407783 -0.56161914 -0.50505853 -0.57528385 -0.43707537\n",
      " -0.46053957         nan -0.71599426 -0.62421618         nan         nan\n",
      "         nan -0.62421618 -0.64834263 -0.53609539 -0.55978089 -0.54493235\n",
      " -0.48503214         nan         nan -0.55045457         nan         nan\n",
      "         nan         nan -0.54306103 -0.63929702 -0.42264722 -0.56781728\n",
      " -0.55543436 -0.65733216         nan -0.5096284  -0.60252764 -0.64286072\n",
      " -0.57091092         nan         nan -0.40888194         nan         nan\n",
      "         nan         nan -0.59828809 -0.68467527         nan -0.63014368\n",
      " -0.58569731 -0.48612924 -0.51102169         nan -0.46053957         nan\n",
      "         nan -0.66305624 -0.56781728 -0.64834263 -0.55644759 -0.54548698\n",
      "         nan -0.54928233         nan -0.49473788 -0.67279892 -0.478885\n",
      "         nan -0.51827219 -0.6530857  -0.60787744 -0.58569731 -0.53019046\n",
      "         nan -0.68114433         nan         nan -0.49480025         nan\n",
      " -0.63869932 -0.53012834         nan -0.60787744 -0.60511711 -0.58598578\n",
      " -0.65140899         nan         nan -0.65733216         nan         nan\n",
      "         nan         nan -0.54306103         nan -0.43707537         nan\n",
      " -0.5330147          nan -0.55045457 -0.63869932 -0.5239305          nan\n",
      " -0.65782847 -0.58350906         nan -0.49085573 -0.52924131         nan\n",
      "         nan         nan -0.60372454 -0.35390623 -0.48895309         nan\n",
      "         nan -0.5143585  -0.53146977         nan]  Accuracy: 0.5630434782608695  F1 Score: 0.7168758716875872 Precision training: 0.5854214123006833 Recall training: <function recall at 0x000002DEF8F891E0> ]\n",
      "TEST: [Iteracion:  0 Accuracy: 0.6293706293706294  F1 Score: 0.7725321888412017 Precision Test: 0.6293706293706294 Recall Test: 1.0 ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin VDK\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\Admin VDK\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#####################\n",
      "TRAINING: [Iteracion:  300  Costo:  [-0.60892043         nan -0.61098237         nan -0.65100969         nan\n",
      "         nan         nan -0.55181581         nan -0.75745283 -0.58216656\n",
      "         nan         nan -0.59338389         nan -0.62454969 -0.68388102\n",
      " -0.59833846 -0.59920918         nan -0.63982555 -0.66564383         nan\n",
      "         nan -0.65851681 -0.67429686         nan -0.53015172         nan\n",
      "         nan         nan         nan         nan -0.65048239         nan\n",
      " -0.62948545 -0.64470208 -0.59085006 -0.74491776         nan         nan\n",
      " -0.66564383         nan -0.60170715         nan -0.68698245 -0.58648576\n",
      " -0.61709401 -0.53640958 -0.57803552 -0.64470208 -0.64761599 -0.59559507\n",
      "         nan         nan         nan -0.69862598         nan -0.55043258\n",
      "         nan         nan         nan -0.54792629 -0.59085006 -0.61153133\n",
      " -0.68602225         nan -0.62637498 -0.60212136         nan -0.63090815\n",
      " -0.58020336         nan -0.61919293 -0.56197685 -0.57365793         nan\n",
      " -0.72912475 -0.71144196 -0.60126741 -0.72333199 -0.57878053 -0.67332708\n",
      "         nan -0.63223168         nan -0.54288269 -0.66997174 -0.61919293\n",
      "         nan         nan -0.54815939         nan         nan -0.63090815\n",
      "         nan -0.59588448 -0.60714088         nan -0.61883522 -0.6696707\n",
      "         nan -0.56380442         nan -0.62020526         nan         nan\n",
      "         nan -0.55181581 -0.6695606          nan         nan         nan\n",
      " -0.73353757 -0.68992072 -0.68590447         nan -0.61791558         nan\n",
      " -0.60798762 -0.57401782 -0.67870912 -0.73252998 -0.71189424 -0.55834281\n",
      "         nan -0.61153133         nan         nan         nan -0.55292562\n",
      " -0.71830105         nan -0.6704172          nan -0.79659179         nan\n",
      " -0.62385008 -0.66217246 -0.66147202 -0.65180242         nan -0.6463213\n",
      " -0.68992072         nan -0.65064835 -0.6696707          nan -0.52584777\n",
      " -0.64502031 -0.6419884  -0.60610342 -0.68602225         nan -0.62194753\n",
      " -0.68671848 -0.54792629         nan -0.59239471 -0.62948545 -0.62454969\n",
      " -0.5859366  -0.53015172 -0.73089597 -0.59776517 -0.60921603 -0.71081307\n",
      " -0.63190387 -0.57872079         nan         nan -0.68843065         nan\n",
      " -0.66557711 -0.62194753 -0.71847283 -0.63410814 -0.71144196         nan\n",
      " -0.75196632         nan         nan -0.62948545 -0.53015172 -0.66217246\n",
      " -0.59085006         nan -0.61888452 -0.65248315         nan -0.66997174\n",
      " -0.65285341 -0.53811592 -0.74491776         nan -0.72909684 -0.71830105\n",
      "         nan         nan         nan         nan -0.73200183 -0.62682791\n",
      " -0.67193745 -0.60352059         nan         nan -0.65392548         nan\n",
      " -0.73049322         nan         nan         nan -0.64103496         nan\n",
      " -0.59273729 -0.62151804 -0.63499724         nan         nan -0.70996913\n",
      "         nan         nan -0.68708288         nan -0.65447448         nan\n",
      "         nan         nan         nan         nan         nan -0.60610342\n",
      " -0.60663437 -0.64913485         nan         nan -0.63545918 -0.56497164\n",
      " -0.60714088         nan         nan -0.55625119 -0.67250062         nan\n",
      " -0.60204576         nan         nan         nan         nan -0.54288269\n",
      " -0.73252998         nan -0.63565571         nan -0.68843065         nan\n",
      " -0.59833846         nan -0.66418725 -0.65447448 -0.54993168 -0.60010575\n",
      " -0.56197685 -0.62482187 -0.68602225 -0.6695606          nan -0.56651797\n",
      "         nan -0.65180242 -0.54792629         nan         nan -0.68992072\n",
      " -0.66997174 -0.57008729 -0.61919293 -0.58216656         nan -0.69692777\n",
      " -0.59688821 -0.71145143 -0.6053728          nan         nan -0.71847283\n",
      " -0.71189424 -0.55956747         nan -0.60298666 -0.67649486         nan\n",
      "         nan         nan         nan -0.65116295 -0.63982555 -0.68698245\n",
      " -0.57872079 -0.68579274 -0.65352447 -0.54792629 -0.64304104 -0.65180242\n",
      "         nan -0.56819385 -0.58020336         nan -0.60298666         nan\n",
      "         nan -0.71830105         nan -0.59559507         nan         nan\n",
      " -0.62556811 -0.56793905         nan         nan -0.59845501 -0.63459235\n",
      " -0.63673472 -0.56819385 -0.73049322 -0.72234041 -0.61032353         nan\n",
      " -0.60798762         nan         nan         nan -0.60010575 -0.65180242\n",
      " -0.60212136 -0.57803552         nan -0.54688432 -0.57365793         nan\n",
      "         nan -0.55292562 -0.63223168 -0.6704172  -0.61791558 -0.73353757\n",
      " -0.7080405          nan -0.54815939 -0.59239471         nan         nan\n",
      "         nan -0.59239471 -0.57813895 -0.65447448 -0.63464026 -0.63673472\n",
      " -0.69356898         nan         nan -0.6422403          nan         nan\n",
      "         nan         nan -0.6463213  -0.58122608 -0.75196632 -0.62682791\n",
      " -0.6419884  -0.57872079         nan -0.66564383 -0.60126741 -0.58216656\n",
      " -0.62642988         nan         nan -0.75745283         nan         nan\n",
      "         nan         nan -0.60961035 -0.55994153         nan -0.5859366\n",
      " -0.62151804 -0.68843065 -0.66464964         nan -0.7080405          nan\n",
      "         nan -0.57008729 -0.62682791 -0.57813895 -0.63004928 -0.64502031\n",
      "         nan -0.63982555         nan -0.68329304 -0.56633371 -0.69692777\n",
      "         nan -0.66557711 -0.57878053 -0.60352059 -0.62151804 -0.65677566\n",
      "         nan -0.56380442         nan         nan -0.67870912         nan\n",
      " -0.58339824 -0.65064835         nan -0.60352059 -0.60170715 -0.61244987\n",
      " -0.57790325         nan         nan -0.57872079         nan         nan\n",
      "         nan         nan -0.6463213          nan -0.73353757         nan\n",
      " -0.65180242         nan -0.6422403  -0.58339824 -0.66418725         nan\n",
      " -0.5741516  -0.61079002         nan -0.68602225 -0.65485165         nan\n",
      "         nan         nan -0.60817645 -0.82822248 -0.68708288         nan\n",
      "         nan -0.66217311 -0.65392548         nan]  Accuracy: 0.85  F1 Score: 0.8738574040219379 Precision training: 0.8884758364312267 Recall training: <function recall at 0x000002DEF8F891E0> ]\n",
      "TEST: [Iteracion:  300 Accuracy: 0.7762237762237763  F1 Score: 0.7922077922077922 Precision Test: 0.953125 Recall Test: 0.6777777777777778 ]\n",
      "#####################\n",
      "TRAINING: [Iteracion:  600  Costo:  [-0.46605701         nan -0.46925347         nan -0.53950218         nan\n",
      "         nan         nan -0.37267466         nan -0.74336795 -0.42085631\n",
      "         nan         nan -0.43629545         nan -0.49463654 -0.59820638\n",
      " -0.4435515  -0.45105959         nan -0.51984041 -0.56816            nan\n",
      "         nan -0.5483255  -0.5848406          nan -0.33756383         nan\n",
      "         nan         nan         nan         nan -0.5368815          nan\n",
      " -0.50193336 -0.52798277 -0.43622073 -0.71356495         nan         nan\n",
      " -0.56816            nan -0.45524386         nan -0.61314293 -0.42684634\n",
      " -0.4797913  -0.3485624  -0.41367308 -0.52798277 -0.53063294 -0.44411845\n",
      "         nan         nan         nan -0.63523895         nan -0.36977572\n",
      "         nan         nan         nan -0.36855837 -0.43622073 -0.47033832\n",
      " -0.60258629         nan -0.49934505 -0.45653547         nan -0.50425968\n",
      " -0.41653765         nan -0.48355136 -0.3861524  -0.40809335         nan\n",
      " -0.68671515 -0.656045   -0.4570934  -0.67346684 -0.4137636  -0.57622908\n",
      "         nan -0.50595828         nan -0.35771932 -0.57795406 -0.48355136\n",
      "         nan         nan -0.36490585         nan         nan -0.50425968\n",
      "         nan -0.44463129 -0.4673662          nan -0.48317781 -0.57969772\n",
      "         nan -0.39022718         nan -0.48619169         nan         nan\n",
      "         nan -0.37267466 -0.57149211         nan         nan         nan\n",
      " -0.69425015 -0.60945471 -0.60246495         nan -0.48392237         nan\n",
      " -0.46362409 -0.40764231 -0.59253446 -0.69598473 -0.65789909 -0.38370131\n",
      "         nan -0.47033832         nan         nan         nan -0.37544891\n",
      " -0.66407263         nan -0.57666999         nan -0.8194626          nan\n",
      " -0.49047149 -0.55765278 -0.55702068 -0.54100394         nan -0.53006819\n",
      " -0.60945471         nan -0.54239377 -0.57969772         nan -0.33253611\n",
      " -0.52762924 -0.51914272 -0.46412706 -0.60258629         nan -0.49028719\n",
      " -0.60683888 -0.36855837         nan -0.43861202 -0.50193336 -0.49463654\n",
      " -0.43005283 -0.33756383 -0.69158497 -0.44593054 -0.46641683 -0.64898123\n",
      " -0.50416359 -0.41261348         nan         nan -0.60853915         nan\n",
      " -0.56404505 -0.49028719 -0.66592806 -0.50832713 -0.656045           nan\n",
      " -0.72678266         nan         nan -0.50193336 -0.33756383 -0.55765278\n",
      " -0.43622073         nan -0.4849114  -0.54676679         nan -0.57795406\n",
      " -0.54682063 -0.34831188 -0.71356495         nan -0.68975697 -0.66407263\n",
      "         nan         nan         nan         nan -0.69008023 -0.49692805\n",
      " -0.57607175 -0.45549796         nan         nan -0.54410227         nan\n",
      " -0.68545525         nan         nan         nan -0.51857287         nan\n",
      " -0.43580901 -0.48371346 -0.5103906          nan         nan -0.65012476\n",
      "         nan         nan -0.6050781          nan -0.54224647         nan\n",
      "         nan         nan         nan         nan         nan -0.46412706\n",
      " -0.46098805 -0.53684249         nan         nan -0.51518098 -0.3904552\n",
      " -0.4673662          nan         nan -0.38244263 -0.57685658         nan\n",
      " -0.45207357         nan         nan         nan         nan -0.35771932\n",
      " -0.69598473         nan -0.51520161         nan -0.60853915         nan\n",
      " -0.4435515          nan -0.5594443  -0.54224647 -0.36891242 -0.45212421\n",
      " -0.3861524  -0.49361531 -0.60258629 -0.57149211         nan -0.39736452\n",
      "         nan -0.54100394 -0.36855837         nan         nan -0.60945471\n",
      " -0.57795406 -0.40220609 -0.48355136 -0.42085631         nan -0.62265149\n",
      " -0.44589878 -0.65564628 -0.46156612         nan         nan -0.66592806\n",
      " -0.65789909 -0.38516454         nan -0.45744786 -0.58567022         nan\n",
      "         nan         nan         nan -0.54094668 -0.51984041 -0.61314293\n",
      " -0.41261348 -0.60307776 -0.54442173 -0.36855837 -0.52654036 -0.54100394\n",
      "         nan -0.39846804 -0.41653765         nan -0.45744786         nan\n",
      "         nan -0.66407263         nan -0.44411845         nan         nan\n",
      " -0.49682087 -0.39716277         nan         nan -0.44633931 -0.51050294\n",
      " -0.518577   -0.39846804 -0.68545525 -0.67171236 -0.4710039          nan\n",
      " -0.46362409         nan         nan         nan -0.45212421 -0.54100394\n",
      " -0.45653547 -0.41367308         nan -0.36139157 -0.40809335         nan\n",
      "         nan -0.37544891 -0.50595828 -0.57666999 -0.48392237 -0.69425015\n",
      " -0.64672911         nan -0.36490585 -0.43861202         nan         nan\n",
      "         nan -0.43861202 -0.41500045 -0.54224647 -0.50959508 -0.518577\n",
      " -0.61503198         nan         nan -0.52206624         nan         nan\n",
      "         nan         nan -0.53006819 -0.42183608 -0.72678266 -0.49692805\n",
      " -0.51914272 -0.41261348         nan -0.56816    -0.4570934  -0.42085631\n",
      " -0.49565559         nan         nan -0.74336795         nan         nan\n",
      "         nan         nan -0.46597275 -0.38629866         nan -0.43005283\n",
      " -0.48371346 -0.60853915 -0.56694845         nan -0.64672911         nan\n",
      "         nan -0.40220609 -0.49692805 -0.41500045 -0.50561689 -0.52762924\n",
      "         nan -0.51984041         nan -0.59723405 -0.39532893 -0.62265149\n",
      "         nan -0.56404505 -0.4137636  -0.45549796 -0.48371346 -0.54802488\n",
      "         nan -0.39022718         nan         nan -0.59253446         nan\n",
      " -0.42401179 -0.54239377         nan -0.45549796 -0.45524386 -0.47420852\n",
      " -0.41362094         nan         nan -0.41261348         nan         nan\n",
      "         nan         nan -0.53006819         nan -0.69425015         nan\n",
      " -0.54100394         nan -0.52206624 -0.42401179 -0.5594443          nan\n",
      " -0.40775989 -0.47428588         nan -0.60258629 -0.54677905         nan\n",
      "         nan         nan -0.46219676 -0.88485326 -0.6050781          nan\n",
      "         nan -0.56244901 -0.54410227         nan]  Accuracy: 0.8804347826086957  F1 Score: 0.905982905982906 Precision training: 0.8631921824104235 Recall training: <function recall at 0x000002DEF8F891E0> ]\n",
      "TEST: [Iteracion:  600 Accuracy: 0.8671328671328671  F1 Score: 0.8950276243093923 Precision Test: 0.8901098901098901 Recall Test: 0.9 ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#####################\n",
      "TRAINING: [Iteracion:  900  Costo:  [-0.37826401         nan -0.38198254         nan -0.46847854         nan\n",
      "         nan         nan -0.27164073         nan -0.74153375 -0.32526872\n",
      "         nan         nan -0.34213795         nan -0.41334458 -0.54348774\n",
      " -0.35025818 -0.36091974         nan -0.44391302 -0.50576874         nan\n",
      "         nan -0.47804262 -0.52755839         nan -0.23382731         nan\n",
      "         nan         nan         nan         nan -0.46459323         nan\n",
      " -0.42185384 -0.45392111 -0.34330078 -0.69821289         nan         nan\n",
      " -0.50576874         nan -0.36585921         nan -0.56638344 -0.33179331\n",
      " -0.39467747 -0.24574439 -0.31694434 -0.45392111 -0.4563603  -0.35250052\n",
      "         nan         nan         nan -0.59586278         nan -0.26829437\n",
      "         nan         nan         nan -0.26772263 -0.34330078 -0.38332773\n",
      " -0.54936381         nan -0.41957717 -0.36753815         nan -0.42466128\n",
      " -0.32000301         nan -0.39926684 -0.28567494 -0.31107131         nan\n",
      " -0.66285767 -0.6224792  -0.3688181  -0.6440537  -0.31672737 -0.5140541\n",
      "         nan -0.42654666         nan -0.25524681 -0.519051   -0.39926684\n",
      "         nan         nan -0.26267266         nan         nan -0.42466128\n",
      "         nan -0.35311251 -0.38122003         nan -0.39888553 -0.52207194\n",
      "         nan -0.29049544         nan -0.40273659         nan         nan\n",
      "         nan -0.27164073 -0.50869945         nan         nan         nan\n",
      " -0.67274775 -0.55823849 -0.5492282          nan -0.40056518         nan\n",
      " -0.37510591 -0.31026201 -0.5374433  -0.67651951 -0.62538289 -0.28389968\n",
      "         nan -0.38332773         nan         nan         nan -0.27496403\n",
      " -0.63144323         nan -0.51664779         nan -0.84789008         nan\n",
      " -0.40729243 -0.49080002 -0.49023052 -0.47039317         nan -0.45624818\n",
      " -0.55823849         nan -0.47329145 -0.52207194         nan -0.22900459\n",
      " -0.45314718 -0.44154331 -0.37686656 -0.54936381         nan -0.40807644\n",
      " -0.55597359 -0.26772263         nan -0.34600634 -0.42185384 -0.41334458\n",
      " -0.336725   -0.23382731 -0.670042   -0.3540701  -0.3786629  -0.61083015\n",
      " -0.4239439  -0.31510128         nan         nan -0.55769135         nan\n",
      " -0.49907733 -0.40807644 -0.63449082 -0.42916755 -0.6224792          nan\n",
      " -0.71623784         nan         nan -0.42185384 -0.23382731 -0.49080002\n",
      " -0.34330078         nan -0.40152524 -0.47926721         nan -0.519051\n",
      " -0.47906583 -0.24472115 -0.69821289         nan -0.66818874 -0.63144323\n",
      "         nan         nan         nan         nan -0.66661821 -0.41560931\n",
      " -0.51468212 -0.36529847         nan         nan -0.47407362         nan\n",
      " -0.6596653          nan         nan         nan -0.4412014          nan\n",
      " -0.34173414 -0.39822605 -0.43187606         nan         nan -0.61338482\n",
      "         nan         nan -0.55281187         nan -0.47076638         nan\n",
      "         nan         nan         nan         nan         nan -0.37686656\n",
      " -0.37184947 -0.46537761         nan         nan -0.43914934 -0.29028104\n",
      " -0.38122003         nan         nan -0.28312517 -0.51560903         nan\n",
      " -0.36096532         nan         nan         nan         nan -0.25524681\n",
      " -0.67651951         nan -0.43906959         nan -0.55769135         nan\n",
      " -0.35025818         nan -0.492481   -0.47076638 -0.26734148 -0.3620645\n",
      " -0.28567494 -0.41162343 -0.54936381 -0.50869945         nan -0.29921577\n",
      "         nan -0.47039317 -0.26772263         nan         nan -0.55823849\n",
      " -0.519051   -0.30440841 -0.39926684 -0.32526872         nan -0.57567865\n",
      " -0.35447902 -0.62181396 -0.37340735         nan         nan -0.63449082\n",
      " -0.62538289 -0.28537302         nan -0.36848668 -0.52755835         nan\n",
      "         nan         nan         nan -0.47067999 -0.44391302 -0.56638344\n",
      " -0.31510128 -0.55034239 -0.47484304 -0.26772263 -0.45261491 -0.47039317\n",
      "         nan -0.30001242 -0.32000301         nan -0.36848668         nan\n",
      "         nan -0.63144323         nan -0.35250052         nan         nan\n",
      " -0.41612874 -0.29830367         nan         nan -0.35431321 -0.43228629\n",
      " -0.44374344 -0.30001242 -0.6596653  -0.6417209  -0.38500448         nan\n",
      " -0.37510591         nan         nan         nan -0.3620645  -0.47039317\n",
      " -0.36753815 -0.31694434         nan -0.25848751 -0.31107131         nan\n",
      "         nan -0.27496403 -0.42654666 -0.51664779 -0.40056518 -0.67274775\n",
      " -0.60891953         nan -0.26267266 -0.34600634         nan         nan\n",
      "         nan -0.34600634 -0.31880326 -0.47076638 -0.43083218 -0.44374344\n",
      " -0.56512929         nan         nan -0.44599688         nan         nan\n",
      "         nan         nan -0.45624818 -0.32712841 -0.71623784 -0.41560931\n",
      " -0.44154331 -0.31510128         nan -0.50576874 -0.3688181  -0.32526872\n",
      " -0.41384298         nan         nan -0.74153375         nan         nan\n",
      "         nan         nan -0.37777098 -0.28675912         nan -0.336725\n",
      " -0.39822605 -0.55769135 -0.50439109         nan -0.60891953         nan\n",
      "         nan -0.30440841 -0.41560931 -0.31880326 -0.42728795 -0.45314718\n",
      "         nan -0.44391302         nan -0.54225483 -0.29647143 -0.57567865\n",
      "         nan -0.49907733 -0.31672737 -0.36529847 -0.39822605 -0.47862452\n",
      "         nan -0.29049544         nan         nan -0.5374433          nan\n",
      " -0.3292153  -0.47329145         nan -0.36529847 -0.36585921 -0.38870751\n",
      " -0.31694087         nan         nan -0.31510128         nan         nan\n",
      "         nan         nan -0.45624818         nan -0.67274775         nan\n",
      " -0.47039317         nan -0.44599688 -0.3292153  -0.492481           nan\n",
      " -0.31037468 -0.38970926         nan -0.54936381 -0.47778705         nan\n",
      "         nan         nan -0.37285948 -0.94202845 -0.55281187         nan\n",
      "         nan -0.49861563 -0.47407362         nan]  Accuracy: 0.8804347826086957  F1 Score: 0.907563025210084 Precision training: 0.8517350157728707 Recall training: <function recall at 0x000002DEF8F891E0> ]\n",
      "TEST: [Iteracion:  900 Accuracy: 0.8951048951048951  F1 Score: 0.9206349206349207 Precision Test: 0.8787878787878788 Recall Test: 0.9666666666666667 ]\n",
      "#####################\n",
      "TRAINING: [Iteracion:  1200  Costo:  [-0.32046427         nan -0.3244419          nan -0.42079123         nan\n",
      "         nan         nan -0.20966162         nan -0.74887741 -0.26412502\n",
      "         nan         nan -0.28135375         nan -0.35915606 -0.50748504\n",
      " -0.28976302 -0.30201575         nan -0.39300376 -0.46410212         nan\n",
      "         nan -0.43086169 -0.48955621         nan -0.17269126         nan\n",
      "         nan         nan         nan         nan -0.4160485          nan\n",
      " -0.36836851 -0.4042271  -0.28317926 -0.69400738         nan         nan\n",
      " -0.46410212         nan -0.30731367         nan -0.53634425 -0.27075167\n",
      " -0.33829278 -0.18428894 -0.25542294 -0.4042271  -0.40650177 -0.29291232\n",
      "         nan         nan         nan -0.57166934         nan -0.20625902\n",
      "         nan         nan         nan -0.20602928 -0.28317926 -0.32592478\n",
      " -0.51448605         nan -0.36627462 -0.30916186         nan -0.37146508\n",
      " -0.25849244         nan -0.34333915 -0.22334635 -0.24957693         nan\n",
      " -0.65144139 -0.60300939 -0.31083767 -0.6280657  -0.25505686 -0.47256315\n",
      "         nan -0.37345736         nan -0.19341103 -0.47988983 -0.34333915\n",
      "         nan         nan -0.2005543          nan         nan -0.37146508\n",
      "         nan -0.29357119 -0.32437783         nan -0.34295083 -0.48378753\n",
      "         nan -0.22833702         nan -0.34727028         nan         nan\n",
      "         nan -0.20966162 -0.46674624         nan         nan         nan\n",
      " -0.66329083 -0.52488977 -0.51432852         nan -0.34521133         nan\n",
      " -0.31692908 -0.2486066  -0.50110021 -0.66871679 -0.60678562 -0.22197605\n",
      "         nan -0.32592478         nan         nan         nan -0.21311185\n",
      " -0.61282433         nan -0.47668134         nan -0.88192644         nan\n",
      " -0.35196464 -0.44594656 -0.44543338 -0.42297643         nan -0.40669616\n",
      " -0.52488977         nan -0.42685495 -0.48378753         nan -0.16828726\n",
      " -0.40315787 -0.38956093 -0.31940473 -0.51448605         nan -0.35334183\n",
      " -0.52283197 -0.20602928         nan -0.28598296 -0.36836851 -0.35915606\n",
      " -0.27650866 -0.17269126 -0.66052613 -0.29431739 -0.320893   -0.58775097\n",
      " -0.37036233 -0.25321744         nan         nan -0.52459431         nan\n",
      " -0.45557731 -0.35334183 -0.61679891 -0.37622985 -0.60300939         nan\n",
      " -0.71613636         nan         nan -0.36836851 -0.17269126 -0.44594656\n",
      " -0.28317926         nan -0.34612856 -0.43398284         nan -0.47988983\n",
      " -0.43355436 -0.18297246 -0.69400738         nan -0.65864524 -0.61282433\n",
      "         nan         nan         nan         nan -0.65556075 -0.36139494\n",
      " -0.47374002 -0.30629862         nan         nan -0.42705775         nan\n",
      " -0.64669284         nan         nan         nan -0.38936295         nan\n",
      " -0.28099132 -0.34157179 -0.37933348         nan         nan -0.59141007\n",
      "         nan         nan -0.51865005         nan -0.42277858         nan\n",
      "         nan         nan         nan         nan         nan -0.31940473\n",
      " -0.31334355 -0.4174071          nan         nan -0.38819489 -0.22790445\n",
      " -0.32437783         nan         nan -0.22150142 -0.47477226         nan\n",
      " -0.30151358         nan         nan         nan         nan -0.19341103\n",
      " -0.66871679         nan -0.3880527          nan -0.52459431         nan\n",
      " -0.28976302         nan -0.44758986 -0.42277858 -0.20531207 -0.30320962\n",
      " -0.22334635 -0.35702149 -0.51448605 -0.46674624         nan -0.23751803\n",
      "         nan -0.42297643 -0.20602928         nan         nan -0.52488977\n",
      " -0.47988983 -0.24273177 -0.34333915 -0.26412502         nan -0.54559936\n",
      " -0.29497381 -0.60215733 -0.31547029         nan         nan -0.61679891\n",
      " -0.60678562 -0.22339            nan -0.3101492  -0.48902278         nan\n",
      "         nan         nan         nan -0.42348293 -0.39300376 -0.53634425\n",
      " -0.25321744 -0.51583046 -0.42814036 -0.20602928 -0.40300332 -0.42297643\n",
      "         nan -0.23814337 -0.25849244         nan -0.3101492          nan\n",
      "         nan -0.61282433         nan -0.29291232         nan         nan\n",
      " -0.36227775 -0.23629499         nan         nan -0.29445605 -0.37993183\n",
      " -0.39354431 -0.23814337 -0.64669284 -0.62524546 -0.32819764         nan\n",
      " -0.31692908         nan         nan         nan -0.30320962 -0.42297643\n",
      " -0.30916186 -0.25542294         nan -0.19627183 -0.24957693         nan\n",
      "         nan -0.21311185 -0.37345736 -0.47668134 -0.34521133 -0.66329083\n",
      " -0.58607359         nan -0.2005543  -0.28598296         nan         nan\n",
      "         nan -0.28598296 -0.25751695 -0.42277858 -0.37813053 -0.39354431\n",
      " -0.5327941          nan         nan -0.39498714         nan         nan\n",
      "         nan         nan -0.40669616 -0.26640045 -0.71613636 -0.36139494\n",
      " -0.38956093 -0.25321744         nan -0.46410212 -0.31083767 -0.26412502\n",
      " -0.35931365         nan         nan -0.74887741         nan         nan\n",
      "         nan         nan -0.31973885 -0.22483154         nan -0.27650866\n",
      " -0.34157179 -0.52459431 -0.46257553         nan -0.58607359         nan\n",
      "         nan -0.24273177 -0.36139494 -0.25751695 -0.37489616 -0.40315787\n",
      "         nan -0.39300376         nan -0.50605131 -0.23453207 -0.54559936\n",
      "         nan -0.45557731 -0.25505686 -0.30629862 -0.34157179 -0.43203455\n",
      "         nan -0.22833702         nan         nan -0.50110021         nan\n",
      " -0.26838787 -0.42685495         nan -0.30629862 -0.30731367 -0.33214938\n",
      " -0.25545172         nan         nan -0.25321744         nan         nan\n",
      "         nan         nan -0.40669616         nan -0.66329083         nan\n",
      " -0.42297643         nan -0.39498714 -0.26838787 -0.44758986         nan\n",
      " -0.24872005 -0.33367791         nan -0.51448605 -0.43145604         nan\n",
      "         nan         nan -0.31423299 -1.00199361 -0.51865005         nan\n",
      "         nan -0.45588667 -0.42705775         nan]  Accuracy: 0.8804347826086957  F1 Score: 0.907563025210084 Precision training: 0.8517350157728707 Recall training: <function recall at 0x000002DEF8F891E0> ]\n",
      "TEST: [Iteracion:  1200 Accuracy: 0.8951048951048951  F1 Score: 0.9206349206349207 Precision Test: 0.8787878787878788 Recall Test: 0.9666666666666667 ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#####################\n",
      "TRAINING: [Iteracion:  1500  Costo:  [-0.27956129         nan -0.28366434         nan -0.38634978         nan\n",
      "         nan         nan -0.16861395         nan -0.76047335 -0.22202745\n",
      "         nan         nan -0.23913824         nan -0.32033298 -0.48195877\n",
      " -0.24757276 -0.26063241         nan -0.35630511 -0.43413841         nan\n",
      "         nan -0.39678616 -0.46239933         nan -0.13365674         nan\n",
      "         nan         nan         nan         nan -0.38098754         nan\n",
      " -0.32997264 -0.36837038 -0.24133504 -0.69539069         nan         nan\n",
      " -0.43413841         nan -0.26608956         nan -0.51556867 -0.22857129\n",
      " -0.29814696 -0.14454268 -0.21329295 -0.36837038 -0.37051288 -0.25123545\n",
      "         nan         nan         nan -0.55575285         nan -0.16530585\n",
      "         nan         nan         nan -0.16526894 -0.24133504 -0.28522143\n",
      " -0.48985686         nan -0.3279986  -0.2680057          nan -0.33325291\n",
      " -0.216298           nan -0.3034538  -0.18164189 -0.20760248         nan\n",
      " -0.64649412 -0.59110826 -0.26990966 -0.61918982 -0.21285269 -0.44274535\n",
      "         nan -0.33530868         nan -0.15307093 -0.4518456  -0.3034538\n",
      "         nan         nan -0.15977443         nan         nan -0.33325291\n",
      "         nan -0.25191413 -0.28406279         nan -0.30306005 -0.45639349\n",
      "         nan -0.18656639         nan -0.30765277         nan         nan\n",
      "         nan -0.16861395 -0.43656481         nan         nan         nan\n",
      " -0.66004296 -0.50149308 -0.4896753          nan -0.30570237         nan\n",
      " -0.27582673 -0.20656378 -0.47526947 -0.66687984 -0.59563721 -0.18053565\n",
      "         nan -0.28522143         nan         nan         nan -0.17201736\n",
      " -0.60169949         nan -0.4480192          nan -0.91763975         nan\n",
      " -0.31241262 -0.41356259 -0.41309899 -0.38872491         nan -0.37092867\n",
      " -0.50149308         nan -0.39329125 -0.45639349         nan -0.12968859\n",
      " -0.36708305 -0.35212426 -0.27873292 -0.48985686         nan -0.31417855\n",
      " -0.49956759 -0.16526894         nan -0.2441418  -0.32997264 -0.32033298\n",
      " -0.23471492 -0.13365674 -0.65721175 -0.25251949 -0.28001162 -0.57285077\n",
      " -0.33189386 -0.21092709         nan         nan -0.50138388         nan\n",
      " -0.42422348 -0.31417855 -0.60643866 -0.33817586 -0.59110826         nan\n",
      " -0.72114401         nan         nan -0.32997264 -0.13365674 -0.41356259\n",
      " -0.24133504         nan -0.30657519 -0.40129131         nan -0.4518456\n",
      " -0.40066758 -0.14313647 -0.69539069         nan -0.65530388 -0.60169949\n",
      "         nan         nan         nan         nan -0.65094152 -0.32254568\n",
      " -0.44433365 -0.26480513         nan         nan -0.39310021         nan\n",
      " -0.64042782         nan         nan         nan -0.35202483         nan\n",
      " -0.2387997  -0.30121792 -0.34153774         nan         nan -0.57742374\n",
      "         nan         nan -0.49459037         nan -0.38812366         nan\n",
      "         nan         nan         nan         nan         nan -0.27873292\n",
      " -0.27206519 -0.38277139         nan         nan -0.35148114 -0.18602403\n",
      " -0.28406279         nan         nan -0.18025448 -0.44544782         nan\n",
      " -0.25979912         nan         nan         nan         nan -0.15307093\n",
      " -0.66687984         nan -0.35129833         nan -0.50138388         nan\n",
      " -0.24757276         nan -0.41520014 -0.38812366 -0.16439885 -0.26185323\n",
      " -0.18164189 -0.31793809 -0.48985686 -0.43656481         nan -0.19573141\n",
      "         nan -0.38872491 -0.16526894         nan         nan -0.50149308\n",
      " -0.4518456  -0.20082775 -0.3034538  -0.22202745         nan -0.52486306\n",
      " -0.25332199 -0.5901154  -0.27453994         nan         nan -0.60643866\n",
      " -0.59563721 -0.18187022         nan -0.26902352 -0.46149237         nan\n",
      "         nan         nan         nan -0.38938385 -0.35630511 -0.51556867\n",
      " -0.21092709 -0.49149273 -0.39441396 -0.16526894 -0.36720211 -0.38872491\n",
      "         nan -0.19625063 -0.216298           nan -0.26902352         nan\n",
      "         nan -0.60169949         nan -0.25123545         nan         nan\n",
      " -0.32365423 -0.19437723         nan         nan -0.25258765 -0.34226301\n",
      " -0.35734056 -0.19625063 -0.64042782 -0.61594475 -0.28786608         nan\n",
      " -0.27582673         nan         nan         nan -0.26185323 -0.38872491\n",
      " -0.2680057  -0.21329295         nan -0.15561335 -0.20760248         nan\n",
      "         nan -0.17201736 -0.33530868 -0.4480192  -0.30570237 -0.66004296\n",
      " -0.57134572         nan -0.15977443 -0.2441418          nan         nan\n",
      "         nan -0.2441418  -0.21548062 -0.38812366 -0.34022688 -0.35734056\n",
      " -0.51022163         nan         nan -0.35821225         nan         nan\n",
      "         nan         nan -0.37092867 -0.22449713 -0.72114401 -0.32254568\n",
      " -0.35212426 -0.21092709         nan -0.43413841 -0.26990966 -0.22202745\n",
      " -0.32025275         nan         nan -0.76047335         nan         nan\n",
      "         nan         nan -0.27869591 -0.18328956         nan -0.23471492\n",
      " -0.30121792 -0.50138388 -0.43248071         nan -0.57134572         nan\n",
      "         nan -0.20082775 -0.32254568 -0.21548062 -0.33722718 -0.36708305\n",
      "         nan -0.35630511         nan -0.48036251 -0.19270412 -0.52486306\n",
      "         nan -0.42422348 -0.21285269 -0.26480513 -0.30121792 -0.39838435\n",
      "         nan -0.18656639         nan         nan -0.47526947         nan\n",
      " -0.22638721 -0.39329125         nan -0.26480513 -0.26608956 -0.29193951\n",
      " -0.21334332         nan         nan -0.21092709         nan         nan\n",
      "         nan         nan -0.37092867         nan -0.66004296         nan\n",
      " -0.38872491         nan -0.35821225 -0.22638721 -0.41520014         nan\n",
      " -0.20667875 -0.29378861         nan -0.48985686 -0.3979841          nan\n",
      "         nan         nan -0.27287373 -1.06144222 -0.49459037         nan\n",
      "         nan -0.42509647 -0.39310021         nan]  Accuracy: 0.8804347826086957  F1 Score: 0.907563025210084 Precision training: 0.8517350157728707 Recall training: <function recall at 0x000002DEF8F891E0> ]\n",
      "TEST: [Iteracion:  1500 Accuracy: 0.8951048951048951  F1 Score: 0.9206349206349207 Precision Test: 0.8787878787878788 Recall Test: 0.9666666666666667 ]\n",
      "#####################\n",
      "TRAINING: [Iteracion:  1800  Costo:  [-0.24903816         nan -0.25319367         nan -0.36011549         nan\n",
      "         nan         nan -0.13980181         nan -0.77402976 -0.19141025\n",
      "         nan         nan -0.20818551         nan -0.29101361 -0.46282868\n",
      " -0.21651906 -0.22996334         nan -0.32841581 -0.41138826         nan\n",
      "         nan -0.37082622 -0.44189831         nan -0.10714589         nan\n",
      "         nan         nan         nan         nan -0.35428469         nan\n",
      " -0.30091835 -0.34109035 -0.2105944  -0.69972472         nan         nan\n",
      " -0.41138826         nan -0.23547301         nan -0.50038253 -0.19779166\n",
      " -0.2680125  -0.11723462 -0.18280484 -0.34109035 -0.34312362 -0.22047793\n",
      "         nan         nan         nan -0.5447366          nan -0.13664322\n",
      "         nan         nan         nan -0.13672022 -0.2105944  -0.25478961\n",
      " -0.47147101         nan -0.29903025 -0.23740653         nan -0.3043195\n",
      " -0.18571539         nan -0.27346777 -0.1520949  -0.17731844         nan\n",
      " -0.64514831 -0.5836041  -0.23944536 -0.61437455 -0.18232857 -0.42011776\n",
      "         nan -0.30641317         nan -0.12513849 -0.430633   -0.27346777\n",
      "         nan         nan -0.13137728         nan         nan -0.3043195\n",
      "         nan -0.22116164 -0.25391364         nan -0.27307005 -0.43568994\n",
      "         nan -0.1568567          nan -0.27782429         nan         nan\n",
      "         nan -0.13980181 -0.41364133         nan         nan         nan\n",
      " -0.66020957 -0.48413854 -0.47126542         nan -0.27597382         nan\n",
      " -0.24520197 -0.17625764 -0.45586823 -0.66828772 -0.58879845 -0.1511725\n",
      "         nan -0.25478961         nan         nan         nan -0.14308767\n",
      " -0.59490736         nan -0.42631075         nan -0.95329897         nan\n",
      " -0.28260806 -0.38889335 -0.38847352 -0.36263055         nan -0.34370711\n",
      " -0.48413854         nan -0.36770717 -0.43568994         nan -0.10357465\n",
      " -0.33963576 -0.32370106 -0.24837724 -0.47147101         nan -0.28464171\n",
      " -0.48230301 -0.13672022         nan -0.21336067 -0.30091835 -0.29101361\n",
      " -0.2040935  -0.10714589 -0.65730934 -0.22166768 -0.24950383 -0.56276581\n",
      " -0.30278102 -0.18038164         nan         nan -0.48417535         nan\n",
      " -0.40037118 -0.28464171 -0.60030156 -0.30934027 -0.5836041          nan\n",
      " -0.72874605         nan         nan -0.30091835 -0.10714589 -0.38889335\n",
      " -0.2105944          nan -0.27680441 -0.37639269         nan -0.430633\n",
      " -0.37560179 -0.1158284  -0.69972472         nan -0.65537651 -0.59490736\n",
      "         nan         nan         nan         nan -0.64989748 -0.29319977\n",
      " -0.4220302  -0.23402214         nan         nan -0.3672312          nan\n",
      " -0.63792987         nan         nan         nan -0.32367335         nan\n",
      " -0.20786264 -0.27091501 -0.31287877         nan         nan -0.56812302\n",
      "         nan         nan -0.47667641         nan -0.36172878         nan\n",
      "         nan         nan         nan         nan         nan -0.24837724\n",
      " -0.24135052 -0.35639766         nan         nan -0.32359357 -0.15626043\n",
      " -0.25391364         nan         nan -0.15102084 -0.42321025         nan\n",
      " -0.22892006         nan         nan         nan         nan -0.12513849\n",
      " -0.66828772         nan -0.32338367         nan -0.48417535         nan\n",
      " -0.21651906         nan -0.39053976 -0.36172878 -0.1357867  -0.23119624\n",
      " -0.1520949  -0.28844745 -0.47147101 -0.41364133         nan -0.16579962\n",
      "         nan -0.36263055 -0.13672022         nan         nan -0.48413854\n",
      " -0.430633   -0.17072254 -0.27346777 -0.19141025         nan -0.50975307\n",
      " -0.22255895 -0.58249985 -0.24404546         nan         nan -0.60030156\n",
      " -0.58879845 -0.15242561         nan -0.23844576 -0.44071284         nan\n",
      "         nan         nan         nan -0.363402   -0.32841581 -0.50038253\n",
      " -0.18038164 -0.47334866 -0.36872296 -0.13672022 -0.33996284 -0.36263055\n",
      "         nan -0.1662475  -0.18571539         nan -0.23844576         nan\n",
      "         nan -0.59490736         nan -0.22047793         nan         nan\n",
      " -0.29445546 -0.16440069         nan         nan -0.22168567 -0.31369391\n",
      " -0.32981466 -0.1662475  -0.63792987 -0.6107511  -0.25767516         nan\n",
      " -0.24520197         nan         nan         nan -0.23119624 -0.36263055\n",
      " -0.23740653 -0.18280484         nan -0.1274164  -0.17731844         nan\n",
      "         nan -0.14308767 -0.30641317 -0.42631075 -0.27597382 -0.66020957\n",
      " -0.56139559         nan -0.13137728 -0.21336067         nan         nan\n",
      "         nan -0.21336067 -0.18501487 -0.36172878 -0.31149168 -0.32981466\n",
      " -0.49356098         nan         nan -0.33026197         nan         nan\n",
      "         nan         nan -0.34370711 -0.19396098 -0.72874605 -0.29319977\n",
      " -0.32370106 -0.18038164         nan -0.41138826 -0.23944536 -0.19141025\n",
      " -0.29075919         nan         nan -0.77402976         nan         nan\n",
      "         nan         nan -0.2480853  -0.15379235         nan -0.2040935\n",
      " -0.27091501 -0.48417535 -0.40961513         nan -0.56139559         nan\n",
      "         nan -0.17072254 -0.29319977 -0.18501487 -0.30867787 -0.33963576\n",
      "         nan -0.32841581         nan -0.46109641 -0.16282001 -0.50975307\n",
      "         nan -0.40037118 -0.18232857 -0.23402214 -0.27091501 -0.37274696\n",
      "         nan -0.1568567          nan         nan -0.45586823         nan\n",
      " -0.19576013 -0.36770717         nan -0.23402214 -0.23547301 -0.26180073\n",
      " -0.18286993         nan         nan -0.18038164         nan         nan\n",
      "         nan         nan -0.34370711         nan -0.66020957         nan\n",
      " -0.36263055         nan -0.33026197 -0.19576013 -0.39053976         nan\n",
      " -0.17637365 -0.26385394         nan -0.47147101 -0.37247718         nan\n",
      "         nan         nan -0.24210049 -1.11901455 -0.47667641         nan\n",
      "         nan -0.40167838 -0.3672312          nan]  Accuracy: 0.8804347826086957  F1 Score: 0.907563025210084 Precision training: 0.8517350157728707 Recall training: <function recall at 0x000002DEF8F891E0> ]\n",
      "TEST: [Iteracion:  1800 Accuracy: 0.8951048951048951  F1 Score: 0.9206349206349207 Precision Test: 0.8787878787878788 Recall Test: 0.9666666666666667 ]\n"
     ]
    }
   ],
   "source": [
    "modelo_1_theta_values, cost_vect, acc_vect, acc_vect_test = \\\n",
    "gradient_descent(X_train_features, Y_train_m1, theta_vector, alpha, iterations, X_test_features, y_test, 'modelo_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2def97b31d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Funcion para graficar el costo y  accuracy durante cada iteracion ##\n",
    "helper.training_graph(cost_vect, acc_vect, acc_vect_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy y Matriz de confusion Training Set\n",
    "\n",
    "![alt text](https://rasbt.github.io/mlxtend/user_guide/evaluate/confusion_matrix_files/confusion_matrix_1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 87.17391304347825 %\n",
      "\n",
      "Matriz de confusion\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[130,  49],\n",
       "       [ 10, 271]], dtype=int64)"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat = get_hipotesis(X_train_features, modelo_1_theta_values)\n",
    "y_predict = np.array([1 if (y > 0.5) else 0 for y in y_hat])\n",
    "\n",
    "print('Accuracy:', accuracy_score(Y_train_m1,y_predict) * 100 ,'%')\n",
    "print()\n",
    "print('Matriz de confusion')\n",
    "confusion_matrix(Y_train_m1, y_predict )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy con el Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = prediccion(X_test_features, modelo_1_theta_values, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 87.41258741258741 %\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy:',accuracy_score(y_test, y_predict)* 100 ,'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Matriz de confusion Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[38, 15],\n",
       "       [ 3, 87]], dtype=int64)"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision:  0.8529411764705882\n",
      "recall:  0.9666666666666667\n"
     ]
    }
   ],
   "source": [
    "print('precision: ',precision_score(y_test, y_predict))\n",
    "\n",
    "print('recall: ', recall_score(y_test, y_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento modelo 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sampleo bootstrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Ingresa en la lista el nombre de las fatrues que quieres utlizar en este modelo (~1 linea)\n",
    "'''\n",
    "X_train_m2, Y_train_m2 = resample(X_train, y_train, n_samples = 400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtrar Features\n",
    "Solo seleccionar los features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_filtro = ['mean perimeter', 'mean area','mean smoothness']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_m2 = helper.fitrar_nombre(X_train_m2,data.feature_names,features_filtro)\n",
    "X_test_features = helper.fitrar_nombre(X_test,data.feature_names,features_filtro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Numero de iteracions ##\n",
    "iterations = 1000  # Define el numero de iteraciones #\n",
    "\n",
    "## Alpha ##\n",
    "alpha = 0.05 # Define el valor de alpha #\n",
    "\n",
    "#### No hay que cambiar el codigo despues de esta linea #####\n",
    "## Numero de feautres ##\n",
    "numero_features = X_train_m2.shape[1] + 1\n",
    "\n",
    "## Creamos un vector donde se almacenan los valores de Theta,\n",
    "## lo inicializamos con numeros aleatorios\n",
    "theta_vector = np.random.rand(numero_features)\n",
    "\n",
    "## Normaliza las features de X_train_features (~1 linea) ##\n",
    "X_train_m2 = normalizar(X_train_m2)\n",
    "\n",
    "# Creando la Matriz X de features, \n",
    "# utilizamos np.ones para agregar el valor constante '1' que es el bias o feature 0\n",
    "X_train_features = np.ones((X_train_m2.shape[0], numero_features))\n",
    "X_train_features[:,:-1] = X_train_m2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin VDK\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:43: RuntimeWarning: invalid value encountered in log\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#####################\n",
      "TRAINING: [Iteracion:  0  Costo:  [-0.48696167 -0.48517485         nan         nan         nan -0.38478495\n",
      " -0.23526599 -0.29127318         nan -0.35275959 -0.52879676         nan\n",
      " -0.36112488         nan -0.49489123 -0.4342206  -0.27431826 -0.54156875\n",
      " -0.32555762 -0.43847276 -0.32994088 -0.30569257 -0.35831221 -0.32463336\n",
      " -0.53052897 -0.27431826 -0.43721131         nan -0.31789169         nan\n",
      "         nan         nan         nan -0.28910431 -0.45592268 -0.4342206\n",
      " -0.37039199         nan         nan -0.45920568 -0.31545338 -0.29337167\n",
      " -0.44081107 -0.43027854 -0.35213667         nan -0.32994088 -0.30895339\n",
      " -0.39187684         nan -0.54156875 -0.25520831 -0.34585192 -0.36138963\n",
      " -0.45595172 -0.26142946 -0.48517485 -0.24678759         nan         nan\n",
      " -0.40643193 -0.49489123 -0.61461866         nan         nan         nan\n",
      " -0.48861586 -0.34085537         nan         nan         nan -0.39387926\n",
      " -0.42584497 -0.38983935 -0.41103544 -0.23302982 -0.4807322  -0.32045551\n",
      " -0.29358751 -0.33046607 -0.43847276         nan -0.45592268 -0.46466409\n",
      " -0.41848017 -0.48517485 -0.31789169 -0.30821754         nan -0.30821754\n",
      "         nan -0.40440417         nan         nan -0.42948944         nan\n",
      " -0.48517485         nan -0.35646783 -0.48018486 -0.37153352 -0.33933882\n",
      "         nan         nan -0.35185116         nan         nan -0.34254722\n",
      "         nan -0.28528514         nan         nan -0.30569257         nan\n",
      "         nan -0.38478495 -0.30569257         nan -0.16493391         nan\n",
      "         nan -0.44560099 -0.32218697 -0.61461866 -0.37039199         nan\n",
      "         nan -0.38394461         nan -0.35223728 -0.29861222         nan\n",
      " -0.22069427 -0.40179466 -0.35275959 -0.31545338         nan         nan\n",
      "         nan         nan -0.31192707 -0.38455752         nan -0.35865406\n",
      " -0.30142077 -0.31510315         nan -0.36194347 -0.36194347 -0.34585192\n",
      "         nan -0.29530096         nan -0.26711473         nan -0.35717818\n",
      " -0.46466409 -0.28528514         nan -0.44389083         nan         nan\n",
      "         nan -0.30660052 -0.31002037 -0.29852694 -0.23709373 -0.45996532\n",
      " -0.27865561         nan -0.4415704          nan         nan -0.36273026\n",
      "         nan -0.44560099 -0.30949812         nan -0.33146949         nan\n",
      " -0.24406682 -0.46385917 -0.39777085         nan         nan -0.42948944\n",
      " -0.31545338         nan         nan         nan -0.40066114 -0.4342206\n",
      "         nan -0.37526918         nan -0.30895339 -0.25110925 -0.24503214\n",
      " -0.35831221         nan -0.29337167         nan         nan -0.3078314\n",
      "         nan         nan -0.46385917 -0.59768594 -0.49489123         nan\n",
      " -0.38319787         nan         nan -0.3596366  -0.40066114         nan\n",
      " -0.28484505 -0.28973555 -0.32463336         nan -0.40066114         nan\n",
      " -0.25479684 -0.35957042 -0.39787454 -0.49821245         nan -0.47087317\n",
      " -0.29978542 -0.32313386 -0.56727854 -0.41103544         nan         nan\n",
      "         nan -0.45920568         nan -0.32692882 -0.59289423         nan\n",
      " -0.33125052 -0.37894508 -0.39225466         nan -0.42950231         nan\n",
      "         nan -0.52173647 -0.35574373 -0.36858421 -0.2827186  -0.35251827\n",
      " -0.34201152 -0.37834906 -0.44389083 -0.38345506         nan -0.39777085\n",
      " -0.33045378 -0.32635964 -0.39787454         nan         nan -0.41517162\n",
      "         nan -0.23175541 -0.32546408 -0.33045378 -0.32967803 -0.53052897\n",
      " -0.31002037         nan         nan -0.36858421         nan -0.36381779\n",
      "         nan         nan -0.37087129         nan         nan -0.35865406\n",
      " -0.31510315         nan         nan -0.4030839          nan         nan\n",
      "         nan -0.28215287 -0.38394461 -0.44560099 -0.39232394         nan\n",
      " -0.44389083         nan         nan -0.38051728         nan -0.38394461\n",
      " -0.24678759 -0.39891216 -0.49489123         nan -0.28338846 -0.52879676\n",
      "         nan -0.39387926         nan -0.54190028         nan         nan\n",
      " -0.43691991 -0.40440417         nan -0.38345506         nan -0.24678759\n",
      "         nan         nan -0.36194347         nan         nan         nan\n",
      " -0.35831221 -0.33045378         nan -0.27431826         nan -0.34254722\n",
      "         nan -0.29978542         nan         nan         nan -0.38180315\n",
      "         nan -0.48018486 -0.4807322  -0.30949812 -0.28982306         nan\n",
      " -0.31192707 -0.13237391         nan -0.30821754 -0.32349166         nan\n",
      " -0.31510315         nan -0.28159291         nan -0.40179466         nan\n",
      " -0.33375556 -0.35432216         nan         nan -0.23175541 -0.48696167\n",
      " -0.41970183 -0.34585192         nan -0.54156875         nan -0.25110925\n",
      "         nan -0.29511543         nan         nan         nan -0.42584497\n",
      " -0.49694895 -0.54165202 -0.39387926 -0.49489123 -0.40206919 -0.29861222\n",
      " -0.54190028         nan         nan         nan -0.29861222         nan\n",
      " -0.54165202         nan -0.35223728         nan -0.38394461 -0.44786023\n",
      " -0.3078314  -0.44560099 -0.33634119         nan         nan -0.2398758\n",
      "         nan -0.34254722 -0.28973555         nan]  Accuracy: 0.6125  F1 Score: 0.7596899224806202 Precision training: 0.6125 Recall training: <function recall at 0x000002DEF8F891E0> ]\n",
      "TEST: [Iteracion:  0 Accuracy: 0.6293706293706294  F1 Score: 0.7725321888412017 Precision Test: 0.6293706293706294 Recall Test: 1.0 ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin VDK\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\Admin VDK\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#####################\n",
      "TRAINING: [Iteracion:  300  Costo:  [-0.20573146 -0.19100611         nan         nan         nan -0.31981595\n",
      " -0.67333395 -0.48846971         nan -0.35315957 -0.1678814          nan\n",
      " -0.34938974         nan -0.2005106  -0.26037377 -0.52529616 -0.16802149\n",
      " -0.43328058 -0.24982042 -0.40483537 -0.46316693 -0.35700969 -0.39666436\n",
      " -0.17083232 -0.52529616 -0.2466089          nan -0.43422666         nan\n",
      "         nan         nan         nan -0.5009541  -0.22714604 -0.26037377\n",
      " -0.33681177         nan         nan -0.22804779 -0.41560124 -0.47770002\n",
      " -0.23473224 -0.25803696 -0.36401494         nan -0.40483537 -0.44966735\n",
      " -0.29619509         nan -0.16802149 -0.58799918 -0.3729502  -0.34589567\n",
      " -0.22704604 -0.57443001 -0.19100611 -0.61663522         nan         nan\n",
      " -0.28579759 -0.2005106  -0.12556047         nan         nan         nan\n",
      " -0.20797707 -0.36387972         nan         nan         nan -0.3017135\n",
      " -0.25541091 -0.31409099 -0.27513856 -0.67976927 -0.20779695 -0.43052664\n",
      " -0.46912898 -0.40715678 -0.24982042         nan -0.22714604 -0.21926342\n",
      " -0.2646274  -0.19100611 -0.43422666 -0.454241           nan -0.454241\n",
      "         nan -0.27865115         nan         nan -0.25511655         nan\n",
      " -0.19100611         nan -0.34366897 -0.20415921 -0.33190788 -0.38819037\n",
      "         nan         nan -0.35143047         nan         nan -0.38412939\n",
      "         nan -0.50725223         nan         nan -0.46316693         nan\n",
      "         nan -0.31981595 -0.46316693         nan -1.06906963         nan\n",
      "         nan -0.23617889 -0.42923583 -0.12556047 -0.33681177         nan\n",
      "         nan -0.29585207         nan -0.35375376 -0.47279837         nan\n",
      " -0.73893215 -0.2886226  -0.35315957 -0.41560124         nan         nan\n",
      "         nan         nan -0.42873965 -0.30530112         nan -0.35654981\n",
      " -0.47715965 -0.4465667          nan -0.33759537 -0.33759537 -0.3729502\n",
      "         nan -0.46471458         nan -0.5835634          nan -0.35947006\n",
      " -0.21926342 -0.50725223         nan -0.2356067          nan         nan\n",
      "         nan -0.46633164 -0.44821265 -0.4617067  -0.6639438  -0.22777365\n",
      " -0.53405116         nan -0.23818249         nan         nan -0.33226382\n",
      "         nan -0.23617889 -0.44238302         nan -0.40076463         nan\n",
      " -0.63575407 -0.219926   -0.29451999         nan         nan -0.25511655\n",
      " -0.41560124         nan         nan         nan -0.29223103 -0.26037377\n",
      "         nan -0.32187918         nan -0.44966735 -0.5897885  -0.61021029\n",
      " -0.35700969         nan -0.47770002         nan         nan -0.46358788\n",
      "         nan         nan -0.219926   -0.13958039 -0.2005106          nan\n",
      " -0.32615384         nan         nan -0.35922293 -0.29223103         nan\n",
      " -0.51494984 -0.49153305 -0.39666436         nan -0.29223103         nan\n",
      " -0.58582568 -0.35195623 -0.2938519  -0.19145653         nan -0.21116869\n",
      " -0.46666292 -0.4115342  -0.15502338 -0.27513856         nan         nan\n",
      "         nan -0.22804779         nan -0.40108731 -0.14404577         nan\n",
      " -0.40643845 -0.32415318 -0.29966565         nan -0.26550395         nan\n",
      "         nan -0.17777905 -0.3590933  -0.3189544  -0.50769741 -0.37280918\n",
      " -0.37127204 -0.31076741 -0.2356067  -0.30986751         nan -0.29451999\n",
      " -0.3969967  -0.41055983 -0.2938519          nan         nan -0.28201278\n",
      "         nan -0.6680375  -0.40927096 -0.3969967  -0.4066326  -0.17083232\n",
      " -0.44821265         nan         nan -0.3189544          nan -0.33443125\n",
      "         nan         nan -0.32771717         nan         nan -0.35654981\n",
      " -0.4465667          nan         nan -0.28937082         nan         nan\n",
      "         nan -0.51721094 -0.29585207 -0.23617889 -0.30463555         nan\n",
      " -0.2356067          nan         nan -0.31869482         nan -0.29585207\n",
      " -0.61663522 -0.27509425 -0.2005106          nan -0.50371142 -0.1678814\n",
      "         nan -0.3017135          nan -0.16367468         nan         nan\n",
      " -0.25564755 -0.27865115         nan -0.30986751         nan -0.61663522\n",
      "         nan         nan -0.33759537         nan         nan         nan\n",
      " -0.35700969 -0.3969967          nan -0.52529616         nan -0.38412939\n",
      "         nan -0.46666292         nan         nan         nan -0.31904916\n",
      "         nan -0.20415921 -0.20779695 -0.44238302 -0.46121633         nan\n",
      " -0.42873965 -1.34847958         nan -0.454241   -0.42114035         nan\n",
      " -0.4465667          nan -0.51870228         nan -0.2886226          nan\n",
      " -0.39029687 -0.36143648         nan         nan -0.6680375  -0.20573146\n",
      " -0.26730056 -0.3729502          nan -0.16802149         nan -0.5897885\n",
      "         nan -0.49027062         nan         nan         nan -0.25541091\n",
      " -0.200037   -0.16597812 -0.3017135  -0.2005106  -0.27482681 -0.47279837\n",
      " -0.16367468         nan         nan         nan -0.47279837         nan\n",
      " -0.16597812         nan -0.35375376         nan -0.29585207 -0.23226806\n",
      " -0.46358788 -0.23617889 -0.38510329         nan         nan -0.6575367\n",
      "         nan -0.38412939 -0.49153305         nan]  Accuracy: 0.895  F1 Score: 0.920152091254753 Precision training: 0.8612099644128114 Recall training: <function recall at 0x000002DEF8F891E0> ]\n",
      "TEST: [Iteracion:  300 Accuracy: 0.8741258741258742  F1 Score: 0.90625 Precision Test: 0.8529411764705882 Recall Test: 0.9666666666666667 ]\n",
      "#####################\n",
      "TRAINING: [Iteracion:  600  Costo:  [-0.10608139 -0.09635324         nan         nan         nan -0.21264377\n",
      " -0.67965993 -0.41698927         nan -0.25182256 -0.07779844         nan\n",
      " -0.24583748         nan -0.10175279 -0.15319644 -0.46888993 -0.07696334\n",
      " -0.3411554  -0.1443131  -0.31023336 -0.38184084 -0.25379537 -0.30396147\n",
      " -0.07947631 -0.46888993 -0.14204889         nan -0.34534641         nan\n",
      "         nan         nan         nan -0.43225753 -0.12488429 -0.15319644\n",
      " -0.23162279         nan         nan -0.12511809 -0.32718406 -0.40395065\n",
      " -0.13254995 -0.15199169 -0.26244189         nan -0.31023336 -0.36591644\n",
      " -0.19048252         nan -0.07696334 -0.55629454 -0.27301022 -0.2425169\n",
      " -0.12479343 -0.53602304 -0.09635324 -0.59809627         nan         nan\n",
      " -0.1787756  -0.10175279 -0.04908393         nan         nan         nan\n",
      " -0.10745833 -0.26583978         nan         nan         nan -0.19481498\n",
      " -0.15068575 -0.20641661 -0.16901257 -0.68989903 -0.10818592 -0.34040674\n",
      " -0.39447207 -0.31246881 -0.1443131          nan -0.12488429 -0.11807114\n",
      " -0.15919087 -0.09635324 -0.34534641 -0.37096454         nan -0.37096454\n",
      "         nan -0.17315918         nan         nan -0.14982574         nan\n",
      " -0.09635324         nan -0.24184824 -0.10570454 -0.22692049 -0.29011747\n",
      "         nan         nan -0.25055562         nan         nan -0.28497549\n",
      "         nan -0.44149912         nan         nan -0.38184084         nan\n",
      "         nan -0.21264377 -0.38184084         nan -1.31287262         nan\n",
      "         nan -0.13292895 -0.33847634 -0.04908393 -0.23162279         nan\n",
      "         nan -0.19197391         nan -0.25259765 -0.39587405         nan\n",
      " -0.77892514 -0.18196469 -0.25182256 -0.32718406         nan         nan\n",
      "         nan         nan -0.34241238 -0.19998473         nan -0.25331891\n",
      " -0.39919225 -0.35966712         nan -0.23464807 -0.23464807 -0.27301022\n",
      "         nan -0.38872992         nan -0.54217964         nan -0.25647287\n",
      " -0.11807114 -0.44149912         nan -0.13273092         nan         nan\n",
      "         nan -0.38489931 -0.36379538 -0.38374672 -0.66601564 -0.12482953\n",
      " -0.4761435          nan -0.13497887         nan         nan -0.22951666\n",
      "         nan -0.13292895 -0.35776812         nan -0.30557431         nan\n",
      " -0.6241682  -0.11864962 -0.1878038          nan         nan -0.14982574\n",
      " -0.32718406         nan         nan         nan -0.18525953 -0.15319644\n",
      "         nan -0.21690381         nan -0.36591644 -0.56167977 -0.59201619\n",
      " -0.25379537         nan -0.40395065         nan         nan -0.38123541\n",
      "         nan         nan -0.11864962 -0.05729177 -0.10175279         nan\n",
      " -0.21879342         nan         nan -0.25547629 -0.18525953         nan\n",
      " -0.45052539 -0.42128072 -0.30396147         nan -0.18525953         nan\n",
      " -0.55412321 -0.24869646 -0.1872387  -0.09536173         nan -0.11161151\n",
      " -0.38853003 -0.31978592 -0.06759174 -0.16901257         nan         nan\n",
      "         nan -0.12511809         nan -0.30762634 -0.05995678         nan\n",
      " -0.311328   -0.21795968 -0.19335632         nan -0.15800428         nan\n",
      "         nan -0.08453447 -0.25658637 -0.21594536 -0.44350794 -0.27067739\n",
      " -0.27271134 -0.20621429 -0.13273092 -0.20419938         nan -0.1878038\n",
      " -0.30207782 -0.31752827 -0.1872387          nan         nan -0.17389433\n",
      "         nan -0.67624868 -0.31645169 -0.30207782 -0.3121026  -0.07947631\n",
      " -0.36379538         nan         nan -0.21594536         nan -0.23123572\n",
      "         nan         nan -0.22321021         nan         nan -0.25331891\n",
      " -0.35966712         nan         nan -0.18237189         nan         nan\n",
      "         nan -0.45466709 -0.19197391 -0.13292895 -0.19765127         nan\n",
      " -0.13273092         nan         nan -0.21271381         nan -0.19197391\n",
      " -0.59809627 -0.17128663 -0.10175279         nan -0.43867147 -0.07779844\n",
      "         nan -0.19481498         nan -0.07432012         nan         nan\n",
      " -0.14908005 -0.17315918         nan -0.20419938         nan -0.59809627\n",
      "         nan         nan -0.23464807         nan         nan         nan\n",
      " -0.25379537 -0.30207782         nan -0.46888993         nan -0.28497549\n",
      "         nan -0.38853003         nan         nan         nan -0.21268589\n",
      "         nan -0.10570454 -0.10818592 -0.35776812 -0.38757082         nan\n",
      " -0.34241238 -1.78278855         nan -0.37096454 -0.32943515         nan\n",
      " -0.35966712         nan -0.4566849          nan -0.18196469         nan\n",
      " -0.29422202 -0.25919197         nan         nan -0.67624868 -0.10608139\n",
      " -0.16118321 -0.27301022         nan -0.07696334         nan -0.56167977\n",
      "         nan -0.41690687         nan         nan         nan -0.15068575\n",
      " -0.1011997  -0.07571995 -0.19481498 -0.10175279 -0.17047445 -0.39587405\n",
      " -0.07432012         nan         nan         nan -0.39587405         nan\n",
      " -0.07571995         nan -0.25259765         nan -0.19197391 -0.12970395\n",
      " -0.38123541 -0.13292895 -0.28815618         nan         nan -0.65508196\n",
      "         nan -0.28497549 -0.42128072         nan]  Accuracy: 0.9125  F1 Score: 0.9325626204238922 Precision training: 0.8832116788321168 Recall training: <function recall at 0x000002DEF8F891E0> ]\n",
      "TEST: [Iteracion:  600 Accuracy: 0.8741258741258742  F1 Score: 0.90625 Precision Test: 0.8529411764705882 Recall Test: 0.9666666666666667 ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#####################\n",
      "TRAINING: [Iteracion:  900  Costo:  [-0.07011516 -0.06280225         nan         nan         nan -0.16651216\n",
      " -0.70227819 -0.38643035         nan -0.20676617 -0.04794967         nan\n",
      " -0.20001119         nan -0.06654424 -0.11050956 -0.44760349 -0.04705817\n",
      " -0.2993886  -0.10285321 -0.26737692 -0.34566031 -0.20791713 -0.26202686\n",
      " -0.04910167 -0.44760349 -0.10105724         nan -0.30517847         nan\n",
      "         nan         nan         nan -0.40341831 -0.08613783 -0.11050956\n",
      " -0.18555249         nan         nan -0.08616908 -0.28719569 -0.37199804\n",
      " -0.09308007 -0.10974288 -0.21705039         nan -0.26737692 -0.32836053\n",
      " -0.14589136         nan -0.04705817 -0.55165818 -0.22813798 -0.19684562\n",
      " -0.08606035 -0.5266918  -0.06280225 -0.60268765         nan         nan\n",
      " -0.13437221 -0.06654424 -0.0270735          nan         nan         nan\n",
      " -0.07111095 -0.22190506         nan         nan         nan -0.14958474\n",
      " -0.10893369 -0.16036732 -0.12552247 -0.71530849 -0.07196942 -0.29962182\n",
      " -0.36191506 -0.26954729 -0.10285321         nan -0.08613783 -0.08032587\n",
      " -0.11658858 -0.06280225 -0.30517847 -0.33371078         nan -0.33371078\n",
      "         nan -0.12961382         nan         nan -0.1079762          nan\n",
      " -0.06280225         nan -0.19682551 -0.07007472 -0.18107206 -0.24584397\n",
      "         nan         nan -0.2057053          nan         nan -0.24028549\n",
      "         nan -0.41444432         nan         nan -0.34566031         nan\n",
      "         nan -0.16651216 -0.34566031         nan -1.51547106         nan\n",
      "         nan -0.09312937 -0.29735724 -0.0270735  -0.18555249         nan\n",
      "         nan -0.14794722         nan -0.20759139 -0.36212885         nan\n",
      " -0.82662849 -0.13751113 -0.20676617 -0.28719569         nan         nan\n",
      "         nan         nan -0.30346404 -0.15507496         nan -0.20743632\n",
      " -0.36492908 -0.32050951         nan -0.18937022 -0.18937022 -0.22813798\n",
      "         nan -0.35540992         nan -0.53124142         nan -0.21062256\n",
      " -0.08032587 -0.41444432         nan -0.09305538         nan         nan\n",
      "         nan -0.34867478 -0.32590056 -0.3493481  -0.68550945 -0.08590523\n",
      " -0.45412825         nan -0.09500548         nan         nan -0.18447135\n",
      "         nan -0.09312937 -0.31975328         nan -0.26246908         nan\n",
      " -0.63380261 -0.08081899 -0.14300361         nan         nan -0.1079762\n",
      " -0.28719569         nan         nan         nan -0.14051089 -0.11050956\n",
      "         nan -0.17141522         nan -0.32836053 -0.5594133  -0.59664355\n",
      " -0.20791713         nan -0.37199804         nan         nan -0.34452139\n",
      "         nan         nan -0.08081899 -0.03260469 -0.06654424         nan\n",
      " -0.17232774         nan         nan -0.20934101 -0.14051089         nan\n",
      " -0.42434969 -0.39144527 -0.26202686         nan -0.14051089         nan\n",
      " -0.5494205  -0.20291695 -0.14250121 -0.06161976         nan -0.075006\n",
      " -0.35408162 -0.27822126 -0.04001193 -0.12552247         nan         nan\n",
      "         nan -0.08616908         nan -0.26530973 -0.03443072         nan\n",
      " -0.26823303 -0.1719797  -0.14841354         nan -0.11485003         nan\n",
      "         nan -0.052973   -0.2109324  -0.17124032 -0.41736359 -0.22490632\n",
      " -0.22843275 -0.16131311 -0.09305538 -0.15897845         nan -0.14300361\n",
      " -0.25913418 -0.27536059 -0.14250121         nan         nan -0.12943848\n",
      "         nan -0.70009173 -0.27440532 -0.25913418 -0.26927742 -0.04910167\n",
      " -0.32590056         nan         nan -0.17124032         nan -0.1859581\n",
      "         nan         nan -0.17768443         nan         nan -0.20743632\n",
      " -0.32050951         nan         nan -0.13777441         nan         nan\n",
      "         nan -0.42963015 -0.14794722 -0.09312937 -0.15225474         nan\n",
      " -0.09305538         nan         nan -0.16701751         nan -0.14794722\n",
      " -0.60268765 -0.12841045 -0.06654424         nan -0.41196304 -0.04794967\n",
      "         nan -0.14958474         nan -0.04519088         nan         nan\n",
      " -0.10690692 -0.12961382         nan -0.15897845         nan -0.60268765\n",
      "         nan         nan -0.18937022         nan         nan         nan\n",
      " -0.20791713 -0.25913418         nan -0.44760349         nan -0.24028549\n",
      "         nan -0.35408162         nan         nan         nan -0.1668526\n",
      "         nan -0.07007472 -0.07196942 -0.31975328 -0.3555045          nan\n",
      " -0.30346404 -2.12530651         nan -0.33371078 -0.28788366         nan\n",
      " -0.32050951         nan -0.43198395         nan -0.13751113         nan\n",
      " -0.25080396 -0.21359995         nan         nan -0.70009173 -0.07011516\n",
      " -0.11821752 -0.22813798         nan -0.04705817         nan -0.5594133\n",
      "         nan -0.38532027         nan         nan         nan -0.10893369\n",
      " -0.06604812 -0.04617943 -0.14958474 -0.06654424 -0.12746648 -0.36212885\n",
      " -0.04519088         nan         nan         nan -0.36212885         nan\n",
      " -0.04617943         nan -0.20759139         nan -0.14794722 -0.09040076\n",
      " -0.34452139 -0.09312937 -0.24440249         nan         nan -0.67138149\n",
      "         nan -0.24028549 -0.39144527         nan]  Accuracy: 0.9025  F1 Score: 0.9242718446601943 Precision training: 0.8814814814814815 Recall training: <function recall at 0x000002DEF8F891E0> ]\n",
      "TEST: [Iteracion:  900 Accuracy: 0.8741258741258742  F1 Score: 0.90625 Precision Test: 0.8529411764705882 Recall Test: 0.9666666666666667 ]\n"
     ]
    }
   ],
   "source": [
    "## Llama a la funcion de gradient descent, recurda definir un nombre diferente para el modelo (~1 linea) ##\n",
    "modelo_2_theta_values, cost_vect, acc_vect, acc_vect_test = \\\n",
    "gradient_descent(X_train_features, Y_train_m2, theta_vector, alpha, iterations, X_test_features, y_test, 'modelo_2')\n",
    "### FIN ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2def96ba390>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Funcion para graficar el costo y  accuracy durante cada iteracion ##\n",
    "helper.training_graph(cost_vect, acc_vect, acc_vect_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy con el Test set [Modelo 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = prediccion(X_test_features, modelo_2_theta_values, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 87.41258741258741 %\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy:',accuracy_score(y_test, y_predict)* 100 ,'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Matriz de confusion Test Set [Modelo 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[38, 15],\n",
       "       [ 3, 87]], dtype=int64)"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision:  0.8529411764705882\n",
      "recall:  0.9666666666666667\n"
     ]
    }
   ],
   "source": [
    "print('precision: ',precision_score(y_test, y_predict))\n",
    "\n",
    "print('recall: ', recall_score(y_test, y_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento modelo 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sampleo bootstrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_m3, Y_train_m3 = resample(X_train, y_train, n_samples = 460)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtrar Features\n",
    "Solo seleccionar los features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Ingresa en la lista el nombre de las fatrues que quieres utlizar en este modelo (~1 linea)\n",
    "'''\n",
    "features_filtro = ['mean radius', 'mean texture', 'mean perimeter', 'mean area']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_m3 = helper.fitrar_nombre(X_train_m3,data.feature_names,features_filtro)\n",
    "X_test_features = helper.fitrar_nombre(X_test,data.feature_names,features_filtro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Numero de iteracions ##\n",
    "iterations = 1500  # Define el numero de iteraciones #\n",
    "\n",
    "## Alpha ##\n",
    "alpha = 0.02 # Define el valor de alpha #\n",
    "\n",
    "#### No hay que cambiar el codigo despues de esta linea #####\n",
    "## Numero de feautres ##\n",
    "numero_features = X_train_m3.shape[1] + 1\n",
    "\n",
    "## Creamos un vector donde se almacenan los valores de Theta,\n",
    "## lo inicializamos con numeros aleatorios\n",
    "theta_vector = np.random.rand(numero_features)\n",
    "\n",
    "## Normaliza las features de X_train_features (~1 linea) ##\n",
    "X_train_m3 = normalizar(X_train_m3)\n",
    "\n",
    "# Creando la Matriz X de features, \n",
    "# utilizamos np.ones para agregar el valor constante '1' que es el bias o feature 0\n",
    "X_train_features = np.ones((X_train_m3.shape[0], numero_features))\n",
    "X_train_features[:,:-1] = X_train_m3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin VDK\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:43: RuntimeWarning: invalid value encountered in log\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#####################\n",
      "TRAINING: [Iteracion:  0  Costo:  [-0.35766411 -0.48415491 -0.18060991 -0.3068182  -0.34921183         nan\n",
      " -0.31705028 -0.23986299         nan         nan         nan         nan\n",
      " -0.27499253 -0.44155758 -0.31859145 -0.28079991 -0.23594444 -0.20380504\n",
      "         nan         nan -0.36306733 -0.31420503 -0.36630273 -0.57365309\n",
      "         nan         nan -0.25592216 -0.25209128 -0.32394144 -0.32198868\n",
      "         nan -0.35868383 -0.22293886 -0.26348666         nan -0.29923898\n",
      " -0.23220845 -0.38919126 -0.49396467 -0.45163992 -0.30601291         nan\n",
      "         nan -0.43278513 -0.52380309 -0.45429202 -0.3429475  -0.27442139\n",
      " -0.24569369 -0.23220845 -0.25819426         nan         nan -0.30872338\n",
      "         nan -0.19474117 -0.37932327 -0.31705028 -0.30581817 -0.35993965\n",
      "         nan -0.22273927         nan -0.19787311         nan -0.27442139\n",
      "         nan -0.52041203 -0.52598169         nan -0.31420503 -0.19267509\n",
      "         nan -0.30600956         nan -0.42150799 -0.18307419 -0.30679748\n",
      "         nan         nan -0.2784584  -0.39972337         nan -0.43411803\n",
      "         nan         nan -0.20379517 -0.36760901         nan         nan\n",
      " -0.21070431 -0.23220845 -0.29923898 -0.26732762         nan -0.24678233\n",
      " -0.25923589         nan -0.24319254 -0.35438659 -0.27918301         nan\n",
      "         nan -0.34941284 -0.52041203         nan -0.26505063         nan\n",
      "         nan         nan         nan -0.22919644 -0.3429475          nan\n",
      "         nan -0.49396467 -0.23594444 -0.27806155 -0.42884015 -0.20379517\n",
      " -0.4247151          nan -0.30996224 -0.25758242         nan -0.23986299\n",
      "         nan         nan -0.37911401 -0.35868383 -0.36662139         nan\n",
      " -0.41213038 -0.27735938         nan         nan         nan         nan\n",
      "         nan -0.31420503         nan -0.27882135         nan         nan\n",
      "         nan -0.47122798 -0.31418328         nan -0.2385454  -0.36877978\n",
      "         nan         nan -0.19554389 -0.33060768         nan         nan\n",
      "         nan         nan         nan -0.35017075         nan -0.28910855\n",
      " -0.41683848 -0.58287111         nan         nan -0.26646412 -0.24923714\n",
      " -0.31076946 -0.47244946 -0.30679748 -0.17189587 -0.3339899          nan\n",
      " -0.41683848         nan         nan -0.26646412 -0.32141984 -0.18261948\n",
      " -0.41146199         nan -0.38084135         nan -0.34595938 -0.31286765\n",
      " -0.36586824 -0.22919644 -0.11332932         nan -0.30520217 -0.30825242\n",
      " -0.19787311 -0.36662139 -0.27918301 -0.30825242 -0.29982986 -0.23512949\n",
      " -0.39036509         nan         nan -0.35993965 -0.25209128 -0.45429202\n",
      " -0.25049859         nan -0.24737574 -0.34254947 -0.32141984 -0.33816032\n",
      " -0.3429475  -0.32198868 -0.46519913         nan -0.23190577         nan\n",
      " -0.23131108 -0.34628719         nan -0.19787311 -0.34628719 -0.35680101\n",
      "         nan -0.35868383 -0.32061404         nan -0.39941001 -0.45429202\n",
      " -0.35161136 -0.52041203 -0.27336701 -0.3429475  -0.27600204 -0.31204689\n",
      "         nan -0.3649504  -0.27442139         nan -0.34941284 -0.27315744\n",
      " -0.33993451 -0.50101366 -0.38084135 -0.41146199         nan -0.52598169\n",
      " -0.3547132  -0.41308    -0.49396467         nan -0.44155758         nan\n",
      "         nan -0.18537992 -0.25923589 -0.41146199         nan         nan\n",
      " -0.39434849 -0.28774534         nan         nan         nan -0.36624938\n",
      " -0.18307419 -0.29636828         nan -0.35766411         nan -0.27386105\n",
      " -0.27425115         nan         nan         nan -0.47061333 -0.30996224\n",
      " -0.35502    -0.31076946 -0.42137358 -0.11332932 -0.45394591 -0.25902721\n",
      "         nan         nan         nan -0.28230835         nan -0.46349916\n",
      " -0.19554389         nan -0.27735938 -0.28079991 -0.46519913         nan\n",
      " -0.57365309 -0.3068182  -0.37304856 -0.25819426 -0.2282194  -0.3405189\n",
      "         nan -0.29923898 -0.28140886         nan -0.36586824 -0.35868383\n",
      " -0.27425115 -0.24923714         nan         nan         nan         nan\n",
      "         nan -0.35993965         nan -0.33816032         nan -0.45394591\n",
      " -0.37781781 -0.35367008 -0.19267509 -0.28304569 -0.41119171 -0.32061404\n",
      " -0.39036509 -0.19057417         nan -0.18261948 -0.54811576 -0.30996224\n",
      " -0.45163992 -0.39941001 -0.42235905 -0.35438659         nan -0.46519913\n",
      " -0.35680101         nan -0.32198868 -0.18307419         nan -0.35868383\n",
      " -0.30609899 -0.27336701         nan         nan -0.29025131         nan\n",
      " -0.41213038 -0.24444942 -0.19174466 -0.19554389         nan         nan\n",
      "         nan         nan -0.23594444 -0.43715107 -0.25449458 -0.6193038\n",
      " -0.0859604  -0.35680101 -0.45115816 -0.47061333 -0.32470686 -0.25122695\n",
      " -0.43171812         nan -0.49396467 -0.35502    -0.28774534 -0.37932327\n",
      "         nan -0.19787311 -0.27425115 -0.36113008 -0.31286765 -0.17863702\n",
      " -0.27315744 -0.35268089         nan         nan         nan         nan\n",
      "         nan         nan -0.20379517 -0.43350946         nan -0.35215459\n",
      " -0.22803138         nan         nan         nan         nan -0.44155758\n",
      "         nan -0.18060991         nan         nan -0.19787311         nan\n",
      " -0.26678334 -0.35993965 -0.24707106 -0.34038572 -0.37911401 -0.24541635\n",
      " -0.44960542 -0.22803138 -0.26348666 -0.31076946 -0.34038572 -0.43411803\n",
      "         nan -0.27386105 -0.41146199 -0.29937079 -0.41146199         nan\n",
      "         nan         nan -0.34830323         nan         nan -0.24678233\n",
      "         nan         nan -0.41146199         nan -0.45115816         nan\n",
      " -0.30679748 -0.24444942 -0.28304569         nan         nan         nan\n",
      " -0.35953675 -0.19474117 -0.38501517         nan -0.22492926 -0.45650338\n",
      " -0.31418328 -0.27499253         nan -0.3339899  -0.38888062         nan\n",
      " -0.44445537 -0.29937079 -0.31418328         nan -0.15733301         nan\n",
      "         nan         nan -0.23986299 -0.44960542]  Accuracy: 0.6543478260869565  F1 Score: 0.7910643889618922 Precision training: 0.6543478260869565 Recall training: <function recall at 0x000002DEF8F891E0> ]\n",
      "TEST: [Iteracion:  0 Accuracy: 0.6293706293706294  F1 Score: 0.7725321888412017 Precision Test: 0.6293706293706294 Recall Test: 1.0 ]\n",
      "#####################\n",
      "TRAINING: [Iteracion:  300  Costo:  [-0.42979907 -0.35688161 -0.62393319 -0.46970707 -0.43644612         nan\n",
      " -0.46168679 -0.53950481         nan         nan         nan         nan\n",
      " -0.49900682 -0.37796831 -0.45954041 -0.49401409 -0.54378682 -0.58665975\n",
      "         nan         nan -0.42609526 -0.46340826 -0.42445317 -0.31971461\n",
      "         nan         nan -0.51949834 -0.52515125 -0.45527695 -0.4570674\n",
      "         nan -0.42948015 -0.56090572 -0.51068791         nan -0.47628559\n",
      " -0.54844893 -0.40908406 -0.35204745 -0.37323456 -0.47120986         nan\n",
      "         nan -0.38302416 -0.33935775 -0.37174752 -0.44060533 -0.5001513\n",
      " -0.53105073 -0.54844893 -0.51771028         nan         nan -0.46850837\n",
      "         nan -0.59994564 -0.41541223 -0.46168679 -0.47067303 -0.42889572\n",
      "         nan -0.55970964         nan -0.59501864         nan -0.5001513\n",
      "         nan -0.3408776  -0.33838023         nan -0.46340826 -0.60553157\n",
      "         nan -0.47146631         nan -0.3891686  -0.61970081 -0.47042725\n",
      "         nan         nan -0.49675014 -0.40231553         nan -0.38238263\n",
      "         nan         nan -0.58748206 -0.42263427         nan         nan\n",
      " -0.5766096  -0.54844893 -0.47628559 -0.50780289         nan -0.53061192\n",
      " -0.51645752         nan -0.53472408 -0.43220318 -0.4956455          nan\n",
      "         nan -0.43541627 -0.3408776          nan -0.51110182         nan\n",
      "         nan         nan         nan -0.55231035 -0.44060533         nan\n",
      "         nan -0.35204745 -0.54378682 -0.49673199 -0.38579774 -0.58748206\n",
      " -0.38746427         nan -0.46738684 -0.51815766         nan -0.53950481\n",
      "         nan         nan -0.41537203 -0.42948015 -0.42374289         nan\n",
      " -0.39484127 -0.49762374         nan         nan         nan         nan\n",
      "         nan -0.46340826         nan -0.49680936         nan         nan\n",
      "         nan -0.36320442 -0.46372601         nan -0.54064993 -0.42183826\n",
      "         nan         nan -0.59773753 -0.45003359         nan         nan\n",
      "         nan         nan         nan -0.43561633         nan -0.4860016\n",
      " -0.39201474 -0.3158582          nan         nan -0.50949722 -0.52763232\n",
      " -0.46682911 -0.36274407 -0.47042725 -0.64064918 -0.44792687         nan\n",
      " -0.39201474         nan         nan -0.50949722 -0.45828108 -0.62118691\n",
      " -0.39522186         nan -0.41423779         nan -0.43823663 -0.46506976\n",
      " -0.42461246 -0.55231035 -0.78235448         nan -0.47200069 -0.4680778\n",
      " -0.59501864 -0.42374289 -0.4956455  -0.4680778  -0.47686377 -0.54465127\n",
      " -0.4076975          nan         nan -0.42889572 -0.52515125 -0.37174752\n",
      " -0.52637686         nan -0.52935189 -0.44097006 -0.45828108 -0.44473392\n",
      " -0.44060533 -0.4570674  -0.36644294         nan -0.54807494         nan\n",
      " -0.54933838 -0.43852663         nan -0.59501864 -0.43852663 -0.43072894\n",
      "         nan -0.42948015 -0.45811382         nan -0.40283153 -0.37174752\n",
      " -0.43438807 -0.3408776  -0.50199745 -0.44060533 -0.49924361 -0.46564796\n",
      "         nan -0.42535533 -0.5001513          nan -0.43541627 -0.5020855\n",
      " -0.44265303 -0.3485988  -0.41423779 -0.39522186         nan -0.33838023\n",
      " -0.43226207 -0.39403312 -0.35204745         nan -0.37796831         nan\n",
      "         nan -0.61712951 -0.51645752 -0.39522186         nan         nan\n",
      " -0.40588617 -0.48781051         nan         nan         nan -0.42406749\n",
      " -0.61970081 -0.47999317         nan -0.42979907         nan -0.50048393\n",
      " -0.50138721         nan         nan         nan -0.36213845 -0.46738684\n",
      " -0.43190167 -0.46682911 -0.38971387 -0.78235448 -0.37174717 -0.51715652\n",
      "         nan         nan         nan -0.49308954         nan -0.36682594\n",
      " -0.59773753         nan -0.49762374 -0.49401409 -0.36644294         nan\n",
      " -0.31971461 -0.46970707 -0.41927465 -0.51771028 -0.55361682 -0.44282245\n",
      "         nan -0.47628559 -0.49363262         nan -0.42461246 -0.42948015\n",
      " -0.50138721 -0.52763232         nan         nan         nan         nan\n",
      "         nan -0.42889572         nan -0.44473392         nan -0.37174717\n",
      " -0.4159524  -0.43287827 -0.60553157 -0.49249779 -0.39472257 -0.45811382\n",
      " -0.4076975  -0.60792888         nan -0.62118691 -0.32932385 -0.46738684\n",
      " -0.37323456 -0.40283153 -0.38885906 -0.43220318         nan -0.36644294\n",
      " -0.43072894         nan -0.4570674  -0.61970081         nan -0.42948015\n",
      " -0.47079559 -0.50199745         nan         nan -0.48506255         nan\n",
      " -0.39484127 -0.53375569 -0.60626288 -0.59773753         nan         nan\n",
      "         nan         nan -0.54378682 -0.38102655 -0.5210256  -0.30337859\n",
      " -0.88559371 -0.43072894 -0.37312929 -0.36213845 -0.45548591 -0.52579281\n",
      " -0.38352846         nan -0.35204745 -0.43190167 -0.48781051 -0.41541223\n",
      "         nan -0.59501864 -0.50138721 -0.42794268 -0.46506976 -0.62770895\n",
      " -0.5020855  -0.43351229         nan         nan         nan         nan\n",
      "         nan         nan -0.58748206 -0.38270979         nan -0.43359392\n",
      " -0.55399847         nan         nan         nan         nan -0.37796831\n",
      "         nan -0.62393319         nan         nan -0.59501864         nan\n",
      " -0.50939402 -0.42889572 -0.52953232 -0.44335481 -0.41537203 -0.53212115\n",
      " -0.37429731 -0.55399847 -0.51068791 -0.46682911 -0.44335481 -0.38238263\n",
      "         nan -0.50048393 -0.39522186 -0.47736127 -0.39522186         nan\n",
      "         nan         nan -0.43587375         nan         nan -0.53061192\n",
      "         nan         nan -0.39522186         nan -0.37312929         nan\n",
      " -0.47042725 -0.53375569 -0.49249779         nan         nan         nan\n",
      " -0.42898814 -0.59994564 -0.41208174         nan -0.55803475 -0.37097567\n",
      " -0.46372601 -0.49900682         nan -0.44792687 -0.40941098         nan\n",
      " -0.37664566 -0.47736127 -0.46372601         nan -0.66785076         nan\n",
      "         nan         nan -0.53950481 -0.37429731]  Accuracy: 0.8630434782608696  F1 Score: 0.9044006069802731 Precision training: 0.8324022346368715 Recall training: <function recall at 0x000002DEF8F891E0> ]\n",
      "TEST: [Iteracion:  300 Accuracy: 0.8531468531468531  F1 Score: 0.8955223880597014 Precision Test: 0.8108108108108109 Recall Test: 1.0 ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#####################\n",
      "TRAINING: [Iteracion:  600  Costo:  [-0.28949251 -0.1977634  -0.60275145 -0.34689477 -0.29843861         nan\n",
      " -0.33556497 -0.45432541         nan         nan         nan         nan\n",
      " -0.39102773 -0.22310414 -0.33220932 -0.38181636 -0.46144204 -0.53498522\n",
      "         nan         nan -0.28458647 -0.33622858 -0.28342705 -0.15836702\n",
      "         nan         nan -0.42334463 -0.43036379 -0.32567606 -0.327009\n",
      "         nan -0.2893293  -0.49043516 -0.41020332         nan -0.35580082\n",
      " -0.4691362  -0.26222981 -0.19277466 -0.21818385 -0.34857201         nan\n",
      "         nan -0.22925279 -0.17801313 -0.21551322 -0.30471711 -0.39160073\n",
      " -0.44090252 -0.4691362  -0.42143266         nan         nan -0.34324732\n",
      "         nan -0.56075045 -0.2696275  -0.33556497 -0.3481892  -0.28736864\n",
      "         nan -0.49128556         nan -0.55128571         nan -0.39160073\n",
      "         nan -0.17986033 -0.17759291         nan -0.33622858 -0.56714794\n",
      "         nan -0.34999054         nan -0.23651658 -0.59422519 -0.3467646\n",
      "         nan         nan -0.38710621 -0.25401243         nan -0.22817708\n",
      "         nan         nan -0.53534763 -0.27938363         nan         nan\n",
      " -0.52024047 -0.4691362  -0.35580082 -0.40398428         nan -0.44279968\n",
      " -0.41712888         nan -0.44640981 -0.29323582 -0.384704           nan\n",
      "         nan -0.29722668 -0.17986033         nan -0.40770097         nan\n",
      "         nan         nan         nan -0.477202   -0.30471711         nan\n",
      "         nan -0.19277466 -0.46144204 -0.38669087 -0.23178343 -0.53534763\n",
      " -0.23439426         nan -0.3418972  -0.41963624         nan -0.45432541\n",
      "         nan         nan -0.26970896 -0.2893293  -0.28060444         nan\n",
      " -0.24374534 -0.3883027          nan         nan         nan         nan\n",
      "         nan -0.33622858         nan -0.38543609         nan         nan\n",
      "         nan -0.20483228 -0.33722023         nan -0.45764884 -0.27837808\n",
      "         nan         nan -0.5556005  -0.31825289         nan         nan\n",
      "         nan         nan         nan -0.2962556          nan -0.36970179\n",
      " -0.23988719 -0.15466052         nan         nan -0.4062617  -0.43566829\n",
      " -0.34160451 -0.20515338 -0.3467646  -0.63128808 -0.314287           nan\n",
      " -0.23988719         nan         nan -0.4062617  -0.32892179 -0.59666915\n",
      " -0.24476415         nan -0.2686808          nan -0.3012658  -0.33857987\n",
      " -0.28245322 -0.477202   -0.91159566         nan -0.34849073 -0.34401882\n",
      " -0.55128571 -0.28060444 -0.384704   -0.34401882 -0.35529461 -0.46279234\n",
      " -0.26004093         nan         nan -0.28736864 -0.43036379 -0.21551322\n",
      " -0.43311359         nan -0.43736868 -0.30651114 -0.32892179 -0.30966661\n",
      " -0.30471711 -0.327009   -0.20880491         nan -0.47013194         nan\n",
      " -0.47069881 -0.30131529         nan -0.55128571 -0.30131529 -0.29028751\n",
      "         nan -0.2893293  -0.32922472         nan -0.25324234 -0.21551322\n",
      " -0.29527608 -0.17986033 -0.39425111 -0.30471711 -0.39002567 -0.33994862\n",
      "         nan -0.28526854 -0.39160073         nan -0.29722668 -0.3967238\n",
      " -0.30775617 -0.18878642 -0.2686808  -0.24476415         nan -0.17759291\n",
      " -0.29171287 -0.2424535  -0.19277466         nan -0.22310414         nan\n",
      "         nan -0.58816459 -0.41712888 -0.24476415         nan         nan\n",
      " -0.25718771 -0.37178939         nan         nan         nan -0.28190052\n",
      " -0.59422519 -0.36062843         nan -0.28949251         nan -0.39319501\n",
      " -0.39252043         nan         nan         nan -0.20444388 -0.3418972\n",
      " -0.29188033 -0.34160451 -0.23694239 -0.91159566 -0.21539163 -0.4197511\n",
      "         nan         nan         nan -0.38053923         nan -0.20961166\n",
      " -0.5556005          nan -0.3883027  -0.38181636 -0.20880491         nan\n",
      " -0.15836702 -0.34689477 -0.27593853 -0.42143266 -0.47850879 -0.30714795\n",
      "         nan -0.35580082 -0.38106842         nan -0.28245322 -0.2893293\n",
      " -0.39252043 -0.43566829         nan         nan         nan         nan\n",
      "         nan -0.28736864         nan -0.30966661         nan -0.21539163\n",
      " -0.27064936 -0.29366569 -0.56714794 -0.37973761 -0.24418664 -0.32922472\n",
      " -0.26004093 -0.57209191         nan -0.59666915 -0.16776533 -0.3418972\n",
      " -0.21818385 -0.25324234 -0.23735327 -0.29323582         nan -0.20880491\n",
      " -0.29028751         nan -0.327009   -0.59422519         nan -0.2893293\n",
      " -0.34758889 -0.39425111         nan         nan -0.36877227         nan\n",
      " -0.24374534 -0.44714111 -0.56938496 -0.5556005          nan         nan\n",
      "         nan         nan -0.46144204 -0.22677318 -0.42473003 -0.14124776\n",
      " -1.12658291 -0.29028751 -0.21693158 -0.20444388 -0.32391777 -0.43209846\n",
      " -0.22968738         nan -0.19277466 -0.29188033 -0.37178939 -0.2696275\n",
      "         nan -0.55128571 -0.39252043 -0.28681794 -0.33857987 -0.61023857\n",
      " -0.3967238  -0.29409151         nan         nan         nan         nan\n",
      "         nan         nan -0.53534763 -0.22829609         nan -0.29570542\n",
      " -0.47833816         nan         nan         nan         nan -0.22310414\n",
      "         nan -0.60275145         nan         nan -0.55128571         nan\n",
      " -0.40645074 -0.28736864 -0.43933259 -0.30758022 -0.26970896 -0.44369969\n",
      " -0.21856627 -0.47833816 -0.41020332 -0.34160451 -0.30758022 -0.22817708\n",
      "         nan -0.39319501 -0.24476415 -0.3566528  -0.24476415         nan\n",
      "         nan         nan -0.29839456         nan         nan -0.44279968\n",
      "         nan         nan -0.24476415         nan -0.21693158         nan\n",
      " -0.3467646  -0.44714111 -0.37973761         nan         nan         nan\n",
      " -0.2882694  -0.56075045 -0.26479423         nan -0.48522104 -0.21429848\n",
      " -0.33722023 -0.39102773         nan -0.314287   -0.26251869         nan\n",
      " -0.22107527 -0.3566528  -0.33722023         nan -0.68328356         nan\n",
      "         nan         nan -0.45432541 -0.21856627]  Accuracy: 0.8652173913043478  F1 Score: 0.9057750759878419 Precision training: 0.834733893557423 Recall training: <function recall at 0x000002DEF8F891E0> ]\n",
      "TEST: [Iteracion:  600 Accuracy: 0.8461538461538461  F1 Score: 0.89 Precision Test: 0.8090909090909091 Recall Test: 0.9888888888888889 ]\n",
      "#####################\n",
      "TRAINING: [Iteracion:  900  Costo:  [-0.22168132 -0.13098011 -0.60141872 -0.28451768 -0.23111734         nan\n",
      " -0.27194153 -0.41070407         nan         nan         nan         nan\n",
      " -0.33517807 -0.15483852 -0.26816262 -0.32398459 -0.41948736 -0.51242133\n",
      "         nan         nan -0.21655163 -0.27218766 -0.21559967 -0.09655934\n",
      "         nan         nan -0.37354009 -0.38135532 -0.26083804 -0.26196754\n",
      "         nan -0.22157334 -0.45567069 -0.35809008         nan -0.29434818\n",
      " -0.42898703 -0.19350658 -0.1265517  -0.1503164  -0.28624303         nan\n",
      "         nan -0.16075386 -0.11322286 -0.14757976 -0.23797983 -0.33549177\n",
      " -0.39445222 -0.42898703 -0.37151069         nan         nan -0.27989542\n",
      "         nan -0.54656629 -0.2008461  -0.27194153 -0.2859373  -0.21920388\n",
      "         nan -0.45766841         nan -0.53401896         nan -0.33549177\n",
      "         nan -0.11488843 -0.11299007         nan -0.27218766 -0.55370742\n",
      "         nan -0.28812717         nan -0.16775105 -0.58985121 -0.28402974\n",
      "         nan         nan -0.33044582 -0.1853278          nan -0.15964454\n",
      "         nan         nan -0.51252586 -0.21098227         nan         nan\n",
      " -0.49438324 -0.42898703 -0.29434818 -0.35019513         nan -0.39753417\n",
      " -0.36567373         nan -0.40100546 -0.22575771 -0.32745101         nan\n",
      "         nan -0.22989348 -0.11488843         nan -0.35415106         nan\n",
      "         nan         nan         nan -0.43950964 -0.23797983         nan\n",
      "         nan -0.1265517  -0.41948736 -0.32985721 -0.16299367 -0.51252586\n",
      " -0.16567351         nan -0.27846622 -0.36861674         nan -0.41070407\n",
      "         nan         nan -0.20096603 -0.22157334 -0.21217982         nan\n",
      " -0.17489576 -0.3318031          nan         nan         nan         nan\n",
      "         nan -0.27218766         nan -0.32798481         nan         nan\n",
      "         nan -0.1374384  -0.27343103         nan -0.41523626 -0.20995158\n",
      "         nan         nan -0.53948439 -0.25271609         nan         nan\n",
      "         nan         nan         nan -0.22852459         nan -0.31001861\n",
      " -0.17101435 -0.09350636         nan         nan -0.35276462 -0.38817509\n",
      " -0.27828053 -0.137927   -0.28402974 -0.63873477 -0.24812648         nan\n",
      " -0.17101435         nan         nan -0.35276462 -0.26410934 -0.59298308\n",
      " -0.17603581         nan -0.20002741         nan -0.23423833 -0.27477819\n",
      " -0.21427797 -0.43950964 -1.02950976         nan -0.28580411 -0.28116073\n",
      " -0.53401896 -0.21217982 -0.32745101 -0.28116073 -0.29337405 -0.42113189\n",
      " -0.19119601         nan         nan -0.21920388 -0.38135532 -0.14757976\n",
      " -0.3849204          nan -0.38992064 -0.24024947 -0.26410934 -0.24308786\n",
      " -0.23797983 -0.26196754 -0.14117409         nan -0.43073522         nan\n",
      " -0.43095463 -0.23419077         nan -0.53401896 -0.23419077 -0.22239356\n",
      "         nan -0.22157334 -0.26460464         nan -0.1841986  -0.14757976\n",
      " -0.22767101 -0.11488843 -0.33852267 -0.23797983 -0.33358698 -0.27645115\n",
      "         nan -0.21769211 -0.33549177         nan -0.22989348 -0.34210979\n",
      " -0.24130295 -0.12292967 -0.20002741 -0.17603581         nan -0.11299007\n",
      " -0.22372604 -0.17355517 -0.1265517          nan -0.15483852         nan\n",
      "         nan -0.58135663 -0.36567373 -0.17603581         nan         nan\n",
      " -0.18817457 -0.31222468         nan         nan         nan -0.21375079\n",
      " -0.58985121 -0.29961844         nan -0.22168132         nan -0.33767711\n",
      " -0.33626169         nan         nan         nan -0.13728405 -0.27846622\n",
      " -0.22407579 -0.27828053 -0.16810203 -1.02950976 -0.14743907 -0.36924966\n",
      "         nan         nan         nan -0.32253246         nan -0.14201411\n",
      " -0.53948439         nan -0.3318031  -0.32398459 -0.14117409         nan\n",
      " -0.09655934 -0.28451768 -0.2076642  -0.37151069 -0.44086441 -0.24041442\n",
      "         nan -0.29434818 -0.32306599         nan -0.21427797 -0.22157334\n",
      " -0.33626169 -0.38817509         nan         nan         nan         nan\n",
      "         nan -0.21920388         nan -0.24308786         nan -0.14743907\n",
      " -0.20198047 -0.22608244 -0.55370742 -0.32163052 -0.17548423 -0.26460464\n",
      " -0.19119601 -0.5604105          nan -0.59298308 -0.10442588 -0.27846622\n",
      " -0.1503164  -0.1841986  -0.16886055 -0.22575771         nan -0.14117409\n",
      " -0.22239356         nan -0.26196754 -0.58985121         nan -0.22157334\n",
      " -0.28503868 -0.33852267         nan         nan -0.30908757         nan\n",
      " -0.17489576 -0.40258841 -0.55695173 -0.53948439         nan         nan\n",
      "         nan         nan -0.41948736 -0.1583454  -0.37489143 -0.08232077\n",
      " -1.33844844 -0.22239356 -0.14887233 -0.13728405 -0.25833785 -0.38366614\n",
      " -0.16113239         nan -0.1265517  -0.22407579 -0.31222468 -0.2008461\n",
      "         nan -0.53401896 -0.33626169 -0.21881757 -0.27477819 -0.61155622\n",
      " -0.34210979 -0.22642001         nan         nan         nan         nan\n",
      "         nan         nan -0.51252586 -0.15969355         nan -0.22852945\n",
      " -0.4403982          nan         nan         nan         nan -0.15483852\n",
      "         nan -0.60141872         nan         nan -0.53401896         nan\n",
      " -0.35308721 -0.21920388 -0.39281112 -0.24079112 -0.20096603 -0.39816221\n",
      " -0.15047423 -0.4403982  -0.35809008 -0.27828053 -0.24079112 -0.15964454\n",
      "         nan -0.33767711 -0.17603581 -0.2950847  -0.17603581         nan\n",
      "         nan         nan -0.23128399         nan         nan -0.39753417\n",
      "         nan         nan -0.17603581         nan -0.14887233         nan\n",
      " -0.28402974 -0.40258841 -0.32163052         nan         nan         nan\n",
      " -0.22035387 -0.54656629 -0.1957792          nan -0.44902551 -0.14636054\n",
      " -0.27343103 -0.33517807         nan -0.24812648 -0.19376465         nan\n",
      " -0.15279584 -0.2950847  -0.27343103         nan -0.70943062         nan\n",
      "         nan         nan -0.41070407 -0.15047423]  Accuracy: 0.8652173913043478  F1 Score: 0.9054878048780487 Precision training: 0.8366197183098592 Recall training: <function recall at 0x000002DEF8F891E0> ]\n",
      "TEST: [Iteracion:  900 Accuracy: 0.8531468531468531  F1 Score: 0.8944723618090452 Precision Test: 0.8165137614678899 Recall Test: 0.9888888888888889 ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#####################\n",
      "TRAINING: [Iteracion:  1200  Costo:  [-0.18213168 -0.0960882  -0.60728294 -0.24670609 -0.19156721         nan\n",
      " -0.23357635 -0.3843796          nan         nan         nan         nan\n",
      " -0.30089298 -0.11779331 -0.22962634 -0.28855593 -0.39432744 -0.50156954\n",
      "         nan         nan -0.17703533 -0.23361887 -0.17619765 -0.0664422\n",
      "         nan         nan -0.34302943 -0.35139802 -0.22198726 -0.2230039\n",
      "         nan -0.18204865 -0.4356537  -0.32608547         nan -0.25697804\n",
      " -0.40510203 -0.15443072 -0.09220223 -0.11369251 -0.24844246         nan\n",
      "         nan -0.12327809 -0.08049928 -0.11110538 -0.19856442 -0.30106286\n",
      " -0.36619427 -0.40510203 -0.34089871         nan         nan -0.24161324\n",
      "         nan -0.54216043 -0.16148276 -0.23357635 -0.24818246 -0.17955513\n",
      "         nan -0.43842403         nan -0.52722364         nan -0.30106286\n",
      "         nan -0.08195262 -0.08034853         nan -0.23361887 -0.54997311\n",
      "         nan -0.2505541          nan -0.12979818 -0.59329414 -0.24603421\n",
      "         nan         nan -0.29567904 -0.14659185         nan -0.1222177\n",
      "         nan         nan -0.50150111 -0.17146426         nan         nan\n",
      " -0.48093469 -0.40510203 -0.25697804 -0.31716181         nan -0.37001278\n",
      " -0.33411813         nan -0.37346336 -0.1862593  -0.29235046         nan\n",
      "         nan -0.19037016 -0.08195262         nan -0.32128212         nan\n",
      "         nan         nan         nan -0.4173372  -0.19856442         nan\n",
      "         nan -0.09220223 -0.39432744 -0.29499682 -0.12527974 -0.50150111\n",
      " -0.12784575         nan -0.24015359 -0.33734541         nan -0.3843796\n",
      "         nan         nan -0.16161613 -0.18204865 -0.17261375         nan\n",
      " -0.13655556 -0.29713367         nan         nan         nan         nan\n",
      "         nan -0.23361887         nan -0.2927735          nan         nan\n",
      "         nan -0.1018506  -0.23497505         nan -0.38972025 -0.17045326\n",
      "         nan         nan -0.53361149 -0.21362864         nan         nan\n",
      "         nan         nan         nan -0.1888454          nan -0.27356302\n",
      " -0.13285341 -0.0639242          nan         nan -0.31991184 -0.35920291\n",
      " -0.24002144 -0.10236298 -0.24603421 -0.65178544 -0.20880511         nan\n",
      " -0.13285341         nan         nan -0.31991184 -0.22522885 -0.59697872\n",
      " -0.13769075         nan -0.16074803         nan -0.19476229 -0.23630433\n",
      " -0.17475946 -0.4173372  -1.13605216         nan -0.2478236  -0.24312568\n",
      " -0.52722364 -0.17261375 -0.29235046 -0.24312568 -0.25575742 -0.39618188\n",
      " -0.15216082         nan         nan -0.17955513 -0.35139802 -0.11110538\n",
      " -0.35549133         nan -0.36100635 -0.20101387 -0.22522885 -0.20365127\n",
      " -0.19856442 -0.2230039  -0.10523667         nan -0.40734884         nan\n",
      " -0.40735738 -0.1946674          nan -0.52722364 -0.1946674  -0.18278693\n",
      "         nan -0.18204865 -0.22581908         nan -0.1453607  -0.11110538\n",
      " -0.18806964 -0.08195262 -0.30431658 -0.19856442 -0.29895995 -0.23811793\n",
      "         nan -0.17834579 -0.30106286         nan -0.19037016 -0.30854426\n",
      " -0.20196111 -0.08900239 -0.16074803 -0.13769075         nan -0.08034853\n",
      " -0.18404453 -0.13526083 -0.09220223         nan -0.11779331         nan\n",
      "         nan -0.58288092 -0.33411813 -0.13769075         nan         nan\n",
      " -0.14918503 -0.2758313          nan         nan         nan -0.17426021\n",
      " -0.59329414 -0.26246049         nan -0.18213168         nan -0.30358847\n",
      " -0.30174746         nan         nan         nan -0.10179615 -0.24015359\n",
      " -0.18446882 -0.24002144 -0.13009936 -1.13605216 -0.11096758 -0.33827482\n",
      "         nan         nan         nan -0.28699887         nan -0.10603289\n",
      " -0.53361149         nan -0.29713367 -0.28855593 -0.10523667         nan\n",
      " -0.0664422  -0.24670609 -0.16829994 -0.34089871 -0.41875007 -0.20095326\n",
      "         nan -0.25697804 -0.28754002         nan -0.17475946 -0.18204865\n",
      " -0.30174746 -0.35920291         nan         nan         nan         nan\n",
      "         nan -0.17955513         nan -0.20365127         nan -0.11096758\n",
      " -0.16262957 -0.18652777 -0.54997311 -0.28603842 -0.13717883 -0.22581908\n",
      " -0.15216082 -0.55805349         nan -0.59697872 -0.07301417 -0.24015359\n",
      " -0.11369251 -0.1453607  -0.13095731 -0.1862593          nan -0.10523667\n",
      " -0.18278693         nan -0.2230039  -0.59329414         nan -0.18204865\n",
      " -0.24713368 -0.30431658         nan         nan -0.27262855         nan\n",
      " -0.13655556 -0.37556959 -0.55399031 -0.53361149         nan         nan\n",
      "         nan         nan -0.39432744 -0.12103087 -0.34438019 -0.05471721\n",
      " -1.52508216 -0.18278693 -0.11226995 -0.10179615 -0.21915139 -0.35407774\n",
      " -0.12361449         nan -0.09220223 -0.18446882 -0.2758313  -0.16148276\n",
      "         nan -0.52722364 -0.30174746 -0.17925302 -0.23630433 -0.61954124\n",
      " -0.30854426 -0.18681799         nan         nan         nan         nan\n",
      "         nan         nan -0.50150111 -0.12223717         nan -0.18910679\n",
      " -0.41809193         nan         nan         nan         nan -0.11779331\n",
      "         nan -0.60728294         nan         nan -0.52722364         nan\n",
      " -0.32030992 -0.17955513 -0.36448385 -0.20129432 -0.16161613 -0.37049292\n",
      " -0.1137571  -0.41809193 -0.32608547 -0.24002144 -0.20129432 -0.1222177\n",
      "         nan -0.30358847 -0.13769075 -0.25764442 -0.13769075         nan\n",
      "         nan         nan -0.19183506         nan         nan -0.37001278\n",
      "         nan         nan -0.13769075         nan -0.11226995         nan\n",
      " -0.24603421 -0.37556959 -0.28603842         nan         nan         nan\n",
      " -0.18078781 -0.54216043 -0.15649418         nan -0.42797768 -0.10996203\n",
      " -0.23497505 -0.30089298         nan -0.20880511 -0.1546654          nan\n",
      " -0.11586745 -0.25764442 -0.23497505         nan -0.73792229         nan\n",
      "         nan         nan -0.3843796  -0.1137571 ]  Accuracy: 0.8804347826086957  F1 Score: 0.9152542372881356 Precision training: 0.853448275862069 Recall training: <function recall at 0x000002DEF8F891E0> ]\n",
      "TEST: [Iteracion:  1200 Accuracy: 0.8671328671328671  F1 Score: 0.9035532994923858 Precision Test: 0.8317757009345794 Recall Test: 0.9888888888888889 ]\n"
     ]
    }
   ],
   "source": [
    "## Llama a la funcion de gradient descent, recurda definir un nombre diferente para el modelo (~1 linea) ##\n",
    "modelo_3_theta_values, cost_vect, acc_vect, acc_vect_test = \\\n",
    "gradient_descent(X_train_features, Y_train_m3, theta_vector, alpha, iterations, X_test_features, y_test, 'modelo_3')\n",
    "### FIN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2defae9fbe0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Funcion para graficar el costo y  accuracy durante cada iteracion\n",
    "helper.training_graph(cost_vect, acc_vect, acc_vect_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy con el Test set [Modelo 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = prediccion(X_test_features, modelo_3_theta_values, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 86.01398601398601 %\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy:',accuracy_score(y_test, y_predict)* 100 ,'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Matriz de confusion Test Set [Modelo 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[35, 18],\n",
       "       [ 2, 88]], dtype=int64)"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision:  0.8301886792452831\n",
      "recall:  0.9777777777777777\n"
     ]
    }
   ],
   "source": [
    "print('precision: ',precision_score(y_test, y_predict))\n",
    "\n",
    "print('recall: ', recall_score(y_test, y_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento modelo 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sampleo bootstrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_m4, Y_train_m4 = resample(X_train, y_train, n_samples = 460)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtrar Features\n",
    "Solo seleccionar los features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Ingresa en la lista el nombre de las fatrues que quieres utlizar en este modelo (~1 linea)\n",
    "'''\n",
    "features_filtro = ['mean radius', 'mean texture', 'mean perimeter', 'mean area','mean smoothness']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_m4 = helper.fitrar_nombre(X_train_m4,data.feature_names,features_filtro)\n",
    "X_test_features = helper.fitrar_nombre(X_test,data.feature_names,features_filtro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Numero de iteracions ##\n",
    "iterations = 2000 # Define el numero de iteraciones #\n",
    "\n",
    "## Alpha ##\n",
    "alpha = 0.03 # Define el valor de alpha #\n",
    "\n",
    "#### No hay que cambiar el codigo despues de esta linea #####\n",
    "## Numero de feautres ##\n",
    "numero_features = X_train_m4.shape[1] + 1\n",
    "\n",
    "## Creamos un vector donde se almacenan los valores de Theta,\n",
    "## lo inicializamos con numeros aleatorios\n",
    "theta_vector = np.random.rand(numero_features)\n",
    "\n",
    "## Normaliza las features de X_train_features (~1 linea) ##\n",
    "X_train_m4 = normalizar(X_train_m4)\n",
    "\n",
    "# Creando la Matriz X de features\n",
    "X_train_features = np.ones((X_train_m4.shape[0], numero_features))\n",
    "X_train_features[:,:-1] = X_train_m4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin VDK\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:43: RuntimeWarning: invalid value encountered in log\n",
      "C:\\Users\\Admin VDK\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\Admin VDK\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#####################\n",
      "TRAINING: [Iteracion:  0  Costo:  [        nan         nan -0.59747523         nan -0.58413909         nan\n",
      " -0.61287578         nan         nan -0.63892686         nan         nan\n",
      " -0.61565088 -0.60341901 -0.69133669 -0.58655678 -0.65906784 -0.62116866\n",
      " -0.63493024         nan         nan -0.63521372         nan -0.62193955\n",
      "         nan         nan -0.60456542         nan -0.6207925  -0.62071959\n",
      " -0.68464656 -0.6389117  -0.63946216 -0.60408759 -0.65456832         nan\n",
      " -0.54120521 -0.63649261 -0.65598734 -0.61707733 -0.65728781 -0.59104757\n",
      "         nan -0.64881914         nan         nan -0.62453119         nan\n",
      " -0.60215153 -0.55937706         nan         nan -0.67737735 -0.63791133\n",
      " -0.67737735         nan         nan -0.61825355 -0.62966648 -0.4963101\n",
      "         nan -0.67904919         nan         nan -0.59224958         nan\n",
      " -0.60559635         nan -0.56547032         nan -0.63475229 -0.58643182\n",
      " -0.68018039 -0.62458862 -0.68464656         nan -0.66038396         nan\n",
      "         nan         nan -0.64745815 -0.57350171 -0.56106272         nan\n",
      " -0.65598734 -0.56617457         nan -0.56545839 -0.66107813 -0.60618923\n",
      " -0.61953733 -0.6289812          nan -0.64777978         nan -0.65837055\n",
      " -0.63871778 -0.62925879 -0.51958524 -0.63521372 -0.55609501 -0.6065602\n",
      "         nan -0.63320102 -0.56835837 -0.62019789         nan -0.6097652\n",
      " -0.58526543         nan         nan         nan -0.68213052         nan\n",
      " -0.66107813         nan -0.56547032         nan -0.67737735 -0.56545839\n",
      " -0.67245892 -0.64074262 -0.65438527         nan         nan -0.64195892\n",
      " -0.64149637 -0.62439916 -0.6389117  -0.64461954 -0.62417315 -0.63360856\n",
      "         nan -0.67720023 -0.64613681 -0.60341901         nan         nan\n",
      " -0.59370758 -0.69199107 -0.63789741 -0.62602843         nan -0.59022407\n",
      " -0.61832472         nan         nan -0.67560546 -0.64872043 -0.58252683\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan -0.68661051 -0.55978341 -0.65010834 -0.62116866         nan\n",
      " -0.64777978 -0.67069597         nan -0.6323106  -0.54120521         nan\n",
      "         nan         nan         nan -0.58792005         nan         nan\n",
      "         nan -0.58910263         nan -0.62519349 -0.67552599 -0.67524612\n",
      "         nan -0.62811956         nan         nan         nan -0.52777593\n",
      " -0.62282736         nan -0.6443463          nan         nan -0.58400664\n",
      "         nan         nan -0.54084637         nan         nan -0.62603582\n",
      " -0.63699286         nan -0.66421801 -0.67720023         nan         nan\n",
      " -0.66219595         nan -0.70141436 -0.60422739         nan -0.66107813\n",
      " -0.62124399         nan -0.64187013 -0.61832472 -0.6058184  -0.60347094\n",
      "         nan         nan         nan         nan -0.63307264 -0.58413909\n",
      "         nan -0.6058184  -0.61179128         nan -0.61364893 -0.67552599\n",
      " -0.60526551 -0.6350211          nan         nan -0.60422739 -0.62602843\n",
      "         nan -0.66270815 -0.59486974         nan -0.62602843 -0.58910263\n",
      " -0.63946216 -0.56407911 -0.58252683 -0.65273664         nan -0.62124399\n",
      " -0.62282736 -0.61679165 -0.5958183          nan -0.56835837         nan\n",
      " -0.64450929         nan         nan -0.59634884 -0.63789741         nan\n",
      " -0.61449968         nan         nan         nan         nan         nan\n",
      " -0.6207925          nan -0.60455009 -0.65273664         nan -0.61449968\n",
      " -0.65105599 -0.59934369 -0.64881914 -0.66270815         nan -0.63861384\n",
      "         nan -0.58495856 -0.62467036         nan         nan         nan\n",
      "         nan -0.58592619 -0.63861384         nan -0.6175597          nan\n",
      " -0.60456542 -0.70141436         nan         nan         nan -0.65078577\n",
      "         nan -0.63307264 -0.59433871         nan -0.66728588         nan\n",
      " -0.62092209         nan -0.63039345 -0.58655678         nan -0.71022976\n",
      "         nan -0.59760917 -0.58138294 -0.63475229 -0.66210673 -0.63193945\n",
      " -0.66038396 -0.4963101  -0.60716632         nan         nan         nan\n",
      "         nan -0.63521372         nan -0.61684008 -0.59634884         nan\n",
      " -0.63750398 -0.55978341 -0.55464262 -0.65728781         nan         nan\n",
      " -0.67720023         nan         nan -0.6065602  -0.59092992         nan\n",
      " -0.62282736 -0.67720023 -0.62300405         nan -0.65105599 -0.59760917\n",
      " -0.65675355         nan         nan -0.61423569         nan -0.65837055\n",
      " -0.61179128 -0.65837055 -0.59486974 -0.61179128 -0.56723326 -0.59022407\n",
      " -0.61334259         nan -0.68862216 -0.59760917 -0.6175597  -0.63946216\n",
      " -0.64074262 -0.65675355         nan -0.60901086         nan -0.64428819\n",
      "         nan -0.63739932         nan -0.59098528         nan -0.62300405\n",
      " -0.64745815 -0.61953733         nan         nan -0.6577142  -0.59760917\n",
      "         nan -0.66107813 -0.58463538 -0.66038396         nan -0.61287578\n",
      "         nan -0.66728588 -0.60422739 -0.61953733         nan -0.63871778\n",
      "         nan -0.56328325 -0.67245892 -0.56429776 -0.55609501         nan\n",
      " -0.63871778 -0.67552599 -0.63791133 -0.56720743         nan         nan\n",
      " -0.65105599 -0.63037236 -0.56097482         nan -0.62124399         nan\n",
      "         nan -0.59201183 -0.60044028         nan -0.58138294 -0.67560546\n",
      " -0.64461954 -0.68307189 -0.62608911 -0.63136845         nan -0.56555995\n",
      " -0.63320102 -0.66839684 -0.6536815          nan -0.63079637 -0.61684008\n",
      " -0.66275287 -0.58623434         nan -0.62124399 -0.62966648 -0.63649261\n",
      "         nan -0.60456542         nan -0.60697548 -0.68156186 -0.60228588\n",
      " -0.62193955         nan -0.64461954 -0.5958183          nan -0.61950728\n",
      "         nan         nan -0.65935577 -0.56407911 -0.60215153 -0.62568899\n",
      "         nan -0.67245892 -0.54120521 -0.56107261 -0.59934369         nan\n",
      " -0.67524612 -0.63037236 -0.68661051 -0.60341901]  Accuracy: 0.6173913043478261  F1 Score: 0.7634408602150538 Precision training: 0.6214442013129103 Recall training: <function recall at 0x000002DEF8F891E0> ]\n",
      "TEST: [Iteracion:  0 Accuracy: 0.6153846153846154  F1 Score: 0.7619047619047619 Precision Test: 0.624113475177305 Recall Test: 0.9777777777777777 ]\n",
      "#####################\n",
      "TRAINING: [Iteracion:  300  Costo:  [        nan         nan -0.50087856         nan -0.54240982         nan\n",
      " -0.45123606         nan         nan -0.36803026         nan         nan\n",
      " -0.41938976 -0.51219936 -0.249719   -0.51493567 -0.30275806 -0.38155116\n",
      " -0.37929334         nan         nan -0.36808198         nan -0.43992351\n",
      "         nan         nan -0.46530415         nan -0.40045968 -0.37005015\n",
      " -0.2469017  -0.36560927 -0.34633118 -0.50017635 -0.30046113         nan\n",
      " -0.75429805 -0.39501793 -0.34762484 -0.40124291 -0.30996015 -0.53789451\n",
      "         nan -0.31934294         nan         nan -0.39089425         nan\n",
      " -0.51495527 -0.60803177         nan         nan -0.26131478 -0.37045834\n",
      " -0.26131478         nan         nan -0.45402583 -0.41191712 -1.11984292\n",
      "         nan -0.25061048         nan         nan -0.48228951         nan\n",
      " -0.4564271          nan -0.62649528         nan -0.4031417  -0.51114428\n",
      " -0.29452996 -0.39539585 -0.2469017          nan -0.29406717         nan\n",
      "         nan         nan -0.32016848 -0.59546786 -0.6159347          nan\n",
      " -0.34762484 -0.63746744         nan -0.62169508 -0.31187389 -0.46519823\n",
      " -0.42930393 -0.36371571         nan -0.34076228         nan -0.30941929\n",
      " -0.35799907 -0.40316168 -0.93907438 -0.36808198 -0.71138682 -0.46869573\n",
      "         nan -0.37542424 -0.6295952  -0.41596118         nan -0.46723085\n",
      " -0.60462722         nan         nan         nan -0.26242758         nan\n",
      " -0.31187389         nan -0.62649528         nan -0.26131478 -0.62169508\n",
      " -0.27802862 -0.34475688 -0.33226535         nan         nan -0.36800726\n",
      " -0.35363326 -0.43963112 -0.36560927 -0.34815635 -0.42384454 -0.39297511\n",
      "         nan -0.28564621 -0.32983387 -0.51219936         nan         nan\n",
      " -0.52528981 -0.25268189 -0.39095827 -0.40948421         nan -0.54587225\n",
      " -0.40680848         nan         nan -0.24449553 -0.31760126 -0.57251634\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan -0.27506425 -0.69449428 -0.35107092 -0.38155116         nan\n",
      " -0.34076228 -0.29105811         nan -0.35973268 -0.75429805         nan\n",
      "         nan         nan         nan -0.53084321         nan         nan\n",
      "         nan -0.52189562         nan -0.3832992  -0.28266095 -0.2861547\n",
      "         nan -0.3845252          nan         nan         nan -0.83714305\n",
      " -0.41894032         nan -0.33590629         nan         nan -0.57187016\n",
      "         nan         nan -0.72954856         nan         nan -0.36682602\n",
      " -0.37114804         nan -0.28857961 -0.28564621         nan         nan\n",
      " -0.28783511         nan -0.23981251 -0.42706796         nan -0.31187389\n",
      " -0.42648319         nan -0.36497603 -0.40680848 -0.48763279 -0.46798396\n",
      "         nan         nan         nan         nan -0.35686765 -0.54240982\n",
      "         nan -0.48763279 -0.49179555         nan -0.4516296  -0.28266095\n",
      " -0.4862663  -0.38179158         nan         nan -0.42706796 -0.40948421\n",
      "         nan -0.30879059 -0.49706556         nan -0.40948421 -0.52189562\n",
      " -0.34633118 -0.60024913 -0.57251634 -0.31036797         nan -0.42648319\n",
      " -0.41894032 -0.4410471  -0.46238251         nan -0.6295952          nan\n",
      " -0.3636958          nan         nan -0.49573618 -0.39095827         nan\n",
      " -0.44848681         nan         nan         nan         nan         nan\n",
      " -0.40045968         nan -0.46673485 -0.31036797         nan -0.44848681\n",
      " -0.32382674 -0.51138507 -0.31934294 -0.30879059         nan -0.3661447\n",
      "         nan -0.50870762 -0.385279           nan         nan         nan\n",
      "         nan -0.53009419 -0.3661447          nan -0.40471719         nan\n",
      " -0.46530415 -0.23981251         nan         nan         nan -0.33535468\n",
      "         nan -0.35686765 -0.46948028         nan -0.30516066         nan\n",
      " -0.42159033         nan -0.36987294 -0.51493567         nan -0.20497049\n",
      "         nan -0.49381603 -0.54044941 -0.4031417  -0.29791514 -0.37631758\n",
      " -0.29406717 -1.11984292 -0.43613418         nan         nan         nan\n",
      "         nan -0.36808198         nan -0.38951249 -0.49573618         nan\n",
      " -0.37076585 -0.69449428 -0.6589381  -0.30996015         nan         nan\n",
      " -0.28564621         nan         nan -0.46869573 -0.51862278         nan\n",
      " -0.41894032 -0.28564621 -0.42577864         nan -0.32382674 -0.49381603\n",
      " -0.33865468         nan         nan -0.43834669         nan -0.30941929\n",
      " -0.49179555 -0.30941929 -0.49706556 -0.49179555 -0.63571282 -0.54587225\n",
      " -0.46130538         nan -0.24694606 -0.49381603 -0.40471719 -0.34633118\n",
      " -0.34475688 -0.33865468         nan -0.47563293         nan -0.31635646\n",
      "         nan -0.34512908         nan -0.53934645         nan -0.42577864\n",
      " -0.32016848 -0.42930393         nan         nan -0.30370388 -0.49381603\n",
      "         nan -0.31187389 -0.55197518 -0.29406717         nan -0.45123606\n",
      "         nan -0.30516066 -0.42706796 -0.42930393         nan -0.35799907\n",
      "         nan -0.63213156 -0.27802862 -0.66278002 -0.71138682         nan\n",
      " -0.35799907 -0.28266095 -0.37045834 -0.61743844         nan         nan\n",
      " -0.32382674 -0.39854668 -0.66658566         nan -0.42648319         nan\n",
      "         nan -0.56139571 -0.46827851         nan -0.54044941 -0.24449553\n",
      " -0.34815635 -0.25475693 -0.3715124  -0.37189751         nan -0.65388638\n",
      " -0.37542424 -0.27720571 -0.3183979          nan -0.37319945 -0.38951249\n",
      " -0.29209883 -0.54685735         nan -0.42648319 -0.41191712 -0.39501793\n",
      "         nan -0.46530415         nan -0.46258403 -0.2609712  -0.45974331\n",
      " -0.43992351         nan -0.34815635 -0.46238251         nan -0.42669045\n",
      "         nan         nan -0.27235653 -0.60024913 -0.51495527 -0.38054795\n",
      "         nan -0.27802862 -0.75429805 -0.66713908 -0.51138507         nan\n",
      " -0.2861547  -0.39854668 -0.27506425 -0.51219936]  Accuracy: 0.8739130434782608  F1 Score: 0.9046052631578947 Precision training: 0.8566978193146417 Recall training: <function recall at 0x000002DEF8F891E0> ]\n",
      "TEST: [Iteracion:  300 Accuracy: 0.8741258741258742  F1 Score: 0.90625 Precision Test: 0.8529411764705882 Recall Test: 0.9666666666666667 ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#####################\n",
      "TRAINING: [Iteracion:  600  Costo:  [        nan         nan -0.40628338         nan -0.46365501         nan\n",
      " -0.34134223         nan         nan -0.24339353         nan         nan\n",
      " -0.30399077 -0.41785388 -0.12691915 -0.42837865 -0.17643949 -0.26163916\n",
      " -0.25598376         nan         nan -0.24415432         nan -0.32560794\n",
      "         nan         nan -0.36061052         nan -0.28208936 -0.24958922\n",
      " -0.12558446 -0.24140174 -0.22128081 -0.40303396 -0.17501581         nan\n",
      " -0.78350904 -0.27202137 -0.21918228 -0.28359916 -0.18324189 -0.45512652\n",
      "         nan -0.19363759         nan         nan -0.27065794         nan\n",
      " -0.42188182 -0.56248958         nan         nan -0.13810388 -0.24606688\n",
      " -0.13810388         nan         nan -0.34298039 -0.2921791  -1.39611035\n",
      "         nan -0.12913465         nan         nan -0.38538391         nan\n",
      " -0.34961561         nan -0.58564827         nan -0.28151146 -0.42279625\n",
      " -0.16585296 -0.27556022 -0.12558446         nan -0.16837852         nan\n",
      "         nan         nan -0.19455862 -0.53911708 -0.57261187         nan\n",
      " -0.21918228 -0.59996419         nan -0.57826924 -0.18441604 -0.36003897\n",
      " -0.3144629  -0.24111281         nan -0.21434552         nan -0.18249459\n",
      " -0.23304628 -0.28274634 -1.08533972 -0.24415432 -0.71097115 -0.36386467\n",
      "         nan -0.25230129 -0.58821595 -0.29917263         nan -0.36116126\n",
      " -0.54571448         nan         nan         nan -0.13834044         nan\n",
      " -0.18441604         nan -0.58564827         nan -0.13810388 -0.57826924\n",
      " -0.15257014 -0.21956185 -0.20467245         nan         nan -0.24270945\n",
      " -0.22806099 -0.32439903 -0.24140174 -0.22213276 -0.30672654 -0.27061054\n",
      "         nan -0.15850898 -0.20416532 -0.41785388         nan         nan\n",
      " -0.4386552  -0.12923741 -0.26747748 -0.29050474         nan -0.46584001\n",
      " -0.28941839         nan         nan -0.12476195 -0.19189473 -0.50382606\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan -0.14817984 -0.68438746 -0.22383101 -0.26163916         nan\n",
      " -0.21434552 -0.16422655         nan -0.23635208 -0.78350904         nan\n",
      "         nan         nan         nan -0.44732117         nan         nan\n",
      "         nan -0.43530908         nan -0.26284665 -0.15644148 -0.15926743\n",
      "         nan -0.2630049          nan         nan         nan -0.91883011\n",
      " -0.30165614         nan -0.21011643         nan         nan -0.50241856\n",
      "         nan         nan -0.74667681         nan         nan -0.24509112\n",
      " -0.24690915         nan -0.1629901  -0.15850898         nan         nan\n",
      " -0.16267768         nan -0.11819918 -0.31604676         nan -0.18441604\n",
      " -0.31066589         nan -0.23954951 -0.28941839 -0.38672572 -0.3639335\n",
      "         nan         nan         nan         nan -0.2333067  -0.46365501\n",
      "         nan -0.38672572 -0.39018472         nan -0.3413478  -0.15644148\n",
      " -0.38553159 -0.25854496         nan         nan -0.31604676 -0.29050474\n",
      "         nan -0.18125382 -0.40284403         nan -0.29050474 -0.43530908\n",
      " -0.22128081 -0.54942386 -0.50382606 -0.18441547         nan -0.31066589\n",
      " -0.30165614 -0.32837817 -0.35994911         nan -0.58821595         nan\n",
      " -0.23762466         nan         nan -0.4005129  -0.26747748         nan\n",
      " -0.33770476         nan         nan         nan         nan         nan\n",
      " -0.28208936         nan -0.3621448  -0.18441547         nan -0.33770476\n",
      " -0.19744544 -0.41845275 -0.19363759 -0.18125382         nan -0.24136132\n",
      "         nan -0.42053261 -0.2645998          nan         nan         nan\n",
      "         nan -0.44750789 -0.24136132         nan -0.28744972         nan\n",
      " -0.36061052 -0.11819918         nan         nan         nan -0.20816351\n",
      "         nan -0.2333067  -0.36883982         nan -0.17738255         nan\n",
      " -0.30523969         nan -0.24705419 -0.42837865         nan -0.09188646\n",
      "         nan -0.39753151 -0.46257288 -0.28151146 -0.17162532 -0.25352906\n",
      " -0.16837852 -1.39611035 -0.32578799         nan         nan         nan\n",
      "         nan -0.24415432         nan -0.27101286 -0.4005129          nan\n",
      " -0.24625583 -0.68438746 -0.63646147 -0.18324189         nan         nan\n",
      " -0.15850898         nan         nan -0.36386467 -0.43096052         nan\n",
      " -0.30165614 -0.15850898 -0.30928678         nan -0.19744544 -0.39753151\n",
      " -0.21037664         nan         nan -0.32607675         nan -0.18249459\n",
      " -0.39018472 -0.18249459 -0.40284403 -0.39018472 -0.59710827 -0.46584001\n",
      " -0.35300328         nan -0.1250728  -0.39753151 -0.28744972 -0.22128081\n",
      " -0.21956185 -0.21037664         nan -0.37158942         nan -0.19157309\n",
      "         nan -0.2205502          nan -0.45697353         nan -0.30928678\n",
      " -0.19455862 -0.3144629          nan         nan -0.17757265 -0.39753151\n",
      "         nan -0.18441604 -0.47612807 -0.16837852         nan -0.34134223\n",
      "         nan -0.17738255 -0.31604676 -0.3144629          nan -0.23304628\n",
      "         nan -0.59458959 -0.15257014 -0.63663912 -0.71097115         nan\n",
      " -0.23304628 -0.15644148 -0.24606688 -0.57203527         nan         nan\n",
      " -0.19744544 -0.27745092 -0.64413672         nan -0.31066589         nan\n",
      "         nan -0.48519485 -0.36551969         nan -0.46257288 -0.12476195\n",
      " -0.22213276 -0.13199186 -0.24980149 -0.24894153         nan -0.62330808\n",
      " -0.25230129 -0.15262522 -0.19169017         nan -0.2507527  -0.27101286\n",
      " -0.16636828 -0.46867901         nan -0.31066589 -0.2921791  -0.27202137\n",
      "         nan -0.36061052         nan -0.35645114 -0.13719911 -0.35467959\n",
      " -0.32560794         nan -0.22213276 -0.35994911         nan -0.31134036\n",
      "         nan         nan -0.14960844 -0.54942386 -0.42188182 -0.25949087\n",
      "         nan -0.15257014 -0.78350904 -0.64484338 -0.41845275         nan\n",
      " -0.15926743 -0.27745092 -0.14817984 -0.41785388]  Accuracy: 0.8782608695652174  F1 Score: 0.9081967213114754 Precision training: 0.8575851393188855 Recall training: <function recall at 0x000002DEF8F891E0> ]\n",
      "TEST: [Iteracion:  600 Accuracy: 0.8601398601398601  F1 Score: 0.8969072164948454 Precision Test: 0.8365384615384616 Recall Test: 0.9666666666666667 ]\n",
      "#####################\n",
      "TRAINING: [Iteracion:  900  Costo:  [        nan         nan -0.35998723         nan -0.42652378         nan\n",
      " -0.28770426         nan         nan -0.18626348         nan         nan\n",
      " -0.24853815 -0.37176064 -0.08059109 -0.38644872 -0.12339644 -0.20554984\n",
      " -0.19876656         nan         nan -0.18727597         nan -0.27014563\n",
      "         nan         nan -0.30928232         nan -0.22590273 -0.19379568\n",
      " -0.07979612 -0.18447069 -0.16525541 -0.35539025 -0.1223855          nan\n",
      " -0.82361781 -0.21429032 -0.16208169 -0.22772057 -0.12946475 -0.41561304\n",
      "         nan -0.13929429         nan         nan -0.21410631         nan\n",
      " -0.37646072 -0.54700128         nan         nan -0.09013465 -0.18890402\n",
      " -0.09013465         nan         nan -0.28876233 -0.2351048  -1.63224603\n",
      "         nan -0.08281532         nan         nan -0.3378669          nan\n",
      " -0.29737759         nan -0.57346964         nan -0.2239464  -0.37998869\n",
      " -0.11314495 -0.21898361 -0.07979612         nan -0.11630927         nan\n",
      "         nan         nan -0.14018853 -0.51630086 -0.55870852         nan\n",
      " -0.16208169 -0.59033013         nan -0.56442891 -0.13029316 -0.30847243\n",
      " -0.25895597 -0.18489026         nan -0.15825029         nan -0.12872745\n",
      " -0.17637449 -0.22568289 -1.21802758 -0.18727597 -0.72859374 -0.31246008\n",
      "         nan -0.19533351 -0.57579209 -0.24318682         nan -0.30915168\n",
      " -0.52147499         nan         nan         nan -0.09012161         nan\n",
      " -0.13029316         nan -0.57346964         nan -0.09013465 -0.56442891\n",
      " -0.10226071 -0.16357849 -0.14887742         nan         nan -0.18535944\n",
      " -0.17144709 -0.26856091 -0.18447069 -0.16568802 -0.25041925 -0.21318326\n",
      "         nan -0.10704725 -0.14902841 -0.37176064         nan         nan\n",
      " -0.39674942 -0.08241629 -0.20971246 -0.23380355         nan -0.42796067\n",
      " -0.23346525         nan         nan -0.07947468 -0.13770692 -0.47305335\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan -0.09798912 -0.69476186 -0.16685025 -0.20554984         nan\n",
      " -0.15825029 -0.11223107         nan -0.18007056 -0.82361781         nan\n",
      "         nan         nan         nan -0.40732664         nan         nan\n",
      "         nan -0.39354578         nan -0.20644156 -0.1054379  -0.10778812\n",
      "         nan -0.2062224          nan         nan         nan -1.00010604\n",
      " -0.24540218         nan -0.15455075         nan         nan -0.47115428\n",
      "         nan         nan -0.777443           nan         nan -0.18899569\n",
      " -0.18977362         nan -0.11149213 -0.10704725         nan         nan\n",
      " -0.11133491         nan -0.07343914 -0.2622999          nan -0.13029316\n",
      " -0.25482338         nan -0.1823272  -0.23346525 -0.33721112 -0.31293325\n",
      "         nan         nan         nan         nan -0.17711701 -0.42652378\n",
      "         nan -0.33721112 -0.34021962         nan -0.28755698 -0.1054379\n",
      " -0.33606564 -0.20124489         nan         nan -0.2622999  -0.23380355\n",
      "         nan -0.12739883 -0.35669415         nan -0.23380355 -0.39354578\n",
      " -0.16525541 -0.53037304 -0.47305335 -0.13077712         nan -0.25482338\n",
      " -0.24540218 -0.27363495 -0.30975704         nan -0.57579209         nan\n",
      " -0.18027163         nan         nan -0.35385728 -0.20971246         nan\n",
      " -0.28368825         nan         nan         nan         nan         nan\n",
      " -0.22590273         nan -0.31088396 -0.13077712         nan -0.28368825\n",
      " -0.14257836 -0.37310395 -0.13929429 -0.12739883         nan -0.18430472\n",
      "         nan -0.37777122 -0.208103           nan         nan         nan\n",
      "         nan -0.40798588 -0.18430472         nan -0.23158889         nan\n",
      " -0.30928232 -0.07343914         nan         nan         nan -0.15227347\n",
      "         nan -0.17711701 -0.31950933         nan -0.12378659         nan\n",
      " -0.24929289         nan -0.19046978 -0.38644872         nan -0.05347117\n",
      "         nan -0.35033939 -0.42588135 -0.2239464  -0.11904398 -0.19663709\n",
      " -0.11630927 -1.63224603 -0.27214292         nan         nan         nan\n",
      "         nan -0.18727597         nan -0.21518076 -0.35385728         nan\n",
      " -0.18906837 -0.69476186 -0.63746291 -0.12946475         nan         nan\n",
      " -0.10704725         nan         nan -0.31246008 -0.38849647         nan\n",
      " -0.24540218 -0.10704725 -0.25318617         nan -0.14257836 -0.35033939\n",
      " -0.15392976         nan         nan -0.27155322         nan -0.12872745\n",
      " -0.34021962 -0.12872745 -0.35669415 -0.34021962 -0.58668143 -0.42796067\n",
      " -0.29998348         nan -0.0792326  -0.35033939 -0.23158889 -0.16525541\n",
      " -0.16357849 -0.15392976         nan -0.32048521         nan -0.13770658\n",
      "         nan -0.16473606         nan -0.41770556         nan -0.25318617\n",
      " -0.14018853 -0.25895597         nan         nan -0.12447553 -0.35033939\n",
      "         nan -0.13029316 -0.44076301 -0.11630927         nan -0.28770426\n",
      "         nan -0.12378659 -0.2622999  -0.25895597         nan -0.17637449\n",
      "         nan -0.58473233 -0.10226071 -0.63518751 -0.72859374         nan\n",
      " -0.17637449 -0.1054379  -0.18890402 -0.55671809         nan         nan\n",
      " -0.14257836 -0.22028772 -0.64525432         nan -0.25482338         nan\n",
      "         nan -0.44973867 -0.31511776         nan -0.42588135 -0.07947468\n",
      " -0.16568802 -0.08495625 -0.193514   -0.19221213         nan -0.61866908\n",
      " -0.19533351 -0.10253265 -0.13719872         nan -0.19409087 -0.21518076\n",
      " -0.11447541 -0.43190739         nan -0.25482338 -0.2351048  -0.21429032\n",
      "         nan -0.30928232         nan -0.30447134 -0.08921314 -0.30323111\n",
      " -0.27014563         nan -0.16568802 -0.30975704         nan -0.25570028\n",
      "         nan         nan -0.10038717 -0.53037304 -0.37646072 -0.20302462\n",
      "         nan -0.10226071 -0.82361781 -0.64608999 -0.37310395         nan\n",
      " -0.10778812 -0.22028772 -0.09798912 -0.37176064]  Accuracy: 0.8782608695652174  F1 Score: 0.9075907590759075 Precision training: 0.8620689655172413 Recall training: <function recall at 0x000002DEF8F891E0> ]\n",
      "TEST: [Iteracion:  900 Accuracy: 0.8601398601398601  F1 Score: 0.8969072164948454 Precision Test: 0.8365384615384616 Recall Test: 0.9666666666666667 ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#####################\n",
      "TRAINING: [Iteracion:  1200  Costo:  [        nan         nan -0.33183024         nan -0.4046783          nan\n",
      " -0.25518832         nan         nan -0.15337195         nan         nan\n",
      " -0.21535641 -0.3437586  -0.05756629 -0.36118073 -0.09485013 -0.17272386\n",
      " -0.16550362         nan         nan -0.15448902         nan -0.2367021\n",
      "         nan         nan -0.27804091         nan -0.19263226 -0.16139403\n",
      " -0.05703022 -0.15171098 -0.13356351 -0.32636468 -0.09407309         nan\n",
      " -0.86309394 -0.18041582 -0.12998781 -0.19460193 -0.10028928 -0.39209185\n",
      "         nan -0.1093963          nan         nan -0.18086301         nan\n",
      " -0.34889429 -0.54139078         nan         nan -0.06573679 -0.15592546\n",
      " -0.06573679         nan         nan -0.25589003 -0.20119643 -1.83305718\n",
      "         nan -0.05960817         nan         nan -0.30893025         nan\n",
      " -0.26564818         nan -0.57046079         nan -0.18995325 -0.35416553\n",
      " -0.08531991 -0.18562718 -0.05703022         nan -0.0885677          nan\n",
      "         nan         nan -0.11024284 -0.50505    -0.55434719         nan\n",
      " -0.12998781 -0.58938669         nan -0.56016587 -0.10092264 -0.27708447\n",
      " -0.22559756 -0.15249939         nan -0.12676732         nan -0.09959589\n",
      " -0.1440087  -0.19194085 -1.33432916 -0.15448902 -0.74961106 -0.28116295\n",
      "         nan -0.16228719 -0.57263482 -0.20976344         nan -0.27749283\n",
      " -0.50924694         nan         nan         nan -0.06563267         nan\n",
      " -0.10092264         nan -0.57046079         nan -0.06573679 -0.56016587\n",
      " -0.07610322 -0.13196134 -0.11791088         nan         nan -0.15238166\n",
      " -0.13926592 -0.23492178 -0.15171098 -0.13377397 -0.21672133 -0.17948977\n",
      "         nan -0.08011303 -0.1183507  -0.3437586          nan         nan\n",
      " -0.37152363 -0.05905855 -0.17591326 -0.20012094         nan -0.40559481\n",
      " -0.20021094         nan         nan -0.05691642 -0.10795505 -0.4558675\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan -0.07216875 -0.70987901 -0.13463977 -0.17272386         nan\n",
      " -0.12676732 -0.0847557          nan -0.14778335 -0.86309394         nan\n",
      "         nan         nan         nan -0.38345439         nan         nan\n",
      "         nan -0.36845108         nan -0.17341557 -0.07878368 -0.08079803\n",
      "         nan -0.17302012         nan         nan         nan -1.07378117\n",
      " -0.21180184         nan -0.12346375         nan         nan -0.45360376\n",
      "         nan         nan -0.80920198         nan         nan -0.15655651\n",
      " -0.15678806         nan -0.08426772 -0.08011303         nan         nan\n",
      " -0.08418323         nan -0.05159704 -0.22993753         nan -0.10092264\n",
      " -0.22132773         nan -0.14950335 -0.20021094 -0.3070322  -0.28188897\n",
      "         nan         nan         nan         nan -0.14496172 -0.4046783\n",
      "         nan -0.3070322  -0.30972505         nan -0.25496056 -0.07878368\n",
      " -0.30590567 -0.16788114         nan         nan -0.22993753 -0.20012094\n",
      "         nan -0.09829526 -0.32861748         nan -0.20012094 -0.36845108\n",
      " -0.13356351 -0.52202358 -0.4558675  -0.10159735         nan -0.22132773\n",
      " -0.21180184 -0.24058019 -0.27921594         nan -0.57263482         nan\n",
      " -0.14744649         nan         nan -0.32546238 -0.17591326         nan\n",
      " -0.25097448         nan         nan         nan         nan         nan\n",
      " -0.19263226         nan -0.27968647 -0.10159735         nan -0.25097448\n",
      " -0.11229663 -0.34558434 -0.1093963  -0.09829526         nan -0.15151277\n",
      "         nan -0.35196267 -0.17501386         nan         nan         nan\n",
      "         nan -0.38441836 -0.15151277         nan -0.19840937         nan\n",
      " -0.27804091 -0.05159704         nan         nan         nan -0.1211305\n",
      "         nan -0.14496172 -0.2894763          nan -0.09497823         nan\n",
      " -0.21580895         nan -0.15774951 -0.36118073         nan -0.0357658\n",
      "         nan -0.3216056  -0.40431731 -0.18995325 -0.09092926 -0.16359772\n",
      " -0.0885677  -1.83305718 -0.23973625         nan         nan         nan\n",
      "         nan -0.15448902         nan -0.18230599 -0.32546238         nan\n",
      " -0.15608141 -0.70987901 -0.64493593 -0.10028928         nan         nan\n",
      " -0.08011303         nan         nan -0.28116295 -0.36289568         nan\n",
      " -0.21180184 -0.08011303 -0.21956676         nan -0.11229663 -0.3216056\n",
      " -0.1224541          nan         nan -0.2386439          nan -0.09959589\n",
      " -0.30972505 -0.09959589 -0.32861748 -0.30972505 -0.58510911 -0.40559481\n",
      " -0.2677584          nan -0.05649805 -0.3216056  -0.19840937 -0.13356351\n",
      " -0.13196134 -0.1224541          nan -0.28933199         nan -0.10809702\n",
      "         nan -0.13316319         nan -0.39436047         nan -0.21956676\n",
      " -0.11024284 -0.22559756         nan         nan -0.0958471  -0.3216056\n",
      "         nan -0.10092264 -0.42018461 -0.0885677          nan -0.25518832\n",
      "         nan -0.09497823 -0.22993753 -0.22559756         nan -0.1440087\n",
      "         nan -0.58355073 -0.07610322 -0.64075898 -0.74961106         nan\n",
      " -0.1440087  -0.07878368 -0.15592546 -0.55126695         nan         nan\n",
      " -0.11229663 -0.18659241 -0.65284778         nan -0.22132773         nan\n",
      "         nan -0.42914156 -0.28442446         nan -0.40431731 -0.05691642\n",
      " -0.13377397 -0.06131257 -0.16086636 -0.15937203         nan -0.6217039\n",
      " -0.16228719 -0.07643829 -0.10733536         nan -0.16122381 -0.18230599\n",
      " -0.08690979 -0.41032457         nan -0.22132773 -0.20119643 -0.18041582\n",
      "         nan -0.27804091         nan -0.27285919 -0.06488191 -0.27194674\n",
      " -0.2367021          nan -0.13377397 -0.27921594         nan -0.22231194\n",
      "         nan         nan -0.07478512 -0.52202358 -0.34889429 -0.17005507\n",
      "         nan -0.07610322 -0.86309394 -0.65379563 -0.34558434         nan\n",
      " -0.08079803 -0.18659241 -0.07216875 -0.3437586 ]  Accuracy: 0.8782608695652174  F1 Score: 0.9075907590759075 Precision training: 0.8620689655172413 Recall training: <function recall at 0x000002DEF8F891E0> ]\n",
      "TEST: [Iteracion:  1200 Accuracy: 0.8601398601398601  F1 Score: 0.8969072164948454 Precision Test: 0.8365384615384616 Recall Test: 0.9666666666666667 ]\n",
      "#####################\n",
      "TRAINING: [Iteracion:  1500  Costo:  [        nan         nan -0.31248683         nan -0.39007019         nan\n",
      " -0.23295899         nan         nan -0.13186169         nan         nan\n",
      " -0.19293249 -0.32452978 -0.04417298 -0.34395027 -0.07717229 -0.15096037\n",
      " -0.14357478         nan         nan -0.13302325         nan -0.21395104\n",
      "         nan         nan -0.25659827         nan -0.17035581 -0.14004864\n",
      " -0.04378327 -0.13030242 -0.11313292 -0.30639998 -0.07654414         nan\n",
      " -0.89957285 -0.15790781 -0.10940791 -0.17240499 -0.08210489 -0.37620109\n",
      "         nan -0.09055321         nan         nan -0.15873993         nan\n",
      " -0.32997844 -0.53980046         nan         nan -0.05128703 -0.13432071\n",
      " -0.05128703         nan         nan -0.23341557 -0.17842804 -2.00542546\n",
      "         nan -0.04602252         nan         nan -0.28904422         nan\n",
      " -0.2439136          nan -0.57101916         nan -0.16724872 -0.33653381\n",
      " -0.06835344 -0.16337436 -0.04378327         nan -0.07152411         nan\n",
      "         nan         nan -0.09135161 -0.49884816 -0.55378741         nan\n",
      " -0.10940791 -0.59167812         nan -0.55970603 -0.08261438 -0.25554227\n",
      " -0.202975   -0.13130231         nan -0.1066044          nan -0.08145734\n",
      " -0.12297938 -0.16937537 -1.43619707 -0.13302325 -0.77037177 -0.25967327\n",
      "         nan -0.14053943 -0.57308609 -0.18722561         nan -0.25576224\n",
      " -0.50229401         nan         nan         nan -0.05114561         nan\n",
      " -0.08261438         nan -0.57101916         nan -0.05128703 -0.55970603\n",
      " -0.06034007 -0.11160707 -0.09825548         nan         nan -0.13083737\n",
      " -0.11843704 -0.21205436 -0.13030242 -0.11321015 -0.19394831 -0.15710381\n",
      "         nan -0.06379926 -0.09884121 -0.32452978         nan         nan\n",
      " -0.35433092 -0.04543248 -0.15350995 -0.1775065          nan -0.39058972\n",
      " -0.17787137         nan         nan -0.04377032 -0.08923304 -0.44492245\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan -0.05674068 -0.72564287 -0.11388861 -0.15096037         nan\n",
      " -0.1066044  -0.06798768         nan -0.12672895 -0.89957285         nan\n",
      "         nan         nan         nan -0.3672924          nan         nan\n",
      "         nan -0.35136705         nan -0.15151596 -0.06266104 -0.06442817\n",
      "         nan -0.15102301         nan         nan         nan -1.13948443\n",
      " -0.18913358         nan -0.10360261         nan         nan -0.44236899\n",
      "         nan         nan -0.83910034         nan         nan -0.135263\n",
      " -0.13516525         nan -0.06764544 -0.06379926         nan         nan\n",
      " -0.06760086         nan -0.03907264 -0.20794763         nan -0.08261438\n",
      " -0.19864866         nan -0.12810152 -0.17787137 -0.28627452 -0.26057481\n",
      "         nan         nan         nan         nan -0.12403612 -0.39007019\n",
      "         nan -0.28627452 -0.28873081         nan -0.23268064 -0.06266104\n",
      " -0.28515879 -0.14585658         nan         nan -0.20794763 -0.1775065\n",
      "         nan -0.080213   -0.30933074         nan -0.1775065  -0.35136705\n",
      " -0.11313292 -0.51819412 -0.44492245 -0.0833701          nan -0.19864866\n",
      " -0.18913358 -0.21806508 -0.25825664         nan -0.57308609         nan\n",
      " -0.12608308         nan         nan -0.30595098 -0.15350995         nan\n",
      " -0.22863117         nan         nan         nan         nan         nan\n",
      " -0.17035581         nan -0.25827268 -0.0833701          nan -0.22863117\n",
      " -0.09316344 -0.32670455 -0.09055321 -0.080213           nan -0.1300962\n",
      "         nan -0.33433747 -0.1530608          nan         nan         nan\n",
      "         nan -0.36848055 -0.1300962          nan -0.17613312         nan\n",
      " -0.25659827 -0.03907264         nan         nan         nan -0.10129712\n",
      "         nan -0.12403612 -0.26884892         nan -0.07715837         nan\n",
      " -0.19318113         nan -0.13627047 -0.34395027         nan -0.02604912\n",
      "         nan -0.30185381 -0.3899156  -0.16724872 -0.07360885 -0.14183419\n",
      " -0.07152411 -2.00542546 -0.21765509         nan         nan         nan\n",
      "         nan -0.13302325         nan -0.16039539 -0.30595098         nan\n",
      " -0.13447138 -0.72564287 -0.65424158 -0.08210489         nan         nan\n",
      " -0.06379926         nan         nan -0.25967327 -0.34542521         nan\n",
      " -0.18913358 -0.06379926 -0.19682098         nan -0.09316344 -0.30185381\n",
      " -0.10240054         nan         nan -0.21623595         nan -0.08145734\n",
      " -0.28873081 -0.08145734 -0.30933074 -0.28873081 -0.58687798 -0.39058972\n",
      " -0.24567341         nan -0.04329481 -0.30185381 -0.17613312 -0.11313292\n",
      " -0.11160707 -0.10240054         nan -0.26791735         nan -0.08944716\n",
      "         nan -0.11280924         nan -0.37860503         nan -0.19682098\n",
      " -0.09135161 -0.202975           nan         nan -0.07809268 -0.30185381\n",
      "         nan -0.08261438 -0.40655972 -0.07152411         nan -0.23295899\n",
      "         nan -0.07715837 -0.20794763 -0.202975           nan -0.12297938\n",
      "         nan -0.58562613 -0.06034007 -0.64848592 -0.77037177         nan\n",
      " -0.12297938 -0.06266104 -0.13432071 -0.54981416         nan         nan\n",
      " -0.09316344 -0.16411767 -0.66225876         nan -0.19864866         nan\n",
      "         nan -0.41551738 -0.26334475         nan -0.3899156  -0.04377032\n",
      " -0.11321015 -0.04743689 -0.13938112 -0.13779512         nan -0.62731379\n",
      " -0.14053943 -0.06069091 -0.08857103         nan -0.13959729 -0.16039539\n",
      " -0.07001553 -0.39592173         nan -0.19864866 -0.17842804 -0.15790781\n",
      "         nan -0.25659827         nan -0.25117818 -0.05050595 -0.25049421\n",
      " -0.21395104         nan -0.11321015 -0.25825664         nan -0.19969625\n",
      "         nan         nan -0.05934971 -0.51819412 -0.32997844 -0.14823957\n",
      "         nan -0.06034007 -0.89957285 -0.66330625 -0.32670455         nan\n",
      " -0.06442817 -0.16411767 -0.05674068 -0.32452978]  Accuracy: 0.8826086956521739  F1 Score: 0.9105960264900662 Precision training: 0.8675078864353313 Recall training: <function recall at 0x000002DEF8F891E0> ]\n",
      "TEST: [Iteracion:  1500 Accuracy: 0.8601398601398601  F1 Score: 0.8969072164948454 Precision Test: 0.8365384615384616 Recall Test: 0.9666666666666667 ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#####################\n",
      "TRAINING: [Iteracion:  1800  Costo:  [        nan         nan -0.29814862         nan -0.37946479         nan\n",
      " -0.21658552         nan         nan -0.1166259          nan         nan\n",
      " -0.17658879 -0.31027392 -0.03556235 -0.3312509  -0.06520925 -0.13536333\n",
      " -0.12793554         nan         nan -0.11780374         nan -0.19727054\n",
      "         nan         nan -0.24074227         nan -0.15425449 -0.12483446\n",
      " -0.0352646  -0.1151508  -0.09883556 -0.29158868 -0.06468352         nan\n",
      " -0.93271236 -0.14174556 -0.09507076 -0.15634507 -0.06973293 -0.3645646\n",
      "         nan -0.07761504         nan         nan -0.14283302         nan\n",
      " -0.31596134 -0.54003582         nan         nan -0.04186026 -0.11899502\n",
      " -0.04186026         nan         nan -0.21686096 -0.16192922 -2.15498983\n",
      "         nan -0.03724259         nan         nan -0.2743109          nan\n",
      " -0.22787286         nan -0.57307949         nan -0.15087362 -0.32352087\n",
      " -0.05701836 -0.14734055 -0.0352646          nan -0.06006526         nan\n",
      "         nan         nan -0.07836953 -0.49518121 -0.55489549         nan\n",
      " -0.09507076 -0.5952227          nan -0.56090572 -0.07015733 -0.23961436\n",
      " -0.18643735 -0.1162791          nan -0.09257343         nan -0.0691267\n",
      " -0.10816501 -0.15308136 -1.52593038 -0.11780374 -0.78983758 -0.2437746\n",
      "         nan -0.12505238 -0.57505957 -0.17083262         nan -0.23969233\n",
      " -0.4980101          nan         nan         nan -0.04170331         nan\n",
      " -0.07015733         nan -0.57307949         nan -0.04186026 -0.56090572\n",
      " -0.04990887 -0.09738024 -0.08467892         nan         nan -0.11559108\n",
      " -0.10381167 -0.19529858 -0.1151508  -0.09882612 -0.17735072 -0.14103051\n",
      "         nan -0.05295941 -0.08534489 -0.31027392         nan         nan\n",
      " -0.341659   -0.03665197 -0.13745744 -0.16112094         nan -0.37966419\n",
      " -0.16167563         nan         nan -0.03530578 -0.07639389 -0.4373139\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan -0.04660295 -0.74077866 -0.09937556 -0.13536333         nan\n",
      " -0.09257343 -0.05677652         nan -0.11185211 -0.93271236         nan\n",
      "         nan         nan         nan -0.3554398          nan         nan\n",
      "         nan -0.3387834          nan -0.13582129 -0.0519609  -0.05353896\n",
      "         nan -0.13526778         nan         nan         nan -1.19804018\n",
      " -0.17263776         nan -0.08980856         nan         nan -0.43452092\n",
      "         nan         nan -0.86651993         nan         nan -0.12013286\n",
      " -0.1198181          nan -0.05652712 -0.05295941         nan         nan\n",
      " -0.05650685         nan -0.03111444 -0.191842           nan -0.07015733\n",
      " -0.18209278         nan -0.11298061 -0.16167563 -0.27088239 -0.24480613\n",
      "         nan         nan         nan         nan -0.10927604 -0.37946479\n",
      "         nan -0.27088239 -0.27315295         nan -0.21627132 -0.0519609\n",
      " -0.26977565 -0.13013097         nan         nan -0.191842   -0.16112094\n",
      "         nan -0.06794292 -0.29503983         nan -0.16112094 -0.3387834\n",
      " -0.09883556 -0.51653445 -0.4373139  -0.07094624         nan -0.18209278\n",
      " -0.17263776 -0.20153806 -0.24275854         nan -0.57505957         nan\n",
      " -0.11101263         nan         nan -0.29149025 -0.13745744         nan\n",
      " -0.21218906         nan         nan         nan         nan         nan\n",
      " -0.15425449         nan -0.2424354  -0.07094624         nan -0.21218906\n",
      " -0.07999845 -0.31271854 -0.07761504 -0.06794292         nan -0.11494298\n",
      "         nan -0.32133008 -0.13731777         nan         nan         nan\n",
      "         nan -0.35680641 -0.11494298         nan -0.15999246         nan\n",
      " -0.24074227 -0.03111444         nan         nan         nan -0.08755848\n",
      "         nan -0.10927604 -0.25358047         nan -0.06511143         nan\n",
      " -0.1766907          nan -0.12100677 -0.3312509          nan -0.02008969\n",
      "         nan -0.28721036 -0.37947464 -0.15087362 -0.06193799 -0.12632352\n",
      " -0.06006526 -2.15498983 -0.20144283         nan         nan         nan\n",
      "         nan -0.11780374         nan -0.14462017 -0.29149025         nan\n",
      " -0.11914075 -0.74077866 -0.66380535 -0.06973293         nan         nan\n",
      " -0.05295941         nan         nan -0.2437746  -0.33253625         nan\n",
      " -0.17263776 -0.05295941 -0.18022684         nan -0.07999845 -0.28721036\n",
      " -0.08850576         nan         nan -0.1997933          nan -0.0691267\n",
      " -0.27315295 -0.0691267  -0.29503983 -0.27315295 -0.58997657 -0.37966419\n",
      " -0.22936728         nan -0.03481738 -0.28721036 -0.15999246 -0.09883556\n",
      " -0.09738024 -0.08850576         nan -0.25205753         nan -0.07664765\n",
      "         nan -0.09856562         nan -0.36707672         nan -0.18022684\n",
      " -0.07836953 -0.18643735         nan         nan -0.06606362 -0.28721036\n",
      "         nan -0.07015733 -0.39675182 -0.06006526         nan -0.21658552\n",
      "         nan -0.06511143 -0.191842   -0.18643735         nan -0.10816501\n",
      "         nan -0.58898473 -0.04990887 -0.65668844 -0.78983758         nan\n",
      " -0.10816501 -0.0519609  -0.11899502 -0.55016454         nan         nan\n",
      " -0.07999845 -0.14792636 -0.67190894         nan -0.18209278         nan\n",
      "         nan -0.40571191 -0.24774581         nan -0.37947464 -0.03530578\n",
      " -0.09882612 -0.03845166 -0.12408046 -0.12245031         nan -0.63370094\n",
      " -0.12505238 -0.05025864 -0.07571855         nan -0.12419971 -0.14462017\n",
      " -0.05868039 -0.38548271         nan -0.18209278 -0.16192922 -0.14174556\n",
      "         nan -0.24074227         nan -0.23515714 -0.04114536 -0.23464511\n",
      " -0.19727054         nan -0.09882612 -0.24275854         nan -0.1831802\n",
      "         nan         nan -0.04913108 -0.51653445 -0.31596134 -0.13263189\n",
      "         nan -0.04990887 -0.93271236 -0.67304631 -0.31271854         nan\n",
      " -0.05353896 -0.14792636 -0.04660295 -0.31027392]  Accuracy: 0.8826086956521739  F1 Score: 0.9105960264900662 Precision training: 0.8675078864353313 Recall training: <function recall at 0x000002DEF8F891E0> ]\n",
      "TEST: [Iteracion:  1800 Accuracy: 0.8671328671328671  F1 Score: 0.9015544041450777 Precision Test: 0.8446601941747572 Recall Test: 0.9666666666666667 ]\n"
     ]
    }
   ],
   "source": [
    "## Llama a la funcion de gradient descent, recurda definir un nombre diferente para el modelo (~1 linea) ##\n",
    "modelo_4_theta_values, cost_vect, acc_vect, acc_vect_test = \\\n",
    "gradient_descent(X_train_features, Y_train_m4, theta_vector, alpha, iterations, X_test_features, y_test, 'modelo_4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2defaac4668>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Funcion para graficar el costo y  accuracy durante cada iteracion ##\n",
    "helper.training_graph(cost_vect, acc_vect, acc_vect_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy con el Test set [Modelo 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = prediccion(X_test_features, modelo_4_theta_values, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 86.7132867132867 %\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy:',accuracy_score(y_test, y_predict)* 100 ,'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Matriz de confusion Test Set [Modelo 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[37, 16],\n",
       "       [ 3, 87]], dtype=int64)"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision:  0.8446601941747572\n",
      "recall:  0.9666666666666667\n"
     ]
    }
   ],
   "source": [
    "print('precision: ',precision_score(y_test, y_predict))\n",
    "\n",
    "print('recall: ', recall_score(y_test, y_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento modelo 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sampleo bootstrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_m5, Y_train_m5 = resample(X_train, y_train, n_samples = 460)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtrar Features\n",
    "Solo seleccionar los features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Ingresa en la lista el nombre de las fatrues que quieres utlizar en este modelo (~1 linea)\n",
    "'''\n",
    "features_filtro = ['mean radius', 'mean texture', 'mean perimeter', 'mean area','mean smoothness']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_m5 = helper.fitrar_nombre(X_train_m5,data.feature_names,features_filtro)\n",
    "X_test_features = helper.fitrar_nombre(X_test,data.feature_names,features_filtro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Numero de iteracions ##\n",
    "iterations = 2000 # Define el numero de iteraciones #\n",
    "\n",
    "## Alpha ##\n",
    "alpha = 0.005 # Define el valor de alpha #\n",
    "\n",
    "#### No hay que cambiar el codigo despues de esta linea #####\n",
    "## Numero de feautres ##\n",
    "numero_features = X_train_m5.shape[1] + 1\n",
    "\n",
    "## Creamos un vector donde se almacenan los valores de Theta,\n",
    "## lo inicializamos con numeros aleatorios\n",
    "theta_vector = np.random.rand(numero_features)\n",
    "\n",
    "## Normaliza las features de X_train_features (~1 linea) ##\n",
    "X_train_m5 = normalizar(X_train_m5)\n",
    "\n",
    "# Creando la Matriz X de features\n",
    "X_train_features = np.ones((X_train_m5.shape[0], numero_features))\n",
    "X_train_features[:,:-1] = X_train_m5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#####################\n",
      "TRAINING: [Iteracion:  0  Costo:  [        nan -0.41159778 -0.41372289         nan         nan -0.5935538\n",
      " -0.39574077         nan -0.44658488 -0.54523855 -0.59445837 -0.53463761\n",
      " -0.4644826  -0.53155392 -0.43737671         nan         nan -0.43961923\n",
      " -0.51760216         nan -0.59867282 -0.54620238 -0.6569142  -0.49075916\n",
      "         nan -0.58165377 -0.46139929         nan         nan -0.43389489\n",
      " -0.59968954 -0.51517778 -0.55129063 -0.56396784 -0.63820282 -0.62380052\n",
      " -0.5584046  -0.60672498         nan -0.5444325  -0.57458653         nan\n",
      " -0.61309803         nan -0.38006767 -0.45710232 -0.49293855 -0.59068647\n",
      " -0.6422935  -0.64233049 -0.59422889         nan -0.46050622         nan\n",
      "         nan -0.59422889 -0.5935538  -0.62549735 -0.43448316 -0.520025\n",
      " -0.58129179 -0.53520013         nan         nan -0.54511907         nan\n",
      " -0.63820282         nan -0.51612212         nan         nan         nan\n",
      "         nan -0.5030781          nan -0.53155392 -0.57839323         nan\n",
      " -0.65608944         nan -0.50893715 -0.47688717 -0.4817381          nan\n",
      " -0.45214403 -0.50893715 -0.51584226         nan         nan -0.60361738\n",
      " -0.54344609 -0.44658488 -0.55965652 -0.50758055 -0.50686595 -0.45712215\n",
      " -0.37625622 -0.55362851 -0.37625622         nan -0.46482927 -0.55117089\n",
      " -0.48998999         nan         nan -0.59867282         nan -0.51308368\n",
      " -0.52033882 -0.50758055 -0.59867282 -0.54470912 -0.49116066         nan\n",
      "         nan         nan -0.42107092 -0.47802411 -0.51455398         nan\n",
      "         nan         nan -0.4060674  -0.46593288         nan -0.55459496\n",
      " -0.42316814 -0.4644826  -0.45891633 -0.60674912 -0.4060674  -0.58353712\n",
      " -0.55129063 -0.50758055         nan -0.52061755         nan -0.49075916\n",
      "         nan -0.58664911 -0.38006767 -0.47742001 -0.45900072 -0.55078816\n",
      "         nan         nan         nan         nan -0.51720893         nan\n",
      "         nan         nan         nan -0.47674156 -0.56396784 -0.57154668\n",
      "         nan         nan -0.49226449 -0.46893551         nan -0.47674156\n",
      " -0.4175153  -0.61811341 -0.55167783 -0.55677883         nan -0.47948957\n",
      " -0.6422935  -0.48070043 -0.48070043         nan         nan -0.5217395\n",
      " -0.49002289 -0.54415272 -0.54229429 -0.59534012         nan -0.53488434\n",
      " -0.49003928         nan -0.53520013         nan -0.41594204 -0.50893448\n",
      " -0.51517778         nan -0.59534012         nan -0.46139929 -0.60672498\n",
      "         nan         nan         nan -0.48642157 -0.42225554 -0.57458653\n",
      "         nan         nan         nan -0.49563079 -0.57839323         nan\n",
      " -0.45214403 -0.53039468 -0.51582143 -0.60797456         nan         nan\n",
      " -0.49003928 -0.57083459 -0.51455398 -0.58714761 -0.48070043 -0.50246691\n",
      " -0.48835804 -0.62354032 -0.55078816 -0.46216897         nan         nan\n",
      " -0.58664911 -0.43448316         nan -0.5217395  -0.5935538          nan\n",
      "         nan         nan -0.43961923 -0.50690291 -0.51380998         nan\n",
      "         nan         nan -0.64567777         nan -0.40199329         nan\n",
      "         nan -0.49227764 -0.40199329 -0.53487116 -0.51394355         nan\n",
      "         nan         nan         nan -0.51075239         nan -0.62354032\n",
      " -0.4858571  -0.53078767 -0.57756936         nan         nan -0.68697655\n",
      " -0.50469526         nan         nan -0.49581464         nan         nan\n",
      " -0.57839323         nan         nan -0.56396784         nan -0.57230184\n",
      " -0.5217395          nan -0.46130349         nan -0.46130349         nan\n",
      "         nan         nan -0.51517778         nan         nan -0.52472293\n",
      " -0.51612212         nan -0.45264457         nan -0.48336272 -0.6422935\n",
      "         nan -0.54415272 -0.47948957 -0.54409405         nan -0.35286717\n",
      "         nan         nan         nan -0.5314068  -0.45264457         nan\n",
      "         nan -0.52033882 -0.49227764 -0.52759451         nan         nan\n",
      " -0.6160891  -0.61252512 -0.641148   -0.56082711         nan -0.5508734\n",
      " -0.46593288 -0.47948957         nan         nan -0.46232096 -0.62047882\n",
      " -0.62047882         nan         nan -0.47361469 -0.50035824 -0.42225554\n",
      " -0.54620238 -0.45710232 -0.4230194  -0.55866011 -0.41688237 -0.40838967\n",
      "         nan         nan -0.62516341 -0.40838967 -0.52759451         nan\n",
      " -0.43448316 -0.57230184         nan -0.51887626         nan         nan\n",
      " -0.46022759 -0.52879133         nan -0.57083459         nan -0.42843926\n",
      "         nan -0.45214403 -0.42748527 -0.51887626         nan -0.44895622\n",
      "         nan -0.5444325          nan         nan -0.41594204 -0.58218311\n",
      " -0.50469526         nan -0.53115239         nan -0.48998999 -0.41952057\n",
      " -0.4060674  -0.45900072 -0.46593288         nan         nan -0.54470912\n",
      "         nan         nan -0.65143957         nan -0.47742001 -0.66662682\n",
      " -0.51075239         nan -0.49116066 -0.43737671 -0.47361469 -0.61327787\n",
      " -0.54511907 -0.45900072 -0.42107092         nan         nan -0.51380998\n",
      " -0.54511907 -0.51760216 -0.46130349         nan -0.50032821         nan\n",
      " -0.58664911 -0.4230194          nan -0.43961923 -0.41159778 -0.54620238\n",
      " -0.62354032 -0.41372289         nan         nan -0.38006767 -0.47742001\n",
      " -0.48047061         nan         nan -0.45214403 -0.49563079 -0.47948957\n",
      "         nan         nan         nan         nan         nan -0.45802419\n",
      "         nan -0.35286717         nan -0.50469526 -0.46050622 -0.54523855\n",
      "         nan         nan -0.54923142         nan         nan         nan\n",
      " -0.33378355 -0.55167783 -0.50469526 -0.45802419 -0.5508734  -0.40390831\n",
      "         nan -0.54511634         nan -0.29593258 -0.49581464         nan\n",
      " -0.41793912 -0.68956847 -0.4644826          nan         nan         nan\n",
      " -0.55965652         nan -0.49563079         nan         nan -0.60361738\n",
      " -0.45891633         nan         nan -0.50163608]  Accuracy: 0.6217391304347826  F1 Score: 0.7667560321715817 Precision training: 0.6217391304347826 Recall training: <function recall at 0x000002DEF8F891E0> ]\n",
      "TEST: [Iteracion:  0 Accuracy: 0.6223776223776224  F1 Score: 0.7672413793103449 Precision Test: 0.6267605633802817 Recall Test: 0.9888888888888889 ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin VDK\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:43: RuntimeWarning: invalid value encountered in log\n",
      "C:\\Users\\Admin VDK\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\Admin VDK\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#####################\n",
      "TRAINING: [Iteracion:  300  Costo:  [        nan -0.69794357 -0.69613            nan         nan -0.65046566\n",
      " -0.7037312          nan -0.68791969 -0.66269339 -0.64182753 -0.64797819\n",
      " -0.6838643  -0.65461703 -0.68278526         nan         nan -0.67275395\n",
      " -0.66910335         nan -0.64560302 -0.66166129 -0.64277081 -0.67011753\n",
      "         nan -0.64494875 -0.66945459         nan         nan -0.68262814\n",
      " -0.65130045 -0.66802317 -0.6486749  -0.65373771 -0.63769412 -0.64924551\n",
      " -0.65930499 -0.65161172         nan -0.65683006 -0.64712505         nan\n",
      " -0.63260705         nan -0.69641761 -0.67525595 -0.67797017 -0.64570529\n",
      " -0.64189745 -0.64099386 -0.64875141         nan -0.68375754         nan\n",
      "         nan -0.64875141 -0.65046566 -0.65011435 -0.68792096 -0.67135381\n",
      " -0.64479822 -0.66600503         nan         nan -0.65860186         nan\n",
      " -0.63769412         nan -0.66069869         nan         nan         nan\n",
      "         nan -0.67409247         nan -0.65461703 -0.6649774          nan\n",
      " -0.63763896         nan -0.67142175 -0.67466633 -0.68097403         nan\n",
      " -0.68364407 -0.67142175 -0.67023846         nan         nan -0.64286369\n",
      " -0.65789904 -0.68791969 -0.64955691 -0.66881746 -0.66560812 -0.66409329\n",
      " -0.70158008 -0.65977548 -0.70158008         nan -0.67622313 -0.66192182\n",
      " -0.66875217         nan         nan -0.64560302         nan -0.66533869\n",
      " -0.66089571 -0.66881746 -0.64560302 -0.65122482 -0.68361361         nan\n",
      "         nan         nan -0.6911042  -0.68564131 -0.67497198         nan\n",
      "         nan         nan -0.68914447 -0.67880728         nan -0.65501394\n",
      " -0.69766371 -0.6838643  -0.67895258 -0.64215416 -0.68914447 -0.6502973\n",
      " -0.6486749  -0.66881746         nan -0.65811307         nan -0.67011753\n",
      "         nan -0.64662748 -0.69641761 -0.67430623 -0.67882137 -0.66083805\n",
      "         nan         nan         nan         nan -0.67790896         nan\n",
      "         nan         nan         nan -0.68560049 -0.65373771 -0.66214925\n",
      "         nan         nan -0.6740811  -0.67007524         nan -0.68560049\n",
      " -0.69254979 -0.62684358 -0.66030569 -0.65703549         nan -0.6801158\n",
      " -0.64189745 -0.67826542 -0.67826542         nan         nan -0.67199585\n",
      " -0.6669935  -0.65515345 -0.66984638 -0.6497084          nan -0.66230031\n",
      " -0.6711827          nan -0.66600503         nan -0.68635502 -0.65922052\n",
      " -0.66802317         nan -0.6497084          nan -0.66945459 -0.65161172\n",
      "         nan         nan         nan -0.66843232 -0.68068014 -0.64712505\n",
      "         nan         nan         nan -0.66668671 -0.6649774          nan\n",
      " -0.68364407 -0.67034231 -0.66820446 -0.63988664         nan         nan\n",
      " -0.6711827  -0.65128067 -0.67497198 -0.65011324 -0.67826542 -0.67209841\n",
      " -0.66344986 -0.64376828 -0.66083805 -0.67809554         nan         nan\n",
      " -0.64662748 -0.68792096         nan -0.67199585 -0.65046566         nan\n",
      "         nan         nan -0.67275395 -0.67673078 -0.66397745         nan\n",
      "         nan         nan -0.63712942         nan -0.70268671         nan\n",
      "         nan -0.67073263 -0.70268671 -0.65344703 -0.66618558         nan\n",
      "         nan         nan         nan -0.65997433         nan -0.64376828\n",
      " -0.67735951 -0.66074803 -0.66001351         nan         nan -0.63683217\n",
      " -0.67193436         nan         nan -0.67563123         nan         nan\n",
      " -0.6649774          nan         nan -0.65373771         nan -0.65388529\n",
      " -0.67199585         nan -0.66931054         nan -0.66931054         nan\n",
      "         nan         nan -0.66802317         nan         nan -0.65851812\n",
      " -0.66069869         nan -0.68206819         nan -0.66312981 -0.64189745\n",
      "         nan -0.65515345 -0.6801158  -0.65540769         nan -0.70833333\n",
      "         nan         nan         nan -0.66858431 -0.68206819         nan\n",
      "         nan -0.66089571 -0.67073263 -0.66372456         nan         nan\n",
      " -0.64769768 -0.64696904 -0.65205477 -0.65693989         nan -0.66590317\n",
      " -0.67880728 -0.6801158          nan         nan -0.67273914 -0.63799439\n",
      " -0.63799439         nan         nan -0.67095943 -0.66880225 -0.68068014\n",
      " -0.66166129 -0.67525595 -0.68750883 -0.65864851 -0.67908162 -0.69560823\n",
      "         nan         nan -0.65619197 -0.69560823 -0.66372456         nan\n",
      " -0.68792096 -0.65388529         nan -0.66754556         nan         nan\n",
      " -0.68449152 -0.67039582         nan -0.65128067         nan -0.69165171\n",
      "         nan -0.68364407 -0.68564302 -0.66754556         nan -0.68650216\n",
      "         nan -0.65683006         nan         nan -0.68635502 -0.65647583\n",
      " -0.67193436         nan -0.65847446         nan -0.66875217 -0.68779282\n",
      " -0.68914447 -0.67882137 -0.67880728         nan         nan -0.65122482\n",
      "         nan         nan -0.63338712         nan -0.67430623 -0.63571276\n",
      " -0.65997433         nan -0.68361361 -0.68278526 -0.67095943 -0.64680539\n",
      " -0.65860186 -0.67882137 -0.6911042          nan         nan -0.66397745\n",
      " -0.65860186 -0.66910335 -0.66931054         nan -0.67580486         nan\n",
      " -0.64662748 -0.68750883         nan -0.67275395 -0.69794357 -0.66166129\n",
      " -0.64376828 -0.69613            nan         nan -0.69641761 -0.67430623\n",
      " -0.66536527         nan         nan -0.68364407 -0.66668671 -0.6801158\n",
      "         nan         nan         nan         nan         nan -0.68505706\n",
      "         nan -0.70833333         nan -0.67193436 -0.68375754 -0.66269339\n",
      "         nan         nan -0.66012042         nan         nan         nan\n",
      " -0.72270497 -0.66030569 -0.67193436 -0.68505706 -0.66590317 -0.6888404\n",
      "         nan -0.66858143         nan -0.73911694 -0.67563123         nan\n",
      " -0.68391219 -0.63328423 -0.6838643          nan         nan         nan\n",
      " -0.64955691         nan -0.66668671         nan         nan -0.64286369\n",
      " -0.67895258         nan         nan -0.66224041]  Accuracy: 0.8456521739130435  F1 Score: 0.8826446280991735 Precision training: 0.8369905956112853 Recall training: <function recall at 0x000002DEF8F891E0> ]\n",
      "TEST: [Iteracion:  300 Accuracy: 0.3776223776223776  F1 Score: 0.02197802197802198 Precision Test: 1.0 Recall Test: 0.011111111111111112 ]\n",
      "#####################\n",
      "TRAINING: [Iteracion:  600  Costo:  [        nan -0.71415317 -0.71020691         nan         nan -0.56430421\n",
      " -0.73174121         nan -0.68046605 -0.59960334 -0.55307795 -0.58531338\n",
      " -0.665453   -0.59503057 -0.67892462         nan         nan -0.66461547\n",
      " -0.62013673         nan -0.55641763 -0.59796719 -0.53165838 -0.63412888\n",
      "         nan -0.56223156 -0.64781797         nan         nan -0.68138204\n",
      " -0.56282471 -0.62015783 -0.578705   -0.57998604 -0.53260461 -0.55157114\n",
      " -0.58983363 -0.56115847         nan -0.59209505 -0.56834321         nan\n",
      " -0.53487582         nan -0.73138007 -0.65819923 -0.64317603 -0.56052761\n",
      " -0.53591495 -0.53471048 -0.56182596         nan -0.66727038         nan\n",
      "         nan -0.56182596 -0.56430421 -0.55185381 -0.68786127 -0.62175418\n",
      " -0.56259787 -0.6081263          nan         nan -0.59437456         nan\n",
      " -0.53260461         nan -0.609353           nan         nan         nan\n",
      "         nan -0.63277676         nan -0.59503057 -0.58820392         nan\n",
      " -0.52567394         nan -0.62693366 -0.64657837 -0.65310027         nan\n",
      " -0.67150741 -0.62693366 -0.6220239          nan         nan -0.55097036\n",
      " -0.59420137 -0.68046605 -0.57665479 -0.62426411 -0.62045664 -0.6419188\n",
      " -0.74141498 -0.59234328 -0.74141498         nan -0.65547777 -0.59538405\n",
      " -0.63357931         nan         nan -0.55641763         nan -0.61764213\n",
      " -0.60781678 -0.62426411 -0.55641763 -0.58503742 -0.65209804         nan\n",
      "         nan         nan -0.69944399 -0.66062329 -0.62902386         nan\n",
      "         nan         nan -0.7052965  -0.65894419         nan -0.58721829\n",
      " -0.7068547  -0.665453   -0.66160259 -0.5492902  -0.7052965  -0.56855934\n",
      " -0.578705   -0.62426411         nan -0.60396331         nan -0.63412888\n",
      "         nan -0.56213003 -0.73138007 -0.6464404  -0.66127402 -0.59456783\n",
      "         nan         nan         nan         nan -0.63127621         nan\n",
      "         nan         nan         nan -0.66148046 -0.57998604 -0.58766247\n",
      "         nan         nan -0.63838189 -0.64436692         nan -0.66148046\n",
      " -0.70304916 -0.52592555 -0.59370643 -0.58701973         nan -0.65247749\n",
      " -0.53591495 -0.64950274 -0.64950274         nan         nan -0.62164056\n",
      " -0.63022037 -0.59097796 -0.6096328  -0.56233983         nan -0.60382575\n",
      " -0.63551822         nan -0.6081263          nan -0.69630243 -0.61154435\n",
      " -0.62015783         nan -0.56233983         nan -0.64781797 -0.56115847\n",
      "         nan         nan         nan -0.63405706 -0.68374337 -0.56834321\n",
      "         nan         nan         nan -0.62719625 -0.58820392         nan\n",
      " -0.67150741 -0.61589552 -0.6196764  -0.54598002         nan         nan\n",
      " -0.63551822 -0.57428216 -0.62902386 -0.56629435 -0.64950274 -0.63090899\n",
      " -0.62581776 -0.54448956 -0.59456783 -0.65847415         nan         nan\n",
      " -0.56213003 -0.68786127         nan -0.62164056 -0.56430421         nan\n",
      "         nan         nan -0.66461547 -0.63497482 -0.61454331         nan\n",
      "         nan         nan -0.52890403         nan -0.7264468          nan\n",
      "         nan -0.63436546 -0.7264468  -0.59319077 -0.61900642         nan\n",
      "         nan         nan         nan -0.61131378         nan -0.54448956\n",
      " -0.64605284 -0.60343524 -0.58307327         nan         nan -0.51541503\n",
      " -0.62974148         nan         nan -0.63886895         nan         nan\n",
      " -0.58820392         nan         nan -0.57998604         nan -0.57747247\n",
      " -0.62164056         nan -0.64874944         nan -0.64874944         nan\n",
      "         nan         nan -0.62015783         nan         nan -0.60340815\n",
      " -0.609353           nan -0.66989053         nan -0.62844161 -0.53591495\n",
      "         nan -0.59097796 -0.65247749 -0.59039014         nan -0.76731843\n",
      "         nan         nan         nan -0.61248156 -0.66989053         nan\n",
      "         nan -0.60781678 -0.63436546 -0.60892526         nan         nan\n",
      " -0.55256214 -0.55279987 -0.54846766 -0.58517599         nan -0.60062018\n",
      " -0.65894419 -0.65247749         nan         nan -0.65130178 -0.53928803\n",
      " -0.53928803         nan         nan -0.64449589 -0.62729079 -0.68374337\n",
      " -0.59796719 -0.65819923 -0.69352403 -0.58976559 -0.68482679 -0.71329923\n",
      "         nan         nan -0.55966975 -0.71329923 -0.60892526         nan\n",
      " -0.68786127 -0.57747247         nan -0.61745914         nan         nan\n",
      " -0.66833707 -0.61732002         nan -0.57428216         nan -0.6967302\n",
      "         nan -0.67150741 -0.68751542 -0.61745914         nan -0.67740485\n",
      "         nan -0.59209505         nan         nan -0.69630243 -0.57615989\n",
      " -0.62974148         nan -0.6001097          nan -0.63357931 -0.69512105\n",
      " -0.7052965  -0.66127402 -0.65894419         nan         nan -0.58503742\n",
      "         nan         nan -0.52258625         nan -0.6464404  -0.52050896\n",
      " -0.61131378         nan -0.65209804 -0.67892462 -0.64449589 -0.55338381\n",
      " -0.59437456 -0.66127402 -0.69944399         nan         nan -0.61454331\n",
      " -0.59437456 -0.62013673 -0.64874944         nan -0.63686703         nan\n",
      " -0.56213003 -0.69352403         nan -0.66461547 -0.71415317 -0.59796719\n",
      " -0.54448956 -0.71020691         nan         nan -0.73138007 -0.6464404\n",
      " -0.63262859         nan         nan -0.67150741 -0.62719625 -0.65247749\n",
      "         nan         nan         nan         nan         nan -0.67052125\n",
      "         nan -0.76731843         nan -0.62974148 -0.66727038 -0.59960334\n",
      "         nan         nan -0.59461239         nan         nan         nan\n",
      " -0.80299317 -0.59370643 -0.62974148 -0.67052125 -0.60062018 -0.70631575\n",
      "         nan -0.60707543         nan -0.85805329 -0.63886895         nan\n",
      " -0.69097642 -0.5104525  -0.665453           nan         nan         nan\n",
      " -0.57665479         nan -0.62719625         nan         nan -0.55097036\n",
      " -0.66160259         nan         nan -0.61880701]  Accuracy: 0.8586956521739131  F1 Score: 0.8865619546247818 Precision training: 0.8850174216027874 Recall training: <function recall at 0x000002DEF8F891E0> ]\n",
      "TEST: [Iteracion:  600 Accuracy: 0.7762237762237763  F1 Score: 0.7922077922077922 Precision Test: 0.953125 Recall Test: 0.6777777777777778 ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#####################\n",
      "TRAINING: [Iteracion:  900  Costo:  [        nan -0.69139373 -0.68602748         nan         nan -0.48130621\n",
      " -0.71680102         nan -0.64236451 -0.52863925 -0.46917934 -0.51532802\n",
      " -0.62046225 -0.5264322  -0.64349984         nan         nan -0.62697658\n",
      " -0.55705613         nan -0.47214226 -0.52673724 -0.43654906 -0.57881824\n",
      "         nan -0.48140658 -0.60143811         nan         nan -0.64788843\n",
      " -0.47852652 -0.55786371 -0.50447623 -0.50343307 -0.44086969 -0.46274151\n",
      " -0.5154826  -0.47596853         nan -0.52040369 -0.48965452         nan\n",
      " -0.44696549         nan -0.7215894  -0.6145731  -0.58817049 -0.47857499\n",
      " -0.44339509 -0.44207761 -0.47850178         nan -0.62348063         nan\n",
      "         nan -0.47850178 -0.48130621 -0.46263617 -0.65481113 -0.55813956\n",
      " -0.48215943 -0.53995866         nan         nan -0.52295961         nan\n",
      " -0.44086969         nan -0.54513178         nan         nan         nan\n",
      "         nan -0.57386659         nan -0.5264322  -0.50937736         nan\n",
      " -0.43047134         nan -0.56634899 -0.59583062 -0.6023546          nan\n",
      " -0.63050663 -0.56634899 -0.55923088         nan         nan -0.46540631\n",
      " -0.52312418 -0.64236451 -0.50083625 -0.56378912 -0.55978876 -0.59518823\n",
      " -0.73491204 -0.51908464 -0.73491204         nan -0.60946125 -0.52232995\n",
      " -0.57906355         nan         nan -0.47214226         nan -0.55557988\n",
      " -0.54260105 -0.56378912 -0.47214226 -0.51286883 -0.59901544         nan\n",
      "         nan         nan -0.67173993 -0.61130682 -0.56741731         nan\n",
      "         nan         nan -0.68287946 -0.61352885         nan -0.51434244\n",
      " -0.67933201 -0.62046225 -0.6174607  -0.46336049 -0.68287946 -0.48805741\n",
      " -0.50447623 -0.56378912         nan -0.53822542         nan -0.57881824\n",
      "         nan -0.48020694 -0.7215894  -0.59600998 -0.61698268 -0.52182307\n",
      "         nan         nan         nan         nan -0.56903118         nan\n",
      "         nan         nan         nan -0.6128007  -0.50343307 -0.51029955\n",
      "         nan         nan -0.58300205 -0.59528274         nan -0.6128007\n",
      " -0.67660917 -0.43676218 -0.52085351 -0.51232758         nan -0.60175425\n",
      " -0.44339509 -0.59818608 -0.59818608         nan         nan -0.55750573\n",
      " -0.57453507 -0.51991998 -0.53984696 -0.47866725         nan -0.53558211\n",
      " -0.5802604          nan -0.53995866         nan -0.66999768 -0.54977407\n",
      " -0.55786371         nan -0.47866725         nan -0.60143811 -0.47596853\n",
      "         nan         nan         nan -0.57982383 -0.65276364 -0.48965452\n",
      "         nan         nan         nan -0.56989184 -0.50937736         nan\n",
      " -0.63050663 -0.54951882 -0.55687027 -0.45963571         nan         nan\n",
      " -0.5802604  -0.4961354  -0.56741731 -0.48453897 -0.59818608 -0.57228863\n",
      " -0.56963024 -0.4550675  -0.52182307 -0.61284796         nan         nan\n",
      " -0.48020694 -0.65481113         nan -0.55750573 -0.48130621         nan\n",
      "         nan         nan -0.62697658 -0.57582109 -0.55125758         nan\n",
      "         nan         nan -0.43558555         nan -0.70862401         nan\n",
      "         nan -0.57883768 -0.70862401 -0.52455192 -0.55738339         nan\n",
      "         nan         nan         nan -0.54882661         nan -0.4550675\n",
      " -0.59328146 -0.53588813 -0.50462085         nan         nan -0.41610576\n",
      " -0.57055002         nan         nan -0.58281072         nan         nan\n",
      " -0.50937736         nan         nan -0.50343307         nan -0.49963186\n",
      " -0.55750573         nan -0.60334611         nan -0.60334611         nan\n",
      "         nan         nan -0.55786371         nan         nan -0.53728793\n",
      " -0.54513178         nan -0.62914383         nan -0.5742312  -0.44339509\n",
      "         nan -0.51991998 -0.60175425 -0.51860333         nan -0.77435877\n",
      "         nan         nan         nan -0.54504712 -0.62914383         nan\n",
      "         nan -0.54260105 -0.57883768 -0.54275087         nan         nan\n",
      " -0.46512585 -0.46585313 -0.45644425 -0.50950968         nan -0.52807783\n",
      " -0.61352885 -0.60175425         nan         nan -0.60478447 -0.45068641\n",
      " -0.45068641         nan         nan -0.59525128 -0.56855189 -0.65276364\n",
      " -0.52673724 -0.6145731  -0.66445614 -0.51599977 -0.65575711 -0.69177835\n",
      "         nan         nan -0.47102966 -0.69177835 -0.54275087         nan\n",
      " -0.65481113 -0.49963186         nan -0.55377267         nan         nan\n",
      " -0.62470998 -0.5519086          nan -0.4961354          nan -0.66703251\n",
      "         nan -0.63050663 -0.6555179  -0.55377267         nan -0.63833504\n",
      "         nan -0.52040369         nan         nan -0.66999768 -0.49591306\n",
      " -0.57055002         nan -0.53200668         nan -0.57906355 -0.66669448\n",
      " -0.68287946 -0.61698268 -0.61352885         nan         nan -0.51286883\n",
      "         nan         nan -0.42832026         nan -0.59600998 -0.42407816\n",
      " -0.54882661         nan -0.59901544 -0.64349984 -0.59525128 -0.46709341\n",
      " -0.52295961 -0.61698268 -0.67173993         nan         nan -0.55125758\n",
      " -0.52295961 -0.55705613 -0.60334611         nan -0.57945858         nan\n",
      " -0.48020694 -0.66445614         nan -0.62697658 -0.69139373 -0.52673724\n",
      " -0.4550675  -0.68602748         nan         nan -0.7215894  -0.59600998\n",
      " -0.5794662          nan         nan -0.63050663 -0.56989184 -0.60175425\n",
      "         nan         nan         nan         nan         nan -0.6279782\n",
      "         nan -0.77435877         nan -0.57055002 -0.62348063 -0.52863925\n",
      "         nan         nan -0.52241026         nan         nan         nan\n",
      " -0.82535151 -0.52085351 -0.57055002 -0.6279782  -0.52807783 -0.68485415\n",
      "         nan -0.53668779         nan -0.90907657 -0.58281072         nan\n",
      " -0.66261598 -0.4107668  -0.62046225         nan         nan         nan\n",
      " -0.50083625         nan -0.56989184         nan         nan -0.46540631\n",
      " -0.6174607          nan         nan -0.55940642]  Accuracy: 0.8869565217391304  F1 Score: 0.9133333333333333 Precision training: 0.8726114649681529 Recall training: <function recall at 0x000002DEF8F891E0> ]\n",
      "TEST: [Iteracion:  900 Accuracy: 0.8531468531468531  F1 Score: 0.88268156424581 Precision Test: 0.8876404494382022 Recall Test: 0.8777777777777778 ]\n",
      "#####################\n",
      "TRAINING: [Iteracion:  1200  Costo:  [        nan -0.66918781 -0.66266336         nan         nan -0.41525974\n",
      " -0.70103658         nan -0.60787938 -0.47017884 -0.40279832 -0.45778968\n",
      " -0.58064698 -0.46972095 -0.61117311         nan         nan -0.59298602\n",
      " -0.50398451         nan -0.4054394  -0.46811706 -0.36365975 -0.53136372\n",
      "         nan -0.41685635 -0.56068518         nan         nan -0.61711604\n",
      " -0.41165161 -0.50537588 -0.44407591 -0.44140807 -0.37004267 -0.39320851\n",
      " -0.45481022 -0.40855911         nan -0.46154478 -0.42642294         nan\n",
      " -0.37846647         nan -0.71017729 -0.57589159 -0.54084763 -0.41332827\n",
      " -0.37197425 -0.37062112 -0.41229446         nan -0.58458882         nan\n",
      "         nan -0.41229446 -0.41525974 -0.39284402 -0.62434863 -0.50465162\n",
      " -0.41786785 -0.48335926         nan         nan -0.46426422         nan\n",
      " -0.37004267         nan -0.49135739         nan         nan         nan\n",
      "         nan -0.52371286         nan -0.46972095 -0.44563325         nan\n",
      " -0.35771749         nan -0.51502763 -0.55171394 -0.55819254         nan\n",
      " -0.59377803 -0.51502763 -0.50633776         nan         nan -0.39799158\n",
      " -0.46467014 -0.60787938 -0.43938288 -0.51258308 -0.50848716 -0.55419218\n",
      " -0.72626734 -0.45912948 -0.72626734         nan -0.56893589 -0.46245133\n",
      " -0.53223384         nan         nan -0.4054394          nan -0.50329448\n",
      " -0.48812804 -0.51258308 -0.4054394  -0.45379801 -0.55307834         nan\n",
      "         nan         nan -0.64554695 -0.56816577 -0.51531634         nan\n",
      "         nan         nan -0.66099958 -0.57345669         nan -0.45477041\n",
      " -0.65323916 -0.58064698 -0.57833544 -0.39575772 -0.66099958 -0.4235946\n",
      " -0.44407591 -0.51258308         nan -0.48342583         nan -0.53136372\n",
      "         nan -0.4149221  -0.71017729 -0.55215138 -0.57774059 -0.46218364\n",
      "         nan         nan         nan         nan -0.51642373         nan\n",
      "         nan         nan         nan -0.57015016 -0.44140807 -0.44757511\n",
      "         nan         nan -0.53544583 -0.55246091         nan -0.57015016\n",
      " -0.65142194 -0.36772486 -0.46115668 -0.45145567         nan -0.55760156\n",
      " -0.37197425 -0.55360767 -0.55360767         nan         nan -0.50363508\n",
      " -0.5268363  -0.46154427 -0.48205943 -0.41221296         nan -0.47899884\n",
      " -0.53282326         nan -0.48335926         nan -0.64497672 -0.49777916\n",
      " -0.50537588         nan -0.41221296         nan -0.56068518 -0.40855911\n",
      "         nan         nan         nan -0.53318464 -0.62397257 -0.42642294\n",
      "         nan         nan         nan -0.5210129  -0.44563325         nan\n",
      " -0.59377803 -0.49410041 -0.50400358 -0.39184822         nan         nan\n",
      " -0.53282326 -0.43313562 -0.51531634 -0.41927357 -0.55360767 -0.52238216\n",
      " -0.52158969 -0.38533917 -0.46218364 -0.57258216         nan         nan\n",
      " -0.4149221  -0.62434863         nan -0.50363508 -0.41525974         nan\n",
      "         nan         nan -0.59298602 -0.52547264 -0.49809395         nan\n",
      "         nan         nan -0.36388463         nan -0.69046577         nan\n",
      "         nan -0.53122052 -0.69046577 -0.46786308 -0.50541795         nan\n",
      "         nan         nan         nan -0.49630128         nan -0.38533917\n",
      " -0.54763128 -0.47980606 -0.44124885         nan         nan -0.34127093\n",
      " -0.52023343         nan         nan -0.53474136         nan         nan\n",
      " -0.44563325         nan         nan -0.44140807         nan -0.43678753\n",
      " -0.50363508         nan -0.5633681          nan -0.5633681          nan\n",
      "         nan         nan -0.50537588         nan         nan -0.48223612\n",
      " -0.49135739         nan -0.59264535         nan -0.52766442 -0.37197425\n",
      "         nan -0.46154427 -0.55760156 -0.45970497         nan -0.7773318\n",
      "         nan         nan         nan -0.48889811 -0.59264535         nan\n",
      "         nan -0.48812804 -0.53122052 -0.48758343         nan         nan\n",
      " -0.39646388 -0.39749774 -0.38496232 -0.44799546         nan -0.46847199\n",
      " -0.57345669 -0.55760156         nan         nan -0.56388624 -0.38163654\n",
      " -0.38163654         nan         nan -0.55233413 -0.51859186 -0.62397257\n",
      " -0.46811706 -0.57589159 -0.63718488 -0.45575436 -0.62851594 -0.67061377\n",
      "         nan         nan -0.40140755 -0.67061377 -0.48758343         nan\n",
      " -0.62434863 -0.43678753         nan -0.50029064         nan         nan\n",
      " -0.58593618 -0.49718473         nan -0.43313562         nan -0.63924552\n",
      "         nan -0.59377803 -0.62588003 -0.50029064         nan -0.60310313\n",
      "         nan -0.46154478         nan         nan -0.64497672 -0.4314487\n",
      " -0.52023343         nan -0.47557068         nan -0.53223384 -0.63991819\n",
      " -0.66099958 -0.57774059 -0.57345669         nan         nan -0.45379801\n",
      "         nan         nan -0.35624441         nan -0.55215138 -0.35076833\n",
      " -0.49630128         nan -0.55307834 -0.61117311 -0.55233413 -0.39916892\n",
      " -0.46426422 -0.57774059 -0.64554695         nan         nan -0.49809395\n",
      " -0.46426422 -0.50398451 -0.5633681          nan -0.53039414         nan\n",
      " -0.4149221  -0.63718488         nan -0.59298602 -0.66918781 -0.46811706\n",
      " -0.38533917 -0.66266336         nan         nan -0.71017729 -0.55215138\n",
      " -0.53364596         nan         nan -0.59377803 -0.5210129  -0.55760156\n",
      "         nan         nan         nan         nan         nan -0.59004592\n",
      "         nan -0.7773318          nan -0.52023343 -0.58458882 -0.47017884\n",
      "         nan         nan -0.46315756         nan         nan         nan\n",
      " -0.84174349 -0.46115668 -0.52023343 -0.59004592 -0.46847199 -0.66376435\n",
      "         nan -0.47851643         nan -0.95121047 -0.53474136         nan\n",
      " -0.63592254 -0.33589707 -0.58064698         nan         nan         nan\n",
      " -0.43938288         nan -0.5210129          nan         nan -0.39799158\n",
      " -0.57833544         nan         nan -0.50906878]  Accuracy: 0.8782608695652174  F1 Score: 0.9078947368421052 Precision training: 0.8571428571428571 Recall training: <function recall at 0x000002DEF8F891E0> ]\n",
      "TEST: [Iteracion:  1200 Accuracy: 0.8881118881118881  F1 Score: 0.9157894736842105 Precision Test: 0.87 Recall Test: 0.9666666666666667 ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#####################\n",
      "TRAINING: [Iteracion:  1500  Costo:  [        nan -0.65134568 -0.643815           nan         nan -0.36322433\n",
      " -0.68885818         nan -0.57954341 -0.42313577 -0.3507106  -0.41155014\n",
      " -0.54790606 -0.42401224 -0.58464345         nan         nan -0.56505567\n",
      " -0.46085145         nan -0.35307886 -0.42096939 -0.30778841 -0.4925397\n",
      "         nan -0.36587629 -0.52720327         nan         nan -0.59188853\n",
      " -0.35907633 -0.46268733 -0.39582378 -0.3919778  -0.31543064 -0.33904979\n",
      " -0.4062406  -0.35564295         nan -0.41427899 -0.37627127         nan\n",
      " -0.32529983         nan -0.70185224 -0.54408463 -0.50209958 -0.36188567\n",
      " -0.31691868 -0.31557045 -0.36018741         nan -0.55260592         nan\n",
      "         nan -0.36018741 -0.36322433 -0.33851195 -0.59939108 -0.46119257\n",
      " -0.36705722 -0.43763231         nan         nan -0.41709453         nan\n",
      " -0.31543064         nan -0.4477584          nan         nan         nan\n",
      "         nan -0.48278172         nan -0.42401224 -0.39488666         nan\n",
      " -0.30207487         nan -0.47321046 -0.51551778 -0.52193727         nan\n",
      " -0.56357927 -0.47321046 -0.46333549         nan         nan -0.34524525\n",
      " -0.41767925 -0.57954341 -0.3903959  -0.47086727 -0.46671594 -0.52053422\n",
      " -0.72042537 -0.41105662 -0.72042537         nan -0.53562259 -0.41441295\n",
      " -0.49389567         nan         nan -0.35307886         nan -0.46077515\n",
      " -0.44400778 -0.47086727 -0.35307886 -0.4064411  -0.51539933         nan\n",
      "         nan         nan -0.62424322 -0.5327252  -0.47288445         nan\n",
      "         nan         nan -0.64342861 -0.54050149         nan -0.40701196\n",
      " -0.63203348 -0.54790606 -0.54616757 -0.34291159 -0.64342861 -0.37258409\n",
      " -0.39582378 -0.47086727         nan -0.43908615         nan -0.4925397\n",
      "         nan -0.3634407  -0.70185224 -0.51615579 -0.54547853 -0.41432337\n",
      "         nan         nan         nan         nan -0.47359287         nan\n",
      "         nan         nan         nan -0.53510237 -0.3919778  -0.39755766\n",
      "         nan         nan -0.49652926 -0.51731405         nan -0.53510237\n",
      " -0.63099213 -0.31439412 -0.41326055 -0.40277699         nan -0.52136264\n",
      " -0.31691868 -0.51703262 -0.51703262         nan         nan -0.4598858\n",
      " -0.48783548 -0.4146334  -0.43543709 -0.35993242         nan -0.43331275\n",
      " -0.49401231         nan -0.43763231         nan -0.62467157 -0.45551708\n",
      " -0.46268733         nan -0.35993242         nan -0.52720327 -0.35564295\n",
      "         nan         nan         nan -0.49500561 -0.60044657 -0.37627127\n",
      "         nan         nan         nan -0.48109571 -0.39488666         nan\n",
      " -0.56357927 -0.44920702 -0.46103149 -0.3389382          nan         nan\n",
      " -0.49401231 -0.38308015 -0.47288445 -0.3677456  -0.51703262 -0.48164703\n",
      " -0.48234289 -0.33119184 -0.41432337 -0.53948733         nan         nan\n",
      " -0.3634407  -0.59939108         nan -0.4598858  -0.36322433         nan\n",
      "         nan         nan -0.56505567 -0.48437347 -0.45493026         nan\n",
      "         nan         nan -0.30881185         nan -0.67617384         nan\n",
      "         nan -0.49226532 -0.67617384 -0.422174   -0.46312993         nan\n",
      "         nan         nan         nan -0.4536382          nan -0.33119184\n",
      " -0.51020326 -0.43449992 -0.39081892         nan         nan -0.28464705\n",
      " -0.47918232         nan         nan -0.49541567         nan         nan\n",
      " -0.39488666         nan         nan -0.3919778          nan -0.38679807\n",
      " -0.4598858          nan -0.53050313         nan -0.53050313         nan\n",
      "         nan         nan -0.46268733         nan         nan -0.43770202\n",
      " -0.4477584          nan -0.56262875         nan -0.48956295 -0.31691868\n",
      "         nan -0.4146334  -0.52136264 -0.41242628         nan -0.78203277\n",
      "         nan         nan         nan -0.44348466 -0.56262875         nan\n",
      "         nan -0.44400778 -0.49226532 -0.44292324         nan         nan\n",
      " -0.34285962 -0.34409385 -0.32960379 -0.39887455         nan -0.42058447\n",
      " -0.54050149 -0.52136264         nan         nan -0.53028557 -0.32802302\n",
      " -0.32802302         nan         nan -0.51709425 -0.47783426 -0.60044657\n",
      " -0.42096939 -0.54408463 -0.6149506  -0.40748572 -0.60630941 -0.65366906\n",
      "         nan         nan -0.3470423  -0.65366906 -0.44292324         nan\n",
      " -0.59939108 -0.38679807         nan -0.4568598          nan         nan\n",
      " -0.55405015 -0.45280151         nan -0.38308015         nan -0.61656445\n",
      "         nan -0.56357927 -0.60163697 -0.4568598          nan -0.57414235\n",
      "         nan -0.41427899         nan         nan -0.62467157 -0.38034281\n",
      " -0.47918232         nan -0.43002761         nan -0.49389567 -0.61811949\n",
      " -0.64342861 -0.54547853 -0.54050149         nan         nan -0.4064411\n",
      "         nan         nan -0.30108832         nan -0.51615579 -0.294913\n",
      " -0.4536382          nan -0.51539933 -0.58464345 -0.51709425 -0.34603113\n",
      " -0.41709453 -0.54547853 -0.62424322         nan         nan -0.45493026\n",
      " -0.41709453 -0.46085145 -0.53050313         nan -0.49029304         nan\n",
      " -0.3634407  -0.6149506          nan -0.56505567 -0.65134568 -0.42096939\n",
      " -0.33119184 -0.643815           nan         nan -0.70185224 -0.51615579\n",
      " -0.49612497         nan         nan -0.56357927 -0.48109571 -0.52136264\n",
      "         nan         nan         nan         nan         nan -0.55884859\n",
      "         nan -0.78203277         nan -0.47918232 -0.55260592 -0.42313577\n",
      "         nan         nan -0.41557655         nan         nan         nan\n",
      " -0.85884999 -0.41326055 -0.47918232 -0.55884859 -0.42058447 -0.64687916\n",
      "         nan -0.43162603         nan -0.99257962 -0.49541567         nan\n",
      " -0.61418776 -0.27939579 -0.54790606         nan         nan         nan\n",
      " -0.3903959          nan -0.48109571         nan         nan -0.34524525\n",
      " -0.54616757         nan         nan -0.46804861]  Accuracy: 0.8760869565217392  F1 Score: 0.9067103109656302 Precision training: 0.8523076923076923 Recall training: <function recall at 0x000002DEF8F891E0> ]\n",
      "TEST: [Iteracion:  1500 Accuracy: 0.8741258741258742  F1 Score: 0.90625 Precision Test: 0.8529411764705882 Recall Test: 0.9666666666666667 ]\n",
      "#####################\n",
      "TRAINING: [Iteracion:  1800  Costo:  [        nan -0.63728715 -0.6288599          nan         nan -0.32165889\n",
      " -0.67990254         nan -0.55627569 -0.38484678 -0.30925487 -0.37395671\n",
      " -0.52089505 -0.38675886 -0.56292149         nan         nan -0.54211131\n",
      " -0.4254594          nan -0.31139185 -0.38261149 -0.26427884 -0.46053819\n",
      "         nan -0.32506184 -0.49957612         nan         nan -0.57128362\n",
      " -0.31716259 -0.42764002 -0.35679721 -0.35208433 -0.27266241 -0.2962413\n",
      " -0.36688581 -0.31351409         nan -0.37587755 -0.33596418         nan\n",
      " -0.28340564         nan -0.69638059 -0.51785357 -0.47014531 -0.32076268\n",
      " -0.27381628 -0.27249531 -0.31860459         nan -0.52623443         nan\n",
      "         nan -0.31860459 -0.32165889 -0.29558802 -0.57903356 -0.42554149\n",
      " -0.32635203 -0.40029049         nan         nan -0.37874573         nan\n",
      " -0.27266241         nan -0.41205395         nan         nan         nan\n",
      "         nan -0.44909814         nan -0.38675886 -0.35397195         nan\n",
      " -0.25883192         nan -0.43883407 -0.48564684 -0.49200322         nan\n",
      " -0.5387194  -0.43883407 -0.42804358         nan         nan -0.30337684\n",
      " -0.37946514 -0.55627569 -0.35085012 -0.4365785  -0.43239515 -0.49277574\n",
      " -0.71723196 -0.37204992 -0.71723196         nan -0.50812824 -0.37541788\n",
      " -0.46227749         nan         nan -0.31139185         nan -0.42586983\n",
      " -0.40790527 -0.4365785  -0.31139185 -0.36801971 -0.48429448         nan\n",
      "         nan         nan -0.60709776 -0.50346564 -0.43801393         nan\n",
      "         nan         nan -0.62959379 -0.51329813         nan -0.36826065\n",
      " -0.61499187 -0.52089505 -0.51964093 -0.30099753 -0.62959379 -0.33167333\n",
      " -0.35679721 -0.4365785          nan -0.40283433         nan -0.46053819\n",
      "         nan -0.32228324 -0.69638059 -0.48644227 -0.51887415 -0.37546177\n",
      "         nan         nan         nan         nan -0.4384039          nan\n",
      "         nan         nan         nan -0.50616489 -0.35208433 -0.3571701\n",
      "         nan         nan -0.46444742 -0.48831235         nan -0.50616489\n",
      " -0.61462791 -0.27255336 -0.37437634 -0.36337131         nan -0.49145068\n",
      " -0.27381628 -0.48684595 -0.48684595         nan         nan -0.4240104\n",
      " -0.45570122 -0.37649405 -0.39740917 -0.31822728         nan -0.39602182\n",
      " -0.46202319         nan -0.40029049         nan -0.60839299 -0.42083684\n",
      " -0.42764002         nan -0.31822728         nan -0.49957612 -0.31351409\n",
      "         nan         nan         nan -0.4635262  -0.58134445 -0.33596418\n",
      "         nan         nan         nan -0.44823068 -0.35397195         nan\n",
      " -0.5387194  -0.41246517 -0.42576863 -0.29703164         nan         nan\n",
      " -0.46202319 -0.34279    -0.43801393 -0.32650682 -0.48684595 -0.44811945\n",
      " -0.4500271  -0.28851069 -0.37546177 -0.51218678         nan         nan\n",
      " -0.32228324 -0.57903356         nan -0.4240104  -0.32165889         nan\n",
      "         nan         nan -0.54211131 -0.45054284 -0.41954432         nan\n",
      "         nan         nan -0.26583879         nan -0.66530787         nan\n",
      "         nan -0.46015526 -0.66530787 -0.38493372 -0.42839352         nan\n",
      "         nan         nan         nan -0.41864937         nan -0.28851069\n",
      " -0.47931733 -0.39750308 -0.35017103         nan         nan -0.24108115\n",
      " -0.44540454         nan         nan -0.4629999          nan         nan\n",
      " -0.35397195         nan         nan -0.35208433         nan -0.34651877\n",
      " -0.4240104          nan -0.50337645         nan -0.50337645         nan\n",
      "         nan         nan -0.42764002         nan         nan -0.40129461\n",
      " -0.41205395         nan -0.53791238         nan -0.45815832 -0.27381628\n",
      "         nan -0.37649405 -0.49145068 -0.37402744         nan -0.78859854\n",
      "         nan         nan         nan -0.40636753 -0.53791238         nan\n",
      "         nan -0.40790527 -0.46015526 -0.40639079         nan         nan\n",
      " -0.30039769 -0.30176219 -0.28608143 -0.35916179         nan -0.38166449\n",
      " -0.51329813 -0.49145068         nan         nan -0.50256362 -0.28576384\n",
      " -0.28576384         nan         nan -0.48799835 -0.44430235 -0.58134445\n",
      " -0.38261149 -0.51785357 -0.59697796 -0.36834312 -0.58835769 -0.64039943\n",
      "         nan         nan -0.30397246 -0.64039943 -0.40639079         nan\n",
      " -0.57903356 -0.34651877         nan -0.42124642         nan         nan\n",
      " -0.52776131 -0.41644014         nan -0.34279            nan -0.59819644\n",
      "         nan -0.5387194  -0.58191647 -0.42124642         nan -0.55033592\n",
      "         nan -0.37587755         nan         nan -0.60839299 -0.33929049\n",
      " -0.44540454         nan -0.39287127         nan -0.46227749 -0.60054271\n",
      " -0.62959379 -0.51887415 -0.51329813         nan         nan -0.36801971\n",
      "         nan         nan -0.25819717         nan -0.48644227 -0.2516571\n",
      " -0.41864937         nan -0.48429448 -0.56292149 -0.48799835 -0.30385635\n",
      " -0.37874573 -0.51887415 -0.60709776         nan         nan -0.41954432\n",
      " -0.37874573 -0.4254594  -0.50337645         nan -0.45725757         nan\n",
      " -0.32228324 -0.59697796         nan -0.54211131 -0.63728715 -0.38261149\n",
      " -0.28851069 -0.6288599          nan         nan -0.69638059 -0.48644227\n",
      " -0.46518653         nan         nan -0.5387194  -0.44823068 -0.49145068\n",
      "         nan         nan         nan         nan         nan -0.53313839\n",
      "         nan -0.78859854         nan -0.44540454 -0.52623443 -0.38484678\n",
      "         nan         nan -0.37691885         nan         nan         nan\n",
      " -0.87707732 -0.37437634 -0.44540454 -0.53313839 -0.38166449 -0.63365343\n",
      "         nan -0.39340686         nan -1.03395578 -0.4629999          nan\n",
      " -0.5966566  -0.23602981 -0.52089505         nan         nan         nan\n",
      " -0.35085012         nan -0.44823068         nan         nan -0.30337684\n",
      " -0.51964093         nan         nan -0.43432397]  Accuracy: 0.8739130434782608  F1 Score: 0.9052287581699346 Precision training: 0.8496932515337423 Recall training: <function recall at 0x000002DEF8F891E0> ]\n",
      "TEST: [Iteracion:  1800 Accuracy: 0.8741258741258742  F1 Score: 0.90625 Precision Test: 0.8529411764705882 Recall Test: 0.9666666666666667 ]\n"
     ]
    }
   ],
   "source": [
    "## Llama a la funcion de gradient descent, recurda definir un nombre diferente para el modelo (~1 linea) ##\n",
    "modelo_5_theta_values, cost_vect, acc_vect, acc_vect_test = \\\n",
    "gradient_descent(X_train_features, Y_train_m5, theta_vector, alpha, iterations, X_test_features, y_test, 'modelo_5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2def9839da0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Funcion para graficar el costo y  accuracy durante cada iteracion ##\n",
    "helper.training_graph(cost_vect, acc_vect, acc_vect_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy con el Test set [Modelo 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = prediccion(X_test_features, modelo_4_theta_values, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 87.41258741258741 %\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy:',accuracy_score(y_test, y_predict)* 100 ,'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Matriz de confusion Test Set [Modelo 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[38, 15],\n",
       "       [ 3, 87]], dtype=int64)"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision:  0.8529411764705882\n",
      "recall:  0.9666666666666667\n"
     ]
    }
   ],
   "source": [
    "print('precision: ',precision_score(y_test, y_predict))\n",
    "\n",
    "print('recall: ', recall_score(y_test, y_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparacion de Modelos\n",
    "Es importante llevar un registro de los resultados de nuestros modelos y los hyperparametros que utilizamos, para ir comparando y ajustando los modelos. En nuestra función de gradient descent esta implementado un registro a bitacora, que almacena cada ejecucion de la funcion.\n",
    "\n",
    "Este registro almacena datos que nos van a servir para comprar los modelos y tambien almacena los valores theta del modelo, por lo que podemos guardar los modelos y utlizarlos despues.\n",
    "A continuacion vamos a desplegar en la siguiente celda y nos va ayudar a tomar una decisión de cual es el mejor modelo.\n",
    "\n",
    "Es recomendable que, luego de analizar los resultados del registro ejecutemos de nuevo alguno de los modelos cambiando los hyperparametros o numero de ejemplos y ver con cuales obtenemos mejores resultados.\n",
    "\n",
    "Podemos ejecutar la funcion `helper.guardar_log_book(log_book)` para almacenar en disco los modelos.\n",
    "\n",
    "Para el ultimo paso donde exportamos el modelo debemos utilizar la funcion `helper.filter_log_book(log_book, nombre_modelo)` para obtener el mejor modelo y exportalo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Nombre Modelo</th>\n",
       "      <th>Numero de features</th>\n",
       "      <th>Numero de ejemplos</th>\n",
       "      <th>Alpha</th>\n",
       "      <th>Numero de Iteraciones</th>\n",
       "      <th>Accuracy Training</th>\n",
       "      <th>Accuracy Test</th>\n",
       "      <th>F1 score Test</th>\n",
       "      <th>Precision Test</th>\n",
       "      <th>Recall Test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>modelo_1_131752473363</td>\n",
       "      <td>5</td>\n",
       "      <td>150</td>\n",
       "      <td>0.010</td>\n",
       "      <td>500</td>\n",
       "      <td>0.686667</td>\n",
       "      <td>0.664336</td>\n",
       "      <td>0.789474</td>\n",
       "      <td>0.652174</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>modelo_1_132034622799</td>\n",
       "      <td>5</td>\n",
       "      <td>150</td>\n",
       "      <td>0.010</td>\n",
       "      <td>500</td>\n",
       "      <td>0.906667</td>\n",
       "      <td>0.874126</td>\n",
       "      <td>0.906250</td>\n",
       "      <td>0.852941</td>\n",
       "      <td>0.966667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>modelo_2_132039593654</td>\n",
       "      <td>3</td>\n",
       "      <td>400</td>\n",
       "      <td>0.050</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.907500</td>\n",
       "      <td>0.874126</td>\n",
       "      <td>0.906250</td>\n",
       "      <td>0.852941</td>\n",
       "      <td>0.966667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>modelo_5_132042169998</td>\n",
       "      <td>5</td>\n",
       "      <td>460</td>\n",
       "      <td>0.005</td>\n",
       "      <td>2000</td>\n",
       "      <td>0.882609</td>\n",
       "      <td>0.860140</td>\n",
       "      <td>0.891304</td>\n",
       "      <td>0.872340</td>\n",
       "      <td>0.911111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>modelo_4_132042578184</td>\n",
       "      <td>5</td>\n",
       "      <td>460</td>\n",
       "      <td>0.005</td>\n",
       "      <td>2000</td>\n",
       "      <td>0.619565</td>\n",
       "      <td>0.629371</td>\n",
       "      <td>0.772532</td>\n",
       "      <td>0.629371</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>modelo_4_132042262012</td>\n",
       "      <td>5</td>\n",
       "      <td>460</td>\n",
       "      <td>0.005</td>\n",
       "      <td>2000</td>\n",
       "      <td>0.619565</td>\n",
       "      <td>0.629371</td>\n",
       "      <td>0.772532</td>\n",
       "      <td>0.629371</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>modelo_3_132043279118</td>\n",
       "      <td>4</td>\n",
       "      <td>460</td>\n",
       "      <td>0.020</td>\n",
       "      <td>1500</td>\n",
       "      <td>0.882609</td>\n",
       "      <td>0.860140</td>\n",
       "      <td>0.897959</td>\n",
       "      <td>0.830189</td>\n",
       "      <td>0.977778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>modelo_4_132043291832</td>\n",
       "      <td>5</td>\n",
       "      <td>460</td>\n",
       "      <td>0.030</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.886957</td>\n",
       "      <td>0.874126</td>\n",
       "      <td>0.906250</td>\n",
       "      <td>0.852941</td>\n",
       "      <td>0.966667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>modelo_5_132043126278</td>\n",
       "      <td>5</td>\n",
       "      <td>460</td>\n",
       "      <td>0.005</td>\n",
       "      <td>2000</td>\n",
       "      <td>0.869565</td>\n",
       "      <td>0.874126</td>\n",
       "      <td>0.906250</td>\n",
       "      <td>0.852941</td>\n",
       "      <td>0.966667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>modelo_1_132049520640</td>\n",
       "      <td>5</td>\n",
       "      <td>150</td>\n",
       "      <td>0.010</td>\n",
       "      <td>2000</td>\n",
       "      <td>0.913333</td>\n",
       "      <td>0.860140</td>\n",
       "      <td>0.896907</td>\n",
       "      <td>0.836538</td>\n",
       "      <td>0.966667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>modelo_1_132050292060</td>\n",
       "      <td>5</td>\n",
       "      <td>450</td>\n",
       "      <td>0.010</td>\n",
       "      <td>2000</td>\n",
       "      <td>0.891111</td>\n",
       "      <td>0.860140</td>\n",
       "      <td>0.896907</td>\n",
       "      <td>0.836538</td>\n",
       "      <td>0.966667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>modelo_1_132051606021</td>\n",
       "      <td>5</td>\n",
       "      <td>460</td>\n",
       "      <td>0.010</td>\n",
       "      <td>2000</td>\n",
       "      <td>0.871739</td>\n",
       "      <td>0.874126</td>\n",
       "      <td>0.906250</td>\n",
       "      <td>0.852941</td>\n",
       "      <td>0.966667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>modelo_4_132053699760</td>\n",
       "      <td>5</td>\n",
       "      <td>460</td>\n",
       "      <td>0.030</td>\n",
       "      <td>2000</td>\n",
       "      <td>0.882609</td>\n",
       "      <td>0.867133</td>\n",
       "      <td>0.901554</td>\n",
       "      <td>0.844660</td>\n",
       "      <td>0.966667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>modelo_1_132100579662</td>\n",
       "      <td>5</td>\n",
       "      <td>460</td>\n",
       "      <td>0.010</td>\n",
       "      <td>2000</td>\n",
       "      <td>0.882609</td>\n",
       "      <td>0.895105</td>\n",
       "      <td>0.920635</td>\n",
       "      <td>0.878788</td>\n",
       "      <td>0.966667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Nombre Modelo Numero de features Numero de ejemplos  Alpha  \\\n",
       "0   modelo_1_131752473363                  5                150  0.010   \n",
       "1   modelo_1_132034622799                  5                150  0.010   \n",
       "2   modelo_2_132039593654                  3                400  0.050   \n",
       "3   modelo_5_132042169998                  5                460  0.005   \n",
       "4   modelo_4_132042578184                  5                460  0.005   \n",
       "5   modelo_4_132042262012                  5                460  0.005   \n",
       "6   modelo_3_132043279118                  4                460  0.020   \n",
       "7   modelo_4_132043291832                  5                460  0.030   \n",
       "8   modelo_5_132043126278                  5                460  0.005   \n",
       "9   modelo_1_132049520640                  5                150  0.010   \n",
       "10  modelo_1_132050292060                  5                450  0.010   \n",
       "11  modelo_1_132051606021                  5                460  0.010   \n",
       "12  modelo_4_132053699760                  5                460  0.030   \n",
       "13  modelo_1_132100579662                  5                460  0.010   \n",
       "\n",
       "   Numero de Iteraciones  Accuracy Training  Accuracy Test  F1 score Test  \\\n",
       "0                    500           0.686667       0.664336       0.789474   \n",
       "1                    500           0.906667       0.874126       0.906250   \n",
       "2                   1000           0.907500       0.874126       0.906250   \n",
       "3                   2000           0.882609       0.860140       0.891304   \n",
       "4                   2000           0.619565       0.629371       0.772532   \n",
       "5                   2000           0.619565       0.629371       0.772532   \n",
       "6                   1500           0.882609       0.860140       0.897959   \n",
       "7                   1000           0.886957       0.874126       0.906250   \n",
       "8                   2000           0.869565       0.874126       0.906250   \n",
       "9                   2000           0.913333       0.860140       0.896907   \n",
       "10                  2000           0.891111       0.860140       0.896907   \n",
       "11                  2000           0.871739       0.874126       0.906250   \n",
       "12                  2000           0.882609       0.867133       0.901554   \n",
       "13                  2000           0.882609       0.895105       0.920635   \n",
       "\n",
       "    Precision Test  Recall Test  \n",
       "0         0.652174     1.000000  \n",
       "1         0.852941     0.966667  \n",
       "2         0.852941     0.966667  \n",
       "3         0.872340     0.911111  \n",
       "4         0.629371     1.000000  \n",
       "5         0.629371     1.000000  \n",
       "6         0.830189     0.977778  \n",
       "7         0.852941     0.966667  \n",
       "8         0.852941     0.966667  \n",
       "9         0.836538     0.966667  \n",
       "10        0.836538     0.966667  \n",
       "11        0.852941     0.966667  \n",
       "12        0.844660     0.966667  \n",
       "13        0.878788     0.966667  "
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "helper.print_log_book(log_book)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guardando la bitacora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "helper.guardar_log_book(log_book)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exportar el modelo\n",
    "\n",
    "Vamos a exportar el modelo para poder hacer deploy, ** el modelo exportado debe tener las cinco primeras feautures **. Por lo cual de todos tus experimentos, selecciona el mejor modelo que cumpla con usar las 5 features ya mencionadas.\n",
    "\n",
    "Selecciona tu mejor modelo:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seleccionamos el mejor modelo\n",
    "\n",
    "Para seleccionar un modelo utilizamos la funcion `helper.filter_log_book()`.\n",
    "\n",
    "Esta funcion recibe de parametros la bitacora y el nombre del modelo que elegiste como el mejor\n",
    "\n",
    "`helper.filter_log_book(log_book, <nombre de nuestro mejor modelo>)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.1634648059745372, -0.4824464897175293, 0.1843360280956183, -1.421926689816284, -0.004327445462714156, 2.1688465665748455]\n"
     ]
    }
   ],
   "source": [
    "### INICIO: TU CODIGO AQUI:  (~1 linea)###\n",
    "modelo = helper.filter_log_book(log_book, 'modelo_1_132100579662')\n",
    "\n",
    "print(modelo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se va a utilizar el siguiente codigo para guardar el modelo elegido en un archivo.\n",
    "\n",
    "Esto genera 2 archivos, en un archivo _model.csv se tienen los parametros de el modelo elegido.\n",
    "\n",
    "El segundo archivo llamado _momentos.csv guarda la media y desviacion estandar de cada features, esto sera utilizado\n",
    "en la aplicacion hecha por el profesor para aplicar normalizacion al realizar predicciones.\n",
    "\n",
    "No hay que cambiar nada de codigo, solo hay que ejecutar la celda para exportar nuestro modelo y luego enviar los archivos .csv al profesor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El modelo ha sido exportado [ 20150066_momentos.csv ]\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "def exportar_modelo(model,carnet):\n",
    "    norm_std, norm_media = helper.get_normalizacion_vales(X_train[:,0:5])\n",
    "    with open(carnet + '_model.csv', 'w') as csvfile:\n",
    "        filewriter = csv.writer(csvfile, delimiter=',',quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "        filewriter.writerow(model)\n",
    "        \n",
    "    with open(carnet + '_momentos.csv', 'w') as csvfile:\n",
    "        filewriter = csv.writer(csvfile, delimiter=',',quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "        filewriter.writerow(norm_std)\n",
    "        filewriter.writerow(norm_media)\n",
    "        \n",
    "    print('El modelo ha sido exportado [',csvfile.name,']')\n",
    "    \n",
    "exportar_modelo(modelo,carnet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/ai_cancer.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Una vez elegido y exportado tu modelo , debes enviar los archivos .csv resultantes al profesor por correo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
